```python
# 3. è¿è¡Œå‰ªæï¼ˆçº¦3åˆ†é’Ÿ+3å°æ—¶åè®­ç»ƒï¼‰
python llama3.py --pruning_ratio 0.25 \
    --base_model /newdata/LLMs/Llama-3-8B-Instruct \
    --block_wise \
    --block_mlp_layer_start 3 --block_mlp_layer_end 30 \
    --block_attention_layer_start 3 --block_attention_layer_end 30 \
    --pruner_type taylor \
    --save_model
```


ğŸš€ ç°åœ¨å¯ä»¥é‡æ–°è¿è¡Œ
# æ‹‰å–æœ€æ–°ä»£ç 
git pull origin claude/fix-tuple-grad-fn-error-011CUywXCqgkBWjHYGzNQQJN

# è¿è¡Œå‰ªæï¼ˆæ‰€æœ‰å±‚ï¼‰
python llama3_unbalanced_pruning.py \
    --base_model /newdata/LLMs/Llama-3-8B-Instruct \
    --save_ckpt_log_name "llama_unbalanced_prune_all_layers" \
    --pruning_ratio 0.25 \
    --importance_method removal \
    --pruning_strategy inverse \
    --alpha 1.5 \
    --block_attention_layer_start 0 \
    --block_attention_layer_end 32 \
    --block_mlp_layer_start 0 \
    --block_mlp_layer_end 32 \
    --save_model \
    --test_after_train