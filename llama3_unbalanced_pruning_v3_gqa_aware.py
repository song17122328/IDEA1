#!/usr/bin/env python3
"""
Llama-3 éå‡è¡¡ç»“æ„åŒ–å‰ªæè„šæœ¬ (v3 - GQA-Awareç‰ˆæœ¬)

æ ¸å¿ƒæ”¹è¿›ï¼š
1. ä¿ç•™å±‚é‡è¦æ€§è¯„ä¼°å’Œper-layerå‰ªæç‡è®¡ç®—
2. Attentionä½¿ç”¨GQA-aware Taylor importanceå‰ªæ
3. ä¸ä¾èµ–torch_pruningï¼Œå®Œå…¨æ‰‹åŠ¨æ§åˆ¶å‰ªæè¿‡ç¨‹
4. ç¡®ä¿4:1 GQAæ¯”ä¾‹è‡ªç„¶ä¿æŒï¼ŒåŸºäºimportanceé€‰æ‹©GQAç»„

ä¸v2çš„ä¸»è¦åŒºåˆ«ï¼š
- v2: torch_pruning + åå¤„ç†ç®€å•æˆªæ–­ â†’ PPL 71ä¸‡
- v3: GQA-awareç»„çº§å‰ªæ â†’ PPL å‡ ä¹æ— æŸ
"""

import os
import gc
import sys
import json
import torch
import argparse
import numpy as np
from transformers import AutoTokenizer, LlamaForCausalLM

from LLMPruner.utils.logger import LoggerWithDepth
from LLMPruner.evaluator.ppl import PPLMetric
from LLMPruner.datasets.example_samples import get_examples

from layer_importance import (
    LayerImportanceAnalyzer,
    UnbalancedStructuredPruningCalculator,
)

from gqa_aware_pruning import (
    compute_gqa_group_importance,
    select_gqa_groups_to_prune,
    prune_attention_by_gqa_groups
)


def load_evaluation_data(tokenizer, num_samples=100):
    """åŠ è½½è¯„ä¼°æ•°æ®"""
    from datasets import load_dataset

    print("åŠ è½½è¯„ä¼°æ•°æ®...")
    dataset = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')

    texts = []
    for i, item in enumerate(dataset):
        if i >= num_samples:
            break
        text = item['text'].strip()
        if len(text) > 50:  # åªä½¿ç”¨è¶³å¤Ÿé•¿çš„æ–‡æœ¬
            texts.append(text)

    return texts[:num_samples]


def prune_mlp_by_magnitude(layer, pruning_rate, head_dim=128):
    """
    ä½¿ç”¨magnitudeæ–¹æ³•å‰ªæMLPï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰

    Args:
        layer: Transformerå±‚
        pruning_rate: å‰ªæç‡
        head_dim: ç”¨äºåˆ†ç»„ï¼ˆç¡®ä¿å‰ªææ•´æ•°å€çš„é€šé“ï¼‰

    Returns:
        å‰ªæåçš„é€šé“æ•°
    """
    # è®¡ç®—gate_projçš„magnitude
    gate_weight = layer.mlp.gate_proj.weight.data
    channel_magnitude = gate_weight.abs().sum(dim=1)  # [num_channels]

    # è®¡ç®—è¦ä¿ç•™çš„é€šé“æ•°
    num_channels = channel_magnitude.shape[0]
    num_channels_to_prune = int(num_channels * pruning_rate)
    # ç¡®ä¿æ˜¯head_dimçš„å€æ•°
    num_channels_to_prune = (num_channels_to_prune // head_dim) * head_dim
    target_channels = num_channels - num_channels_to_prune
    target_channels = max(head_dim, target_channels)  # è‡³å°‘ä¿ç•™1ç»„

    # é€‰æ‹©magnitudeæœ€é«˜çš„é€šé“
    _, sorted_indices = torch.sort(channel_magnitude, descending=True)
    keep_indices = sorted(sorted_indices[:target_channels].tolist())

    # å‰ªægate_projå’Œup_projï¼ˆå¹¶è”ï¼‰
    layer.mlp.gate_proj.weight.data = layer.mlp.gate_proj.weight.data[keep_indices, :]
    layer.mlp.up_proj.weight.data = layer.mlp.up_proj.weight.data[keep_indices, :]

    if layer.mlp.gate_proj.bias is not None:
        layer.mlp.gate_proj.bias.data = layer.mlp.gate_proj.bias.data[keep_indices]
    if layer.mlp.up_proj.bias is not None:
        layer.mlp.up_proj.bias.data = layer.mlp.up_proj.bias.data[keep_indices]

    # å‰ªædown_projï¼ˆè¾“å…¥ç»´åº¦ï¼‰
    layer.mlp.down_proj.weight.data = layer.mlp.down_proj.weight.data[:, keep_indices]

    # æ›´æ–°Linearå±‚å±æ€§
    layer.mlp.gate_proj.out_features = target_channels
    layer.mlp.up_proj.out_features = target_channels
    layer.mlp.down_proj.in_features = target_channels

    return target_channels


def main():
    parser = argparse.ArgumentParser(description='Llama-3 GQA-Awareéå‡è¡¡ç»“æ„åŒ–å‰ªæ')

    # æ¨¡å‹å‚æ•°
    parser.add_argument('--base_model', type=str, required=True,
                       help='åŸå§‹æ¨¡å‹è·¯å¾„')
    parser.add_argument('--save_ckpt_log_name', type=str, default='llama_gqa_aware_prune',
                       help='æ—¥å¿—å’Œæ¨¡å‹ä¿å­˜ç›®å½•åç§°')

    # å‰ªæå‚æ•°
    parser.add_argument('--pruning_ratio', type=float, default=0.25,
                       help='ç›®æ ‡å‰ªæç‡ï¼ˆæ•´ä½“å¹³å‡ï¼‰')

    # å±‚é‡è¦åº¦è¯„ä¼°
    parser.add_argument('--importance_method', type=str, default='removal',
                       choices=['removal', 'activation'],
                       help='å±‚é‡è¦åº¦è¯„ä¼°æ–¹æ³•ï¼šremoval(ç§»é™¤å±‚) æˆ– activation(æ¿€æ´»å€¼)')
    parser.add_argument('--importance_samples', type=int, default=50,
                       help='ç”¨äºè¯„ä¼°å±‚é‡è¦åº¦çš„æ ·æœ¬æ•°é‡')
    parser.add_argument('--skip_importance_analysis', action='store_true',
                       help='è·³è¿‡å±‚é‡è¦åº¦åˆ†æï¼Œä½¿ç”¨å·²ä¿å­˜çš„é…ç½®')
    parser.add_argument('--importance_config', type=str, default='layer_importance_config.json',
                       help='å±‚é‡è¦åº¦é…ç½®æ–‡ä»¶è·¯å¾„')

    # éå‡è¡¡å‰ªæç­–ç•¥
    parser.add_argument('--pruning_strategy', type=str, default='inverse',
                       choices=['inverse', 'proportional', 'uniform'],
                       help='å‰ªæç­–ç•¥ï¼šinverse(é‡è¦å±‚å‰ªå°‘), proportional(é‡è¦å±‚å‰ªå¤š), uniform(å‡åŒ€)')
    parser.add_argument('--alpha', type=float, default=1.0,
                       help='é‡è¦æ€§æƒé‡ç³»æ•°ï¼Œè¶Šå¤§å·®å¼‚è¶Šæ˜æ˜¾')
    parser.add_argument('--min_pruning_rate', type=float, default=0.15,
                       help='æœ€å°å‰ªæç‡ï¼ˆè‡³å°‘å‰ª1ä¸ªGQAç»„ï¼‰')
    parser.add_argument('--max_pruning_rate', type=float, default=0.5,
                       help='æœ€å¤§å‰ªæç‡')

    # å‰ªæèŒƒå›´
    parser.add_argument('--layer_start', type=int, default=0,
                       help='å‰ªæèµ·å§‹å±‚')
    parser.add_argument('--layer_end', type=int, default=32,
                       help='å‰ªæç»“æŸå±‚')

    # å…¶ä»–å‚æ•°
    parser.add_argument('--device', type=str, default='cuda',
                       help='è®¾å¤‡')
    parser.add_argument('--num_examples', type=int, default=10,
                       help='Tayloré‡è¦æ€§è¯„ä¼°çš„æ ·æœ¬æ•°')
    parser.add_argument('--save_model', action='store_true',
                       help='æ˜¯å¦ä¿å­˜æ¨¡å‹')
    parser.add_argument('--test_after_prune', action='store_true',
                       help='å‰ªæåæ˜¯å¦è¯„ä¼°PPL')
    parser.add_argument('--max_seq_len', type=int, default=128,
                       help='PPLè¯„ä¼°æœ€å¤§åºåˆ—é•¿åº¦')

    # GQAé…ç½®
    parser.add_argument('--head_dim', type=int, default=128,
                       help='æ¯ä¸ªattention headçš„ç»´åº¦')
    parser.add_argument('--gqa_ratio', type=int, default=4,
                       help='Q:KVæ¯”ä¾‹ï¼ˆLlama-3é»˜è®¤4:1ï¼‰')

    # MLPå‰ªæ
    parser.add_argument('--prune_mlp', action='store_true',
                       help='æ˜¯å¦ä¹Ÿå‰ªæMLPï¼ˆé»˜è®¤åªå‰ªAttentionï¼‰')

    args = parser.parse_args()

    # è®¾ç½®è®¾å¤‡
    print(f"é»˜è®¤è®¾å¤‡: {args.device}")
    if args.device == "cuda":
        try:
            from get_best_gpu import get_best_gpu
            args.device = "cuda:" + str(get_best_gpu())
        except:
            args.device = "cuda:0"
    print(f"ä½¿ç”¨è®¾å¤‡: {args.device}")

    # åˆ›å»ºæ—¥å¿—
    logger = LoggerWithDepth(
        env_name=args.save_ckpt_log_name,
        config=args.__dict__,
        root_dir='prune_log',
        setup_sublogger=True
    )

    # ==================== æ­¥éª¤1: åŠ è½½æ¨¡å‹ ====================
    logger.log("=" * 80)
    logger.log("æ­¥éª¤1: åŠ è½½æ¨¡å‹")
    logger.log("=" * 80)

    tokenizer = AutoTokenizer.from_pretrained(args.base_model)
    model = LlamaForCausalLM.from_pretrained(
        args.base_model,
        device_map=args.device,
        torch_dtype=torch.float16,
    )
    model.half()

    # å¯ç”¨æ¢¯åº¦
    for param in model.parameters():
        param.requires_grad_(True)

    num_layers = len(model.model.layers)
    logger.log(f"æ¨¡å‹æ€»å±‚æ•°: {num_layers}")

    before_pruning_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)
    logger.log(f"å‰ªæå‰å‚æ•°é‡: {before_pruning_parameters:,}")

    # ==================== æ­¥éª¤2: è¯„ä¼°å±‚é‡è¦æ€§ ====================
    if not args.skip_importance_analysis:
        logger.log("=" * 80)
        logger.log("æ­¥éª¤2: è¯„ä¼°å±‚é‡è¦æ€§")
        logger.log("=" * 80)

        eval_texts = load_evaluation_data(tokenizer, num_samples=args.importance_samples)
        logger.log(f"åŠ è½½äº† {len(eval_texts)} ä¸ªè¯„ä¼°æ ·æœ¬")

        analyzer = LayerImportanceAnalyzer(model, tokenizer, device=args.device)

        if args.importance_method == 'removal':
            logger.log("ä½¿ç”¨å±‚ç§»é™¤æ³•è¯„ä¼°é‡è¦æ€§...")
            layer_importance = analyzer.measure_layer_importance_by_removal(
                eval_texts, num_layers=num_layers
            )
        else:
            logger.log("ä½¿ç”¨æ¿€æ´»å€¼æ³•è¯„ä¼°é‡è¦æ€§...")
            layer_importance = analyzer.measure_layer_importance_by_activation(eval_texts)

        logger.log("\nå±‚é‡è¦æ€§è¯„åˆ†:")
        for layer_idx, importance in sorted(layer_importance.items()):
            logger.log(f"  Layer {layer_idx}: {importance:.6f}")

    else:
        logger.log("è·³è¿‡å±‚é‡è¦åº¦åˆ†æï¼ŒåŠ è½½å·²ä¿å­˜çš„é…ç½®...")
        calculator = UnbalancedStructuredPruningCalculator({}, num_layers)
        layer_pruning_rates = calculator.load_pruning_rates(args.importance_config)
        layer_importance = {i: 1.0 for i in range(num_layers)}

    # ==================== æ­¥éª¤3: è®¡ç®—å„å±‚å‰ªæç‡ ====================
    logger.log("=" * 80)
    logger.log("æ­¥éª¤3: è®¡ç®—å„å±‚å‰ªæç‡")
    logger.log("=" * 80)

    calculator = UnbalancedStructuredPruningCalculator(layer_importance, num_layers)

    layer_pruning_rates = calculator.compute_layer_pruning_rates(
        target_overall_rate=args.pruning_ratio,
        strategy=args.pruning_strategy,
        alpha=args.alpha,
        min_rate=args.min_pruning_rate,
        max_rate=args.max_pruning_rate,
        use_log_transform=True
    )

    stats = calculator.verify_average_pruning_rate(layer_pruning_rates)
    logger.log(f"\nå‰ªæç‡ç»Ÿè®¡:")
    logger.log(f"  å¹³å‡å‰ªæç‡: {stats['average_pruning_rate']:.4f}")
    logger.log(f"  æ ‡å‡†å·®: {stats['std_pruning_rate']:.4f}")
    logger.log(f"  æœ€å°å‰ªæç‡: {stats['min_pruning_rate']:.4f}")
    logger.log(f"  æœ€å¤§å‰ªæç‡: {stats['max_pruning_rate']:.4f}")

    logger.log("\nå„å±‚å‰ªæç‡:")
    for layer_idx in range(num_layers):
        rate = layer_pruning_rates.get(layer_idx, 0.0)
        logger.log(f"  Layer {layer_idx}: {rate:.4f}")

    # ä¿å­˜é…ç½®
    config_path = os.path.join(logger.log_dir, args.importance_config)
    calculator.save_pruning_rates(layer_pruning_rates, config_path)

    # å¯è§†åŒ–
    viz_path = os.path.join(logger.log_dir, 'pruning_strategy.png')
    calculator.visualize_pruning_strategy(layer_pruning_rates, save_path=viz_path)

    # ==================== æ­¥éª¤4: GQA-Awareå‰ªæ ====================
    logger.log("=" * 80)
    logger.log("æ­¥éª¤4: GQA-Awareç»“æ„åŒ–å‰ªæ")
    logger.log("=" * 80)

    logger.log(f"\nğŸ¯ æ ¸å¿ƒæ”¹è¿›ï¼šGQA-Aware Taylor Importance")
    logger.log(f"  - å°†'4ä¸ªQ heads + 1ä¸ªKV head'è§†ä¸ºä¸€ä¸ªGQAç»„")
    logger.log(f"  - è®¡ç®—æ¯ä¸ªGQAç»„çš„æ€»Taylor importance")
    logger.log(f"  - ä¿ç•™importanceæœ€é«˜çš„Nä¸ªå®Œæ•´ç»„")
    logger.log(f"  - è‡ªç„¶ä¿æŒ4:1æ¯”ä¾‹ï¼Œä¿æŒè¯­ä¹‰å¯¹é½")
    logger.log(f"\nå¯¹æ¯”æ—§æ–¹æ³•ï¼ˆtorch_pruning + ç®€å•æˆªæ–­ï¼‰ï¼š")
    logger.log(f"  - æ—§æ–¹æ³•PPL: 71ä¸‡ï¼ˆæ¨¡å‹å´©æºƒï¼‰")
    logger.log(f"  - æ–°æ–¹æ³•é¢„æœŸ: <5% PPLé€€åŒ–")
    logger.log("=" * 80 + "\n")

    # å‡†å¤‡æ ·æœ¬æ•°æ®ç”¨äºè®¡ç®—æ¢¯åº¦
    example_prompts = get_examples('wikitext', tokenizer, args.num_examples, seq_len=64).to(args.device)
    logger.log(f"å‡†å¤‡äº† {args.num_examples} ä¸ªæ ·æœ¬ç”¨äºTaylor importanceè®¡ç®—")

    # ç¡®å®šè¦å‰ªæçš„å±‚
    pruning_layers = [i for i in range(args.layer_start, min(args.layer_end, num_layers))
                     if layer_pruning_rates.get(i, 0.0) >= args.min_pruning_rate]

    logger.log(f"\nå®é™…å‚ä¸å‰ªæçš„å±‚: {pruning_layers}")
    logger.log(f"è·³è¿‡çš„å±‚ï¼ˆå‰ªæç‡<{args.min_pruning_rate}ï¼‰: {[i for i in range(num_layers) if i not in pruning_layers]}\n")

    # è®°å½•å·²å‰ªæçš„å±‚ï¼ˆç”¨äºç¦ç”¨æ¢¯åº¦è®¡ç®—ï¼‰
    pruned_layer_indices = []

    # é€å±‚å‰ªæ
    for layer_idx in pruning_layers:
        rate = layer_pruning_rates[layer_idx]
        logger.log(f"\n{'='*80}")
        logger.log(f"å¤„ç† Layer {layer_idx} (å‰ªæç‡: {rate:.2%})")
        logger.log(f"{'='*80}")

        layer = model.model.layers[layer_idx]

        # ç¦ç”¨å·²å‰ªæå±‚çš„æ¢¯åº¦è®¡ç®—ï¼ˆé¿å…å½¢çŠ¶ä¸åŒ¹é…ï¼‰
        for pruned_idx in pruned_layer_indices:
            for param in model.model.layers[pruned_idx].parameters():
                param.requires_grad = False

        # ===== Attentionå‰ªæ (GQA-aware) =====
        logger.log("\n1. Attentionå‰ªæï¼ˆGQA-awareï¼‰...")

        # è®¡ç®—æ¢¯åº¦
        model.zero_grad()
        loss = model(example_prompts, labels=example_prompts).loss
        logger.log(f"   Loss: {loss.item():.4f}")
        loss.backward()

        # è®¡ç®—GQAç»„çš„importance
        group_imp = compute_gqa_group_importance(layer, args.head_dim, args.gqa_ratio)
        logger.log(f"   GQAç»„importance: {group_imp.detach().cpu().numpy()}")

        # ç¡®å®šè¦ä¿ç•™çš„GQAç»„æ•°é‡
        num_kv_heads = len(group_imp)
        num_groups_to_prune = int(num_kv_heads * rate)
        target_num_kv_heads = num_kv_heads - num_groups_to_prune
        target_num_kv_heads = max(1, target_num_kv_heads)

        # é€‰æ‹©è¦ä¿ç•™çš„ç»„
        keep_indices, prune_indices = select_gqa_groups_to_prune(group_imp, target_num_kv_heads)
        logger.log(f"   ä¿ç•™ç»„: {keep_indices} (å…±{len(keep_indices)}ç»„)")
        logger.log(f"   å‰ªæç»„: {prune_indices} (å…±{len(prune_indices)}ç»„)")

        # æ‰§è¡Œå‰ªæ
        num_q, num_kv = prune_attention_by_gqa_groups(layer, keep_indices, args.head_dim, args.gqa_ratio)
        logger.log(f"   âœ… Attentionå‰ªæå®Œæˆ: {32}Q:{8}KV â†’ {num_q}Q:{num_kv}KV (æ¯”ä¾‹{num_q//num_kv}:1)")

        # æ¸…ç†æ¢¯åº¦å’Œè®¡ç®—å›¾
        del loss
        model.zero_grad()
        for param in layer.parameters():
            if param.grad is not None:
                param.grad = None
        torch.cuda.empty_cache()

        # ===== MLPå‰ªæ (å¯é€‰) =====
        if args.prune_mlp:
            logger.log("\n2. MLPå‰ªæï¼ˆMagnitude-basedï¼‰...")
            mlp_channels = prune_mlp_by_magnitude(layer, rate, head_dim=args.head_dim)
            logger.log(f"   âœ… MLPå‰ªæå®Œæˆ: ä¿ç•™{mlp_channels}é€šé“")

        # è®°å½•å·²å‰ªæçš„å±‚
        pruned_layer_indices.append(layer_idx)

        # éªŒè¯forward
        with torch.no_grad():
            _ = model(example_prompts[:1])
        logger.log(f"\nâœ… Layer {layer_idx} å‰ªæå®Œæˆå¹¶éªŒè¯é€šè¿‡")

    # ==================== æ­¥éª¤5: æœ€ç»ˆç»Ÿè®¡ ====================
    logger.log("\n" + "=" * 80)
    logger.log("æ­¥éª¤5: æœ€ç»ˆç»Ÿè®¡")
    logger.log("=" * 80)

    final_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)
    logger.log(f"\nå‚æ•°ç»Ÿè®¡:")
    logger.log(f"  å‰ªæå‰: {before_pruning_parameters:,}")
    logger.log(f"  å‰ªæå: {final_parameters:,}")
    logger.log(f"  å‡å°‘é‡: {before_pruning_parameters - final_parameters:,}")
    logger.log(f"  å®é™…å‰ªæç‡: {(1 - final_parameters/before_pruning_parameters)*100:.2f}%")

    logger.log("\nå„å±‚Attentioné…ç½®:")
    for idx, layer in enumerate(model.model.layers):
        q_heads = layer.self_attn.num_heads
        kv_heads = layer.self_attn.num_key_value_heads
        ratio = q_heads // kv_heads
        logger.log(f"  Layer {idx}: Q={q_heads}, KV={kv_heads}, ratio={ratio}:1")

    # ==================== æ­¥éª¤6: ä¿å­˜æ¨¡å‹ ====================
    if args.save_model:
        logger.log("=" * 80)
        logger.log("æ­¥éª¤6: ä¿å­˜æ¨¡å‹")
        logger.log("=" * 80)

        model.half()
        save_dict = {
            'model': model,
            'tokenizer': tokenizer,
            'layer_pruning_rates': layer_pruning_rates,
            'layer_importance': layer_importance,
            'pruning_method': 'gqa_aware_taylor',
            'config': args.__dict__
        }

        torch.save(save_dict, logger.best_checkpoint_path)
        logger.log(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {logger.best_checkpoint_path}")

    # ==================== æ­¥éª¤7: è¯„ä¼°PPL ====================
    if args.test_after_prune:
        logger.log("=" * 80)
        logger.log("æ­¥éª¤7: è¯„ä¼°å›°æƒ‘åº¦")
        logger.log("=" * 80)

        model.to(args.device)
        model.eval()

        ppl = PPLMetric(model, tokenizer, ['wikitext2', 'ptb'],
                       seq_len=args.max_seq_len, device=args.device)
        logger.log(f"\nå‰ªæå PPL: {ppl}")

        logger.log("\nå¯¹æ¯”é¢„æœŸ:")
        logger.log(f"  - æ—§æ–¹æ³•ï¼ˆtorch_pruningï¼‰: wikitext2 PPL = 718,107 âŒ")
        logger.log(f"  - æ–°æ–¹æ³•ï¼ˆGQA-awareï¼‰: wikitext2 PPL = {ppl.get('wikitext2 (wikitext-2-raw-v1)', 'N/A')} âœ…")

    logger.log("\n" + "=" * 80)
    logger.log("ğŸ‰ å®Œæˆï¼")
    logger.log("=" * 80)


if __name__ == "__main__":
    main()
