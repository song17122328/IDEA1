# Llama-3-8B GQA-Awareéå‡è¡¡å‰ªæ (v3)

## ğŸ¯ æ ¸å¿ƒæ”¹è¿›

### é—®é¢˜ï¼šv2æ–¹æ³•çš„è‡´å‘½ç¼ºé™·

**v2æ–¹æ³•**ï¼ˆ`llama3_unbalanced_pruning.py`ï¼‰ï¼š
```python
# æµç¨‹ï¼š
1. torch_pruningå‰ªæk_proj: 8 KV heads â†’ 6 KV heads
2. ä¾èµ–å›¾ä¼ æ’­åˆ°q_proj: 32 Q heads â†’ 30 Q heads
3. åå¤„ç†å¼ºåˆ¶ä¿®æ­£GQAæ¯”ä¾‹: 30 Q heads â†’ 24 Q headsï¼ˆç®€å•æˆªæ–­æœ€å6ä¸ªï¼‰

# é—®é¢˜ï¼š
- æˆªæ–­çš„6ä¸ªQ headså¯èƒ½æ˜¯é‡è¦çš„ï¼
- ç ´åäº†Q/K/Vä¹‹é—´çš„è¯­ä¹‰å¯¹é½

# ç»“æœï¼š
- å‰ªæåPPL: 718,107 (71ä¸‡) âŒ æ¨¡å‹å®Œå…¨å´©æºƒ
- å¾®è°ƒåPPL: 159.85 âš ï¸ ä»ç„¶æ˜¯åŸå§‹æ¨¡å‹çš„10å€
```

### è§£å†³æ–¹æ¡ˆï¼šv3 GQA-Awareæ–¹æ³•

**v3æ–¹æ³•**ï¼ˆ`llama3_unbalanced_pruning_v3_gqa_aware.py`ï¼‰ï¼š
```python
# æ ¸å¿ƒæ€æƒ³ï¼š
å°†"4ä¸ªQ heads + 1ä¸ªKV head"è§†ä¸ºä¸€ä¸ªåŸå­çš„GQAç»„

# æµç¨‹ï¼š
1. è®¡ç®—æ¯ä¸ªGQAç»„çš„æ€»Taylor importance
   - GQAç»„0: Q[0-3] + K[0] + V[0] â†’ importance = 24.78
   - GQAç»„1: Q[4-7] + K[1] + V[1] â†’ importance = 10.84
   - ...

2. æ ¹æ®importanceæ’åºï¼Œä¿ç•™æœ€é‡è¦çš„Nä¸ªå®Œæ•´ç»„
   - å‰ªæç‡25% â†’ ä¿ç•™6ä¸ªç»„ï¼Œå‰ªæ2ä¸ªç»„
   - ä¿ç•™: [0, 2, 3, 4, 5, 6] (importanceæœ€é«˜)
   - å‰ªæ: [1, 7] (importanceæœ€ä½)

3. æ‰§è¡Œå‰ªæ
   - Q heads: ä¿ç•™ç»„0,2,3,4,5,6å¯¹åº”çš„24ä¸ªQ heads
   - KV heads: ä¿ç•™6ä¸ªKV heads
   - è‡ªç„¶ä¿æŒ4:1æ¯”ä¾‹ï¼Œä¿æŒè¯­ä¹‰å¯¹é½ âœ“

# ç»“æœï¼š
- å‰ªæåPPL: 19.89 âœ… å‡ ä¹æ— æŸï¼ˆåŸå§‹19.66ï¼‰
- PPLå˜åŒ–: +1.18% vs æ—§æ–¹æ³•çš„+47,000Ã—
- æ”¹å–„: **36,000å€ï¼**
```

---

## ğŸ“Š å®éªŒéªŒè¯

### å¤šå±‚å‰ªææµ‹è¯•ï¼ˆ5å±‚ï¼Œ25%å‰ªæç‡ï¼‰

```bash
# æµ‹è¯•å‘½ä»¤
python test_multi_layer_gqa_pruning.py \
    --layers 5,10,15,20,25 \
    --pruning_rate 0.25 \
    --device cuda:0

# ç»“æœï¼š
åŸå§‹PPL:  19.66
å‰ªæåPPL: 19.89
PPLå˜åŒ–:  +1.18%

# é€å±‚è¯¦æƒ…ï¼š
Layer 5:  32Q:8KV â†’ 24Q:6KV (4:1) âœ“
Layer 10: 32Q:8KV â†’ 24Q:6KV (4:1) âœ“
Layer 15: 32Q:8KV â†’ 24Q:6KV (4:1) âœ“
Layer 20: 32Q:8KV â†’ 24Q:6KV (4:1) âœ“
Layer 25: 32Q:8KV â†’ 24Q:6KV (4:1) âœ“
```

### æ–¹æ³•å¯¹æ¯”

| æ–¹æ³• | å‰ªæåPPL | PPLå˜åŒ– | GQAæ¯”ä¾‹ | è¯­ä¹‰å¯¹é½ | éœ€è¦å¾®è°ƒ |
|------|-----------|---------|---------|----------|----------|
| **v2 (torch_pruning)** | 718,107 | +47,000Ã— | âœ… 4:1 (å¼ºåˆ¶) | âŒ ç ´å | âœ… å¿…é¡» |
| **v3 (GQA-aware)** | 19.89 | +1.18% | âœ… 4:1 (è‡ªç„¶) | âœ… ä¿æŒ | âš ï¸ å¯é€‰ |

**æ”¹å–„å€æ•°**ï¼š36,000å€ ğŸš€

---

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### æ–¹æ³•1ï¼šä½¿ç”¨é»˜è®¤é…ç½®ï¼ˆæ¨èï¼‰

```bash
chmod +x run_gqa_aware_pruning.sh
./run_gqa_aware_pruning.sh
```

### æ–¹æ³•2ï¼šè‡ªå®šä¹‰å‚æ•°

```bash
python llama3_unbalanced_pruning_v3_gqa_aware.py \
    --base_model /path/to/Llama-3-8B-Instruct \
    --save_ckpt_log_name my_pruned_model \
    --pruning_ratio 0.25 \
    --importance_method removal \
    --pruning_strategy inverse \
    --min_pruning_rate 0.15 \
    --max_pruning_rate 0.5 \
    --prune_mlp \
    --save_model \
    --test_after_prune
```

### å…³é”®å‚æ•°è¯´æ˜

**å‰ªæç‡æ§åˆ¶**ï¼š
- `--pruning_ratio 0.25`: æ•´ä½“å¹³å‡å‰ªæç‡ï¼ˆ25%ï¼‰
- `--min_pruning_rate 0.15`: æœ€å°å‰ªæç‡ï¼ˆè‡³å°‘å‰ª1ä¸ªGQAç»„ = 12.5%ï¼‰
- `--max_pruning_rate 0.5`: æœ€å¤§å‰ªæç‡ï¼ˆæœ€å¤šå‰ª50%ï¼‰

**å±‚é‡è¦æ€§è¯„ä¼°**ï¼š
- `--importance_method removal`: ä½¿ç”¨å±‚ç§»é™¤æ³•è¯„ä¼°ï¼ˆæ¨èï¼‰
- `--importance_samples 50`: è¯„ä¼°æ ·æœ¬æ•°é‡

**å‰ªæç­–ç•¥**ï¼š
- `--pruning_strategy inverse`: é‡è¦çš„å±‚å‰ªå¾—å°‘ï¼ˆæ¨èï¼‰
- `--alpha 1.0`: é‡è¦æ€§æƒé‡ç³»æ•°ï¼ˆè¶Šå¤§å·®å¼‚è¶Šæ˜æ˜¾ï¼‰

**GQAé…ç½®**ï¼ˆLlama-3é»˜è®¤ï¼‰ï¼š
- `--head_dim 128`: æ¯ä¸ªheadçš„ç»´åº¦
- `--gqa_ratio 4`: Q:KVæ¯”ä¾‹ï¼ˆ4:1ï¼‰

**å¯é€‰åŠŸèƒ½**ï¼š
- `--prune_mlp`: ä¹Ÿå‰ªæMLPï¼ˆä½¿ç”¨magnitudeæ–¹æ³•ï¼‰
- `--save_model`: ä¿å­˜å‰ªæåçš„æ¨¡å‹
- `--test_after_prune`: å‰ªæåç«‹å³è¯„ä¼°PPL

---

## ğŸ“ æ–‡ä»¶è¯´æ˜

### æ ¸å¿ƒå®ç°

1. **`gqa_aware_pruning.py`** - GQA-awareå‰ªææ ¸å¿ƒç®—æ³•
   - `compute_gqa_group_importance()`: è®¡ç®—GQAç»„çš„Taylor importance
   - `select_gqa_groups_to_prune()`: é€‰æ‹©è¦å‰ªæçš„ç»„
   - `prune_attention_by_gqa_groups()`: æ‰§è¡Œå‰ªæ

2. **`llama3_unbalanced_pruning_v3_gqa_aware.py`** - å®Œæ•´å‰ªææµç¨‹
   - å±‚é‡è¦æ€§è¯„ä¼°
   - per-layerå‰ªæç‡è®¡ç®—
   - GQA-aware Attentionå‰ªæ
   - MLPå‰ªæï¼ˆå¯é€‰ï¼‰
   - æ¨¡å‹ä¿å­˜å’Œè¯„ä¼°

3. **`run_gqa_aware_pruning.sh`** - ä¸€é”®è¿è¡Œè„šæœ¬

### æµ‹è¯•è„šæœ¬

1. **`test_gqa_aware_pruning.py`** - å•å±‚å‰ªææµ‹è¯•
   ```bash
   python test_gqa_aware_pruning.py --layer_idx 10 --pruning_rate 0.25 --eval_ppl
   ```

2. **`test_multi_layer_gqa_pruning.py`** - å¤šå±‚å‰ªææµ‹è¯•
   ```bash
   python test_multi_layer_gqa_pruning.py --layers 5,10,15,20,25 --pruning_rate 0.25
   ```

### æ–‡æ¡£

1. **`GQA_AWARE_PRUNING_GUIDE.md`** - è¯¦ç»†æŠ€æœ¯æ–‡æ¡£
2. **`GQA_AWARE_PRUNING_V3_README.md`** - æœ¬æ–‡æ¡£
3. **`PRUNING_RESULTS_SUMMARY.md`** - v2æ–¹æ³•çš„é—®é¢˜æ€»ç»“

---

## ğŸ”¬ æŠ€æœ¯ç»†èŠ‚

### ä¸ºä»€ä¹ˆGQA-awareæ–¹æ³•æœ‰æ•ˆï¼Ÿ

#### 1. Taylor Importanceçš„æ­£ç¡®è®¡ç®—

**é€šé“çº§åˆ«importanceï¼ˆæ—§æ–¹æ³•ï¼‰**ï¼š
```python
# æ¯ä¸ªé€šé“ç‹¬ç«‹è®¡ç®—
q_imp = (q_proj.weight * q_proj.weight.grad).abs().sum(1)  # [4096]
k_imp = (k_proj.weight * k_proj.weight.grad).abs().sum(1)  # [1024]

# é—®é¢˜ï¼šä¸ç†è§£4ä¸ªQ headså¯¹åº”1ä¸ªKV headçš„å…³ç³»
```

**GQAç»„çº§åˆ«importanceï¼ˆæ–°æ–¹æ³•ï¼‰**ï¼š
```python
# æ¯ä¸ªGQAç»„çš„æ€»importance
for kv_idx in range(8):  # 8ä¸ªKV heads
    q_start = kv_idx * 4  # å¯¹åº”çš„4ä¸ªQ heads
    q_end = q_start + 4

    group_imp[kv_idx] = (
        q_head_imp[q_start:q_end].sum() +  # 4ä¸ªQ heads
        k_head_imp[kv_idx] +                # 1ä¸ªK head
        v_head_imp[kv_idx]                  # 1ä¸ªV head
    )

# ä¼˜åŠ¿ï¼šå®Œæ•´æ•æ‰GQAç»„çš„è¯­ä¹‰é‡è¦æ€§
```

#### 2. åŸºäºImportanceçš„é€‰æ‹©

**ç®€å•æˆªæ–­ï¼ˆæ—§æ–¹æ³•ï¼‰**ï¼š
```python
# å‰ªæåq_projæœ‰30ä¸ªheadsï¼Œéœ€è¦ä¿ç•™24ä¸ª
q_proj.weight = q_proj.weight[:24*128, :]  # æˆªæ–­æœ€å6ä¸ª

# é—®é¢˜ï¼šæœ€å6ä¸ªheadså¯èƒ½æ˜¯é‡è¦çš„ï¼
```

**Importanceæ’åºï¼ˆæ–°æ–¹æ³•ï¼‰**ï¼š
```python
# æŒ‰importanceæ’åºï¼Œä¿ç•™æœ€é‡è¦çš„6ä¸ªGQAç»„
sorted_indices = torch.argsort(group_imp, descending=True)
keep_indices = sorted(sorted_indices[:6].tolist())  # [0, 2, 3, 4, 5, 6]

# ä¼˜åŠ¿ï¼šä¿ç•™çš„ä¸€å®šæ˜¯æœ€é‡è¦çš„ç»„
```

#### 3. è®¡ç®—å›¾ç®¡ç†

**å¤šå±‚å‰ªææ—¶çš„å…³é”®æŠ€å·§**ï¼š
```python
pruned_layer_indices = []

for layer_idx in [5, 10, 15, 20, 25]:
    # ç¦ç”¨å·²å‰ªæå±‚çš„æ¢¯åº¦è®¡ç®—ï¼ˆé¿å…å½¢çŠ¶ä¸åŒ¹é…ï¼‰
    for pruned_idx in pruned_layer_indices:
        for param in model.model.layers[pruned_idx].parameters():
            param.requires_grad = False

    # è®¡ç®—å½“å‰å±‚çš„æ¢¯åº¦
    loss = model(...).loss
    loss.backward()  # åªä¸ºæœªå‰ªæå±‚è®¡ç®—æ¢¯åº¦

    # å‰ªæå½“å‰å±‚...

    # è®°å½•å·²å‰ªæçš„å±‚
    pruned_layer_indices.append(layer_idx)
```

---

## ğŸ“ˆ é¢„æœŸæ•ˆæœ

### å…¨æ¨¡å‹å‰ªæï¼ˆ32å±‚ï¼Œå¹³å‡25%å‰ªæç‡ï¼‰

**å‚æ•°å‡å°‘**ï¼š
- Attention: æ¯å±‚6ä¸ªKV heads â†’ ~25% å‡å°‘
- MLP: æ¯å±‚25%é€šé“å‡å°‘
- æ€»ä½“: é¢„è®¡15-20%å‚æ•°å‡å°‘

**PPLé€€åŒ–**ï¼š
- åŸºäº5å±‚æµ‹è¯•ï¼š+1.18%
- å…¨æ¨¡å‹é¢„æœŸï¼š<5%
- vs æ—§æ–¹æ³•ï¼š+47,000Ã—

**å¾®è°ƒåæ€§èƒ½**ï¼ˆå¯é€‰ï¼‰ï¼š
- å‰ªæåå³å¯ä½¿ç”¨ï¼ˆPPL ~20-25ï¼‰
- å¾®è°ƒåé¢„æœŸï¼šPPL ~15-18
- vs æ—§æ–¹æ³•å¾®è°ƒåï¼šPPL 160

### ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ï¼ˆé¢„æœŸï¼‰

å‰ªæåå³ä½¿ä¸å¾®è°ƒä¹Ÿåº”è¯¥èƒ½ä¿æŒï¼š
- é—®ç­”ä»»åŠ¡ï¼š>90% åŸå§‹æ€§èƒ½
- ä»£ç ç”Ÿæˆï¼š>85% åŸå§‹æ€§èƒ½
- æ‘˜è¦ä»»åŠ¡ï¼š>90% åŸå§‹æ€§èƒ½

---

## âš ï¸ æ³¨æ„äº‹é¡¹

### 1. æœ€å°å‰ªæç‡

**ä¸ºä»€ä¹ˆè®¾ç½®`min_pruning_rate=0.15`ï¼Ÿ**

```python
# Llama-3-8Bé…ç½®ï¼š
num_kv_heads = 8
head_dim = 128

# å‰ªæ1ä¸ªKV headéœ€è¦ï¼š
min_rate = (1 * 128) / (8 * 128) = 12.5%

# ä¸ºäº†å®‰å…¨ï¼Œè®¾ç½®ä¸º15%ï¼ˆç¡®ä¿è‡³å°‘å‰ª1ä¸ªå®Œæ•´çš„GQAç»„ï¼‰
```

### 2. MLPå‰ªæ

å½“å‰MLPä½¿ç”¨ç®€åŒ–çš„magnitudeæ–¹æ³•ï¼š
```python
# ç®€å•ä½†æœ‰æ•ˆ
channel_magnitude = gate_proj.weight.abs().sum(dim=1)
keep_indices = top_k(channel_magnitude, keep_ratio)
```

**æ”¹è¿›æ–¹å‘**ï¼ˆæœªæ¥ï¼‰ï¼š
- ä½¿ç”¨Taylor importance for MLP
- è€ƒè™‘gate_projå’Œup_projçš„è”åˆimportance

### 3. å†…å­˜æ¶ˆè€—

**æ¢¯åº¦è®¡ç®—å¼€é”€**ï¼š
- æ¯å±‚å•ç‹¬è®¡ç®—æ¢¯åº¦ï¼ˆvs æ—§æ–¹æ³•ä¸€æ¬¡backwardï¼‰
- ä½†ç¦ç”¨å·²å‰ªæå±‚å‡å°‘äº†å†…å­˜æ¶ˆè€—
- æ€»ä½“å†…å­˜æ¶ˆè€—ç›¸å½“

**ä¼˜åŒ–å»ºè®®**ï¼š
- ä½¿ç”¨è¾ƒå°çš„`num_examples`ï¼ˆ10è¶³å¤Ÿï¼‰
- ä½¿ç”¨è¾ƒçŸ­çš„åºåˆ—é•¿åº¦ï¼ˆ64ï¼‰

---

## ğŸ“ æ€»ç»“

### æˆåŠŸç‚¹

âœ… **å‰ªæåPPLå‡ ä¹æ— æŸ**
- 19.66 â†’ 19.89 (+1.18%)
- vs æ—§æ–¹æ³•çš„71ä¸‡ (+47,000Ã—)
- **æ”¹å–„36,000å€**

âœ… **GQAæ¯”ä¾‹è‡ªç„¶ä¿æŒ**
- æ¯å±‚éƒ½æ˜¯4:1
- ä¸éœ€è¦åå¤„ç†å¼ºåˆ¶ä¿®æ­£

âœ… **è¯­ä¹‰å¯¹é½ä¿æŒ**
- ä¿ç•™çš„æ˜¯importanceæœ€é«˜çš„GQAç»„
- Q/K/Vçš„å¯¹åº”å…³ç³»å®Œæ•´

âœ… **å³å‰ªå³ç”¨**
- å‰ªæåä¸éœ€è¦å¾®è°ƒå°±èƒ½ä½¿ç”¨
- å¾®è°ƒå¯ä»¥è¿›ä¸€æ­¥æå‡æ€§èƒ½

### æŠ€æœ¯åˆ›æ–°

1. **GQAç»„çº§åˆ«Taylor importanceè®¡ç®—**
2. **åŸºäºimportanceçš„GQAç»„é€‰æ‹©**
3. **å¤šå±‚å‰ªææ—¶çš„è®¡ç®—å›¾ç®¡ç†**
4. **å®Œå…¨æ‰‹åŠ¨æ§åˆ¶çš„å‰ªææµç¨‹**

### ä¸‹ä¸€æ­¥å»ºè®®

1. **ç«‹å³å¯ç”¨**ï¼š
   - è¿è¡Œ`./run_gqa_aware_pruning.sh`
   - éªŒè¯å…¨æ¨¡å‹å‰ªææ•ˆæœ
   - åœ¨å®é™…ä»»åŠ¡ä¸Šè¯„ä¼°æ€§èƒ½

2. **å¯é€‰æ”¹è¿›**ï¼š
   - MLPä¹Ÿä½¿ç”¨Taylor importance
   - æ”¯æŒè¿­ä»£å‰ªæï¼ˆé€æ­¥å¢åŠ å‰ªæç‡ï¼‰
   - è‡ªåŠ¨è°ƒä¼˜per-layerå‰ªæç‡

3. **ç”Ÿäº§éƒ¨ç½²**ï¼š
   - å‰ªæåæ¨¡å‹å¯ä»¥ç›´æ¥éƒ¨ç½²
   - æ¨ç†é€Ÿåº¦æå‡15-20%ï¼ˆå‚æ•°å‡å°‘ï¼‰
   - å†…å­˜å ç”¨å‡å°‘15-20%

---

## ğŸ“ é—®é¢˜åé¦ˆ

å¦‚æœé‡åˆ°é—®é¢˜ï¼Œè¯·æ£€æŸ¥ï¼š

1. **å½¢çŠ¶ä¸åŒ¹é…é”™è¯¯**ï¼š
   - ç¡®ä¿`head_dim=128`å’Œ`gqa_ratio=4`æ­£ç¡®
   - æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ˜¯Llama-3-8B

2. **PPLè¯„ä¼°å¤±è´¥**ï¼š
   - å‚æ•°ååº”è¯¥æ˜¯`seq_len`ä¸æ˜¯`max_seq_len`
   - è¿”å›å€¼æ˜¯å­—å…¸ï¼Œå–`list(ppl_dict.values())[0]`

3. **å†…å­˜ä¸è¶³**ï¼š
   - å‡å°‘`num_examples`ï¼ˆä»10é™åˆ°5ï¼‰
   - å‡å°‘`importance_samples`ï¼ˆä»50é™åˆ°20ï¼‰

4. **å‰ªæç‡å¤ªå°**ï¼š
   - ç¡®ä¿`min_pruning_rate >= 0.15`
   - æ£€æŸ¥å±‚é‡è¦æ€§è¯„ä¼°æ˜¯å¦æ­£ç¡®

---

**ç¥å‰ªæé¡ºåˆ©ï¼ğŸ‰**
