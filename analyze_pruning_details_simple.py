#!/usr/bin/env python3
"""
è¯¦ç»†åˆ†æéå‡è¡¡ç»“æ„åŒ–å‰ªæè¿‡ç¨‹ï¼ˆæ— éœ€åŠ è½½æ¨¡å‹ï¼‰
å±•ç¤ºæ­¥éª¤3ï¼ˆåˆ›å»ºæ¨¡å—çº§å‰ªæç‡å­—å…¸ï¼‰å’Œæ­¥éª¤4ï¼ˆç»“æ„åŒ–å‰ªæï¼‰çš„ç»†èŠ‚
"""

import json
from typing import Dict


def load_pruning_config(config_path: str) -> Dict:
    """åŠ è½½å‰ªæé…ç½®"""
    with open(config_path, 'r') as f:
        config = json.load(f)
    return config['layer_pruning_rates']


def analyze_layer_filtering(layer_pruning_rates: Dict[str, float],
                            block_attention_start: int = 3,
                            block_attention_end: int = 30,
                            block_mlp_start: int = 3,
                            block_mlp_end: int = 30):
    """
    åˆ†ææ­¥éª¤3ï¼šå±‚è¿‡æ»¤é€»è¾‘

    è§£é‡Šä¸ºä»€ä¹ˆè™½ç„¶è®¡ç®—äº†æ‰€æœ‰å±‚ï¼ˆ0-31ï¼‰çš„å‰ªæç‡ï¼Œ
    ä½†å®é™…åªå‰ªæ3-29å±‚
    """
    print("=" * 80)
    print("æ­¥éª¤3è¯¦è§£ï¼šå±‚è¿‡æ»¤å’Œæ¨¡å—çº§å‰ªæç‡å­—å…¸åˆ›å»º")
    print("=" * 80)
    print()

    # è½¬æ¢keyä¸ºint
    layer_pruning_rates = {int(k): v for k, v in layer_pruning_rates.items()}

    # 1. æ˜¾ç¤ºæ‰€æœ‰å±‚çš„è®¡ç®—ç»“æœ
    print("1ï¸âƒ£  æ‰€æœ‰å±‚çš„å‰ªæç‡ï¼ˆæ­¥éª¤2è®¡ç®—ç»“æœï¼‰:")
    print("-" * 80)
    for layer_idx in sorted(layer_pruning_rates.keys()):
        rate = layer_pruning_rates[layer_idx]
        print(f"   Layer {layer_idx:2d}: {rate:.4f}")
    print()

    # 2. æ˜¾ç¤ºè¿‡æ»¤é€»è¾‘
    print("2ï¸âƒ£  å±‚è¿‡æ»¤é€»è¾‘ï¼ˆllama3_unbalanced_pruning.py:222-228è¡Œï¼‰:")
    print("-" * 80)
    print(f"   å‚æ•°é…ç½®:")
    print(f"     --block_attention_layer_start = {block_attention_start}")
    print(f"     --block_attention_layer_end   = {block_attention_end}")
    print(f"     --block_mlp_layer_start       = {block_mlp_start}")
    print(f"     --block_mlp_layer_end         = {block_mlp_end}")
    print()
    print(f"   Python range() è§„åˆ™:")
    print(f"     range({block_attention_start}, {block_attention_end}) = [{block_attention_start}, {block_attention_start+1}, ..., {block_attention_end-1}]  (ä¸åŒ…å«{block_attention_end})")
    print()
    print("   ä»£ç :")
    print("     pruning_layers = set(range(args.block_attention_layer_start,")
    print("                                args.block_attention_layer_end)) | \\")
    print("                      set(range(args.block_mlp_layer_start,")
    print("                                args.block_mlp_layer_end))")
    print()

    # 3. æ˜¾ç¤ºè¿‡æ»¤ç»“æœ
    pruning_layers = set(range(block_attention_start, block_attention_end)) | \
                     set(range(block_mlp_start, block_mlp_end))

    filtered_rates = {
        idx: rate for idx, rate in layer_pruning_rates.items()
        if idx in pruning_layers
    }

    print("3ï¸âƒ£  è¿‡æ»¤åå®é™…å‚ä¸å‰ªæçš„å±‚:")
    print("-" * 80)
    print(f"   å®é™…å‰ªæå±‚é›†åˆ: {sorted(pruning_layers)}")
    print(f"   å…± {len(pruning_layers)} å±‚")
    print()

    # 4. æ˜¾ç¤ºè¢«ä¿æŠ¤çš„å±‚
    all_layers = set(layer_pruning_rates.keys())
    protected_layers = all_layers - pruning_layers

    print("4ï¸âƒ£  è¢«ä¿æŠ¤ï¼ˆä¸å‰ªæï¼‰çš„å±‚:")
    print("-" * 80)
    print(f"   ä¸å‰ªæå±‚é›†åˆ: {sorted(protected_layers)}")
    print()
    print("   ğŸ›¡ï¸  ä¿æŠ¤åŸå› :")
    early_layers = [i for i in protected_layers if i < block_attention_start]
    late_layers = [i for i in protected_layers if i >= block_attention_end]

    if early_layers:
        print(f"     - å‰ {len(early_layers)} å±‚ {early_layers}: åº•å±‚ç‰¹å¾æå–å±‚ï¼Œå¯¹æ¨¡å‹æ€§èƒ½å½±å“å¤§")
    if late_layers:
        print(f"     - å {len(late_layers)} å±‚ {late_layers}: é«˜å±‚è¯­ä¹‰ç†è§£å±‚ï¼Œå¯¹æ¨¡å‹æ€§èƒ½å½±å“å¤§")
    print()

    return filtered_rates, pruning_layers


def analyze_ch_sparsity_dict_creation(filtered_rates: Dict[int, float],
                                      prune_attention: bool = True,
                                      prune_mlp: bool = True):
    """
    åˆ†æ ch_sparsity_dict çš„åˆ›å»ºè¿‡ç¨‹
    """
    print("=" * 80)
    print("æ­¥éª¤3è¯¦è§£ï¼šch_sparsity_dictï¼ˆæ¨¡å—çº§å‰ªæç‡å­—å…¸ï¼‰åˆ›å»º")
    print("=" * 80)
    print()

    print("5ï¸âƒ£  ch_sparsity_dict æ˜¯ä»€ä¹ˆï¼Ÿ")
    print("-" * 80)
    print("   ch_sparsity_dict æ˜¯ä¸€ä¸ª Python å­—å…¸ï¼Œå°† PyTorch æ¨¡å—å¯¹è±¡æ˜ å°„åˆ°å…¶å‰ªæç‡")
    print("   æ ¼å¼: {module_object: pruning_rate}")
    print()
    print("   ä½œç”¨: å‘Šè¯‰ MetaPruner æ¯ä¸ªæ¨¡å—åº”è¯¥å‰ªæå¤šå°‘æ¯”ä¾‹çš„é€šé“æ•°")
    print()

    print("6ï¸âƒ£  ä¸ºä»€ä¹ˆé€‰æ‹© k_proj å’Œ gate_proj ä½œä¸º root modulesï¼Ÿ")
    print("-" * 80)
    print("   Llama æ¨¡å‹æ¶æ„:")
    print("     æ¯å±‚åŒ…å«ä¸¤ä¸ªä¸»è¦ç»„ä»¶:")
    print("       1. Self-Attention: q_proj, k_proj, v_proj, o_proj")
    print("       2. MLP:           gate_proj, up_proj, down_proj")
    print()
    print("   ç»“æ„åŒ–å‰ªæè§„åˆ™:")
    print("     - Attention: å‰ªæ k_proj çš„è¾“å‡ºé€šé“ â†’ è‡ªåŠ¨ä¼ æ’­åˆ° q_proj, v_proj, o_proj")
    print("     - MLP:       å‰ªæ gate_proj çš„è¾“å‡ºé€šé“ â†’ è‡ªåŠ¨ä¼ æ’­åˆ° up_proj, down_proj")
    print()
    print("   è¿™æ ·åªéœ€è®¾ç½® 2 ä¸ª root moduleï¼Œå°±èƒ½å‰ªææ•´å±‚çš„ 7 ä¸ªçº¿æ€§å±‚ï¼")
    print()

    print("7ï¸âƒ£  ch_sparsity_dict åˆ›å»ºä»£ç ï¼ˆlayer_importance.py:312-327ï¼‰:")
    print("-" * 80)
    print("   def create_ch_sparsity_dict_for_llama(model, layer_pruning_rates, ...):")
    print("       ch_sparsity_dict = {}")
    print()
    print("       for layer_idx, pruning_rate in layer_pruning_rates.items():")
    print("           layer = model.model.layers[layer_idx]")
    print()
    print("           # Attention æ¨¡å—")
    print("           if prune_attention:")
    print("               ch_sparsity_dict[layer.self_attn.k_proj] = pruning_rate")
    print()
    print("           # MLP æ¨¡å—")
    print("           if prune_mlp:")
    print("               ch_sparsity_dict[layer.mlp.gate_proj] = pruning_rate")
    print()
    print("       return ch_sparsity_dict")
    print()

    print("8ï¸âƒ£  ch_sparsity_dict å†…å®¹ç¤ºä¾‹:")
    print("-" * 80)

    layer_module_info = []
    for layer_idx, pruning_rate in sorted(filtered_rates.items()):
        modules_in_layer = []
        if prune_attention:
            modules_in_layer.append(('k_proj', pruning_rate))
        if prune_mlp:
            modules_in_layer.append(('gate_proj', pruning_rate))
        layer_module_info.append((layer_idx, modules_in_layer))

    # æ˜¾ç¤ºå‰3å±‚å’Œå3å±‚ä½œä¸ºç¤ºä¾‹
    print("   ç¤ºä¾‹ï¼ˆå‰3å±‚ï¼‰:")
    for layer_idx, modules in layer_module_info[:3]:
        print(f"\n   Layer {layer_idx} (å‰ªæç‡: {filtered_rates[layer_idx]:.4f}):")
        for mod_name, rate in modules:
            print(f"     - model.model.layers[{layer_idx}].self_attn.{mod_name}: {rate:.4f}" if 'proj' in mod_name and mod_name != 'gate_proj' else f"     - model.model.layers[{layer_idx}].mlp.{mod_name}: {rate:.4f}")

    print("\n   ...")

    print("\n   ç¤ºä¾‹ï¼ˆå3å±‚ï¼‰:")
    for layer_idx, modules in layer_module_info[-3:]:
        print(f"\n   Layer {layer_idx} (å‰ªæç‡: {filtered_rates[layer_idx]:.4f}):")
        for mod_name, rate in modules:
            print(f"     - model.model.layers[{layer_idx}].self_attn.{mod_name}: {rate:.4f}" if 'proj' in mod_name and mod_name != 'gate_proj' else f"     - model.model.layers[{layer_idx}].mlp.{mod_name}: {rate:.4f}")

    num_modules = len(filtered_rates) * (int(prune_attention) + int(prune_mlp))
    print()
    print(f"   æ€»è®¡: {num_modules} ä¸ªæ¨¡å—è®¾ç½®äº†è‡ªå®šä¹‰å‰ªæç‡")
    print(f"   è®¡ç®—: {len(filtered_rates)} å±‚ Ã— {int(prune_attention) + int(prune_mlp)} æ¨¡å—/å±‚ = {num_modules} ä¸ªæ¨¡å—")
    print()


def explain_metapruner_workflow():
    """
    è§£é‡Šæ­¥éª¤4ï¼šMetaPruner çš„å·¥ä½œæµç¨‹
    """
    print("=" * 80)
    print("æ­¥éª¤4è¯¦è§£ï¼šMetaPruner ç»“æ„åŒ–å‰ªæå·¥ä½œæµç¨‹")
    print("=" * 80)
    print()

    print("9ï¸âƒ£  ä»€ä¹ˆæ˜¯ç»“æ„åŒ–å‰ªæï¼Ÿ")
    print("-" * 80)
    print("   éç»“æ„åŒ–å‰ªæï¼ˆç¨€ç–å‰ªæï¼‰:")
    print("     - å°†æƒé‡çŸ©é˜µä¸­çš„æŸäº›å…ƒç´ è®¾ä¸º 0")
    print("     - å‚æ•°é€»è¾‘ä¸Šå‡å°‘ï¼Œä½†ç‰©ç†å†…å­˜ä¸å‡å°‘")
    print("     - éœ€è¦ç¨€ç–çŸ©é˜µè¿ç®—æ”¯æŒæ‰èƒ½åŠ é€Ÿ")
    print()
    print("   ç»“æ„åŒ–å‰ªæï¼ˆé€šé“å‰ªæï¼‰:")
    print("     - åˆ é™¤æ•´ä¸ªé€šé“ï¼ˆç¥ç»å…ƒï¼‰")
    print("     - ç‰©ç†ä¸Šå‡å°æ¨¡å‹å°ºå¯¸")
    print("     - ç›´æ¥åŠ é€Ÿï¼Œæ— éœ€ç‰¹æ®Šç¡¬ä»¶æ”¯æŒ")
    print()
    print("   ç¤ºä¾‹:")
    print("     åŸå§‹:    Linear(4096 â†’ 4096)")
    print("              æƒé‡çŸ©é˜µ: [4096, 4096]")
    print("              å‚æ•°é‡: 16,777,216")
    print()
    print("     å‰ªæ 25%: Linear(4096 â†’ 3072)  âœ… çœŸæ­£å‡å°‘å‚æ•°å’Œæ˜¾å­˜")
    print("              æƒé‡çŸ©é˜µ: [3072, 4096]")
    print("              å‚æ•°é‡: 12,582,912")
    print("              å‡å°‘: 4,194,304 (25%)")
    print()

    print("ğŸ”Ÿ  MetaPruner æ˜¯ä»€ä¹ˆï¼Ÿ")
    print("-" * 80)
    print("   MetaPruner æ˜¯ Torch-Pruning åº“çš„æ ¸å¿ƒå‰ªæå™¨")
    print("   ç‰¹ç‚¹:")
    print("     1. è‡ªåŠ¨è¿½è¸ªæ¨¡å—ä¹‹é—´çš„ä¾èµ–å…³ç³»")
    print("     2. ç¡®ä¿å‰ªæåæ¨¡å‹ç»“æ„ä¸€è‡´æ€§")
    print("     3. æ”¯æŒå„ç§é‡è¦æ€§è¯„ä¼°æ–¹æ³•ï¼ˆTaylorã€L1ã€L2ç­‰ï¼‰")
    print()

    print("1ï¸âƒ£1ï¸âƒ£  MetaPruner å·¥ä½œæµç¨‹:")
    print("-" * 80)
    print()
    print("   ç¬¬1æ­¥: æ„å»ºä¾èµ–å›¾ï¼ˆDependency Graphï¼‰")
    print("   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    print("     è¾“å…¥: forward_promptsï¼ˆç¤ºä¾‹è¾“å…¥å¼ é‡ï¼‰")
    print("     è¿‡ç¨‹: æ‰§è¡Œä¸€æ¬¡å‰å‘ä¼ æ’­ï¼Œè®°å½•æ‰€æœ‰æ¨¡å—çš„è¾“å…¥è¾“å‡ºå…³ç³»")
    print("     è¾“å‡º: ä¾èµ–å›¾ï¼Œè®°å½•å“ªäº›æ¨¡å—çš„è¾“å‡ºè¿æ¥åˆ°å“ªäº›æ¨¡å—çš„è¾“å…¥")
    print()
    print("     ç¤ºä¾‹ï¼ˆLayer 3ï¼‰:")
    print("       ")
    print("       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print("       â”‚          Layer 3 Self-Attention            â”‚")
    print("       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
    print("       ")
    print("       è¾“å…¥: hidden_states [batch, seq_len, 4096]")
    print("          â”‚")
    print("          â”œâ”€â”€â†’ q_proj: Linear(4096 â†’ 4096)")
    print("          â”‚")
    print("          â”œâ”€â”€â†’ k_proj: Linear(4096 â†’ 1024) â† root module")
    print("          â”‚")
    print("          â””â”€â”€â†’ v_proj: Linear(4096 â†’ 1024)")
    print("          ")
    print("          â†’ Attention è®¡ç®—")
    print("          ")
    print("          â†’ o_proj: Linear(4096 â†’ 4096)")
    print()

    print("   ç¬¬2æ­¥: è®¡ç®—é€šé“é‡è¦æ€§")
    print("   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    print("     å¯¹äºæ¯ä¸ªè¦å‰ªæçš„æ¨¡å—:")
    print("       - Taylor é‡è¦æ€§: importance = |âˆ‚L/âˆ‚W Ã— W|")
    print("       - L1 é‡è¦æ€§:     importance = |W|")
    print("       - L2 é‡è¦æ€§:     importance = ||W||â‚‚")
    print()
    print("     ä¸ºæ¯ä¸ªè¾“å‡ºé€šé“è®¡ç®—é‡è¦æ€§åˆ†æ•°")
    print()
    print("     ç¤ºä¾‹ï¼ˆLayer 5 çš„ k_projï¼‰:")
    print("       k_proj æƒé‡: [1024, 4096]  (1024ä¸ªè¾“å‡ºé€šé“)")
    print()
    print("       è®¡ç®—æ¯ä¸ªè¾“å‡ºé€šé“çš„é‡è¦æ€§:")
    print("         Channel 0:   importance = 0.523")
    print("         Channel 1:   importance = 0.891")
    print("         Channel 2:   importance = 0.156  â† ä¸é‡è¦")
    print("         ...")
    print("         Channel 1023: importance = 0.734")
    print()

    print("   ç¬¬3æ­¥: é€‰æ‹©è¦å‰ªæçš„é€šé“")
    print("   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    print("     å¯¹äºæ¯ä¸ªæ¨¡å— (æ ¹æ® ch_sparsity_dict):")
    print()
    print("       ä¾‹å¦‚: Layer 5 çš„ k_proj, å‰ªæç‡ = 0.2629")
    print()
    print("         åŸå§‹é€šé“æ•°: 1024")
    print("         ä¿ç•™é€šé“æ•°: 1024 Ã— (1 - 0.2629) â‰ˆ 755")
    print("         å‰ªæé€šé“æ•°: 1024 - 755 = 269")
    print()
    print("         é€‰æ‹©é‡è¦æ€§æœ€ä½çš„ 269 ä¸ªé€šé“è¿›è¡Œå‰ªæ")
    print("         ä¾‹å¦‚: [2, 15, 37, 89, ...]  (269ä¸ªé€šé“ç´¢å¼•)")
    print()

    print("   ç¬¬4æ­¥: ä¼ æ’­å‰ªæå†³ç­–")
    print("   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    print("     æ ¹æ®ä¾èµ–å›¾ï¼Œè‡ªåŠ¨ä¼ æ’­å‰ªæ:")
    print()
    print("       Layer 5 ç¤ºä¾‹:")
    print()
    print("         k_proj è¾“å‡º: 1024 â†’ 755 é€šé“")
    print("           â†“")
    print("         (GQA: q_proj å’Œ k_proj çš„æ¯”ä¾‹å…³ç³»)")
    print("           â†“")
    print("         q_proj è¾“å‡º: 4096 â†’ 3020 é€šé“  (4:1 æ¯”ä¾‹)")
    print("         v_proj è¾“å‡º: 1024 â†’ 755 é€šé“   (1:1 æ¯”ä¾‹)")
    print("           â†“")
    print("         o_proj è¾“å…¥: 4096 â†’ 3020 é€šé“  (å¿…é¡»åŒ¹é…)")
    print()
    print("       è¿™ä¿è¯äº† Attention æœºåˆ¶çš„ç»´åº¦ä¸€è‡´æ€§ï¼")
    print()
    print("       åŒç†ï¼ŒMLP çš„ä¼ æ’­:")
    print("         gate_proj è¾“å‡º: 14336 â†’ 10570 é€šé“")
    print("         up_proj   è¾“å‡º: 14336 â†’ 10570 é€šé“")
    print("         down_proj è¾“å…¥: 14336 â†’ 10570 é€šé“")
    print()

    print("   ç¬¬5æ­¥: ç‰©ç†æ‰§è¡Œå‰ªæ")
    print("   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    print("     å¯¹äºæ¯ä¸ªè¢«æ ‡è®°å‰ªæçš„é€šé“:")
    print("       1. ä»æƒé‡çŸ©é˜µä¸­åˆ é™¤å¯¹åº”çš„è¡Œ/åˆ—")
    print("       2. æ›´æ–°æ¨¡å—çš„ in_features å’Œ out_features")
    print("       3. é‡Šæ”¾æ˜¾å­˜")
    print()
    print("     ç¤ºä¾‹:")
    print("       åŸå§‹: k_proj = Linear(in=4096, out=1024)")
    print("         weight.shape = [1024, 4096]")
    print("         å‚æ•°é‡ = 4,194,304")
    print("         æ˜¾å­˜å ç”¨ (FP16) = 8.4 MB")
    print()
    print("       å‰ªæå: k_proj = Linear(in=4096, out=755)")
    print("         weight.shape = [755, 4096]")
    print("         å‚æ•°é‡ = 3,092,480")
    print("         æ˜¾å­˜å ç”¨ (FP16) = 6.2 MB")
    print()
    print("         å‡å°‘å‚æ•° = 1,101,824 (26.29%)")
    print("         å‡å°‘æ˜¾å­˜ = 2.2 MB")
    print()

    print("1ï¸âƒ£2ï¸âƒ£  è¿­ä»£å¼å‰ªæï¼ˆIterative Pruningï¼‰")
    print("-" * 80)
    print("   ä¸ºä»€ä¹ˆéœ€è¦å¤šæ¬¡è¿­ä»£ï¼Ÿ")
    print("     - ä¸€æ¬¡æ€§å¤§å¹…å‰ªæä¼šä¸¥é‡æŸå®³æ¨¡å‹æ€§èƒ½")
    print("     - é€æ­¥å‰ªæå…è®¸æ¨¡å‹åœ¨æ¯æ­¥åé‡æ–°è®¡ç®—é‡è¦æ€§")
    print()
    print("   ç¤ºä¾‹ï¼ˆiterative_steps=1ï¼Œç›®æ ‡å‰ªæç‡25%ï¼‰:")
    print("     è¿­ä»£ 1: ç›´æ¥å‰ªæ 25%")
    print()
    print("   ç¤ºä¾‹ï¼ˆiterative_steps=3ï¼Œç›®æ ‡å‰ªæç‡25%ï¼‰:")
    print("     è¿­ä»£ 1: å‰ªæ 8.33%  â†’ é‡æ–°è®¡ç®—é‡è¦æ€§")
    print("     è¿­ä»£ 2: å‰ªæ 8.33%  â†’ é‡æ–°è®¡ç®—é‡è¦æ€§")
    print("     è¿­ä»£ 3: å‰ªæ 8.34%")
    print("     æ€»è®¡:   25%")
    print()
    print("   ä»æ—¥å¿—çœ‹ï¼Œæ‚¨ä½¿ç”¨ iterative_steps=1ï¼ˆä¸€æ¬¡æ€§å‰ªæï¼‰")
    print()

    print("1ï¸âƒ£3ï¸âƒ£  å‰ªæå™¨é…ç½®å‚æ•°ï¼ˆllama3_unbalanced_pruning.py:272-289ï¼‰")
    print("-" * 80)
    print("   kwargs = {")
    print("       'importance': imp,              # Taylor/L1/L2 é‡è¦æ€§")
    print("       'global_pruning': False,        # ä¸ä½¿ç”¨å…¨å±€å‰ªæ")
    print("       'iterative_steps': 1,           # è¿­ä»£æ¬¡æ•°")
    print("       'ch_sparsity': 0.25,            # é»˜è®¤å‰ªæç‡ï¼ˆå¤‡ç”¨ï¼‰")
    print("       'ch_sparsity_dict': {...},      # â­ æ¯ä¸ªæ¨¡å—çš„å‰ªæç‡")
    print("       'ignored_layers': [],           # å¿½ç•¥çš„å±‚")
    print("       'consecutive_groups': {...},    # è¿ç»­åˆ†ç»„çº¦æŸï¼ˆGQAï¼‰")
    print("       'root_instances': [             # å‰ªæå…¥å£æ¨¡å—")
    print("           model.layers[3].self_attn.k_proj,")
    print("           model.layers[3].mlp.gate_proj,")
    print("           ...,")
    print("           model.layers[29].self_attn.k_proj,")
    print("           model.layers[29].mlp.gate_proj,")
    print("       ]")
    print("   }")
    print()
    print("   pruner = tp.pruner.MetaPruner(model, forward_prompts, **kwargs)")
    print("   pruner.step()  # æ‰§è¡Œå‰ªæ")
    print()


def explain_why_3_to_29():
    """
    æ€»ç»“ï¼šä¸ºä»€ä¹ˆåªå‰ªæ3-29å±‚
    """
    print("=" * 80)
    print("ğŸ¯ æ ¸å¿ƒé—®é¢˜ï¼šä¸ºä»€ä¹ˆåªå‰ªæ 3-29 å±‚ï¼Ÿ")
    print("=" * 80)
    print()

    print("ç­”æ¡ˆ:")
    print("-" * 80)
    print()
    print("  1ï¸âƒ£  æ­¥éª¤2 ç¡®å®è®¡ç®—äº†æ‰€æœ‰å±‚ï¼ˆ0-31ï¼‰çš„å‰ªæç‡")
    print("      åŸºäºå±‚é‡è¦æ€§è¯„ä¼°ï¼Œæ¯å±‚éƒ½æœ‰ä¸€ä¸ªå‰ªæç‡")
    print()
    print("  2ï¸âƒ£  æ­¥éª¤3 é€šè¿‡å‚æ•°è¿‡æ»¤ï¼Œåªä¿ç•™ 3-29 å±‚")
    print("      ä»£ç  (line 222-228):")
    print("        pruning_layers = set(range(3, 30)) | set(range(3, 30))")
    print("        â†’ ç»“æœ: [3, 4, 5, ..., 29]")
    print()
    print("  3ï¸âƒ£  ä¸ºä»€ä¹ˆè¦è¿‡æ»¤ï¼Ÿä¿æŠ¤å…³é”®å±‚ï¼")
    print("      ğŸ›¡ï¸  Layer 0-2:  åº•å±‚ç‰¹å¾æå–ï¼Œä¸å‰ªæ")
    print("      âœ‚ï¸  Layer 3-29: ä¸­é—´å±‚ï¼Œæ ¹æ®é‡è¦æ€§å‰ªæ")
    print("      ğŸ›¡ï¸  Layer 30-31: é«˜å±‚è¯­ä¹‰ï¼Œä¸å‰ªæ")
    print()
    print("  4ï¸âƒ£  è¿™æ˜¯ä¸€ä¸ªç»éªŒæ€§çš„è®¾è®¡é€‰æ‹©")
    print("      - å€Ÿé‰´äº†å¾ˆå¤š LLM å‰ªæè®ºæ–‡çš„åšæ³•")
    print("      - åœ¨å‰ªæç‡å’Œæ€§èƒ½ä¹‹é—´å–å¾—å¹³è¡¡")
    print("      - æ‚¨å¯ä»¥é€šè¿‡ä¿®æ”¹å‚æ•°æ¥è°ƒæ•´:")
    print("        --block_attention_layer_start 0  â† ä»ç¬¬0å±‚å¼€å§‹")
    print("        --block_attention_layer_end 32    â† åˆ°ç¬¬31å±‚ç»“æŸ")
    print()

    print("å¯¹æ¯”:")
    print("-" * 80)
    print()
    print("  åœºæ™¯A: å‰ªææ‰€æœ‰å±‚ï¼ˆ0-31ï¼‰")
    print("    ä¼˜ç‚¹: æ›´é«˜çš„å‚æ•°å‡å°‘ç‡")
    print("    ç¼ºç‚¹: PPL æ˜¾è‘—ä¸Šå‡ï¼Œæ€§èƒ½ä¸‹é™æ˜æ˜¾")
    print()
    print("  åœºæ™¯B: åªå‰ªæ 3-29 å±‚ï¼ˆå½“å‰æ–¹æ¡ˆï¼‰")
    print("    ä¼˜ç‚¹: ä¿æŒè¾ƒå¥½æ€§èƒ½ï¼ŒPPL ä¸Šå‡è¾ƒå°")
    print("    ç¼ºç‚¹: å‚æ•°å‡å°‘ç‡ç•¥ä½")
    print()
    print("  ä»æ‚¨çš„æ—¥å¿—çœ‹:")
    print("    å®é™…å‰ªæç‡: 17.19% (ç›®æ ‡ 25%)")
    print("    â†’ å› ä¸ºä¿æŠ¤äº† 5 å±‚ï¼Œå®é™…å‰ªæçš„å±‚æ•°å‡å°‘")
    print("    â†’ å¦‚æœå‰ªææ‰€æœ‰å±‚ï¼Œå®é™…å‰ªæç‡ä¼šæ›´æ¥è¿‘ 25%")
    print()


def main():
    import argparse
    parser = argparse.ArgumentParser(description='åˆ†æéå‡è¡¡ç»“æ„åŒ–å‰ªæçš„è¯¦ç»†è¿‡ç¨‹')
    parser.add_argument('--config', type=str,
                       default='prune_log/llama_unbalanced_prune/layer_importance_config.json',
                       help='å±‚é‡è¦æ€§é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--block_attention_layer_start', type=int, default=3)
    parser.add_argument('--block_attention_layer_end', type=int, default=30)
    parser.add_argument('--block_mlp_layer_start', type=int, default=3)
    parser.add_argument('--block_mlp_layer_end', type=int, default=30)

    args = parser.parse_args()

    print()
    print("ğŸ” éå‡è¡¡ç»“æ„åŒ–å‰ªæè¯¦ç»†åˆ†æ")
    print("=" * 80)
    print()

    # åŠ è½½é…ç½®
    print("åŠ è½½å‰ªæé…ç½®...")
    layer_pruning_rates = load_pruning_config(args.config)
    print(f"âœ… æˆåŠŸåŠ è½½ {len(layer_pruning_rates)} å±‚çš„å‰ªæç‡é…ç½®")
    print()

    # åˆ†æå±‚è¿‡æ»¤
    filtered_rates, pruning_layers = analyze_layer_filtering(
        layer_pruning_rates,
        args.block_attention_layer_start,
        args.block_attention_layer_end,
        args.block_mlp_layer_start,
        args.block_mlp_layer_end
    )

    # åˆ†æ ch_sparsity_dict åˆ›å»º
    analyze_ch_sparsity_dict_creation(
        filtered_rates,
        prune_attention=True,
        prune_mlp=True
    )

    # è§£é‡Š MetaPruner å·¥ä½œæµç¨‹
    explain_metapruner_workflow()

    # å›ç­”æ ¸å¿ƒé—®é¢˜
    explain_why_3_to_29()

    print("=" * 80)
    print("âœ… åˆ†æå®Œæˆï¼")
    print("=" * 80)
    print()


if __name__ == '__main__':
    main()
