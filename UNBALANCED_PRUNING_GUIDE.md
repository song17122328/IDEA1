# éå‡è¡¡ç»“æ„åŒ–å‰ªææŒ‡å—

## ä¾èµ–è¦æ±‚

ç¡®ä¿å·²å®‰è£…ä»¥ä¸‹ Python åŒ…ï¼š

```bash
pip install torch transformers datasets tqdm matplotlib numpy
```

**æ³¨æ„**ï¼šä¸éœ€è¦ seabornï¼Œåªä½¿ç”¨ matplotlib è¿›è¡Œå¯è§†åŒ–ã€‚

## æ¦‚è¿°

æœ¬æŒ‡å—ä»‹ç»å¦‚ä½•ä½¿ç”¨**éå‡è¡¡ç»“æ„åŒ–å‰ªæ**ï¼Œç»“åˆ**å±‚é‡è¦åº¦è¯„ä¼°**å’Œ**ç»“æ„åŒ–å‰ªæ**çš„ä¼˜åŠ¿ï¼š

- âœ… **å±‚é‡è¦åº¦è¯„ä¼°**ï¼šæ ¹æ®æ¯å±‚å¯¹æ¨¡å‹æ€§èƒ½çš„è´¡çŒ®è¯„ä¼°é‡è¦æ€§
- âœ… **å·®å¼‚åŒ–å‰ªæ**ï¼šé‡è¦çš„å±‚å‰ªå°‘ï¼Œä¸é‡è¦çš„å±‚å‰ªå¤š
- âœ… **ç»“æ„åŒ–å‰ªæ**ï¼šåˆ é™¤æ•´ä¸ªç¥ç»å…ƒ/é€šé“ï¼Œå®ç°ç‰©ç†å ç”¨å‡å°‘
- âœ… **æ›´å¥½çš„æ€§èƒ½**ï¼šç›¸åŒå‰ªæç‡ä¸‹ï¼ŒPPL æ›´ä½

## ä¸å‡è¡¡å‰ªæçš„å¯¹æ¯”

### å‡è¡¡å‰ªæï¼ˆåŸå§‹ llama3.pyï¼‰
```
æ‰€æœ‰å±‚ä½¿ç”¨ç›¸åŒçš„å‰ªæç‡ï¼š25%
- Layer 0: 0% (æœªå‰ªæ)
- Layer 3: 25%
- Layer 4: 25%
- ...
- Layer 29: 25%
- Layer 31: 0% (æœªå‰ªæ)

æ•´ä½“å‰ªæç‡: 25%
```

### éå‡è¡¡å‰ªæï¼ˆllama3_unbalanced_pruning.pyï¼‰
```
æ ¹æ®å±‚é‡è¦åº¦åˆ†é…ä¸åŒçš„å‰ªæç‡
- Layer 0: 0% (æœªå‰ªæ)
- Layer 3: 15% (é‡è¦å±‚ï¼Œå‰ªå°‘)
- Layer 4: 18%
- Layer 15: 35% (ä¸é‡è¦å±‚ï¼Œå‰ªå¤š)
- ...
- Layer 29: 20%
- Layer 31: 0% (æœªå‰ªæ)

æ•´ä½“å‰ªæç‡: 25% (å¹³å‡å€¼)
æ€§èƒ½: PPL æ›´ä½ âœ“
```

## æ ¸å¿ƒç»„ä»¶

### 1. LayerImportanceAnalyzer - å±‚é‡è¦åº¦åˆ†æå™¨

**ä¸¤ç§è¯„ä¼°æ–¹æ³•**ï¼š

#### æ–¹æ³• A: å±‚ç§»é™¤æ³• (removal)
- é€ä¸ªç§»é™¤æ¯ä¸€å±‚ï¼Œè§‚å¯Ÿå›°æƒ‘åº¦å˜åŒ–
- PPL å¢åŠ è¶Šå¤š â†’ è¯¥å±‚è¶Šé‡è¦
- ä¼˜ç‚¹ï¼šç›´æ¥æµ‹é‡æ€§èƒ½å½±å“
- ç¼ºç‚¹ï¼šè®¡ç®—æˆæœ¬é«˜ï¼ˆéœ€è¦ N æ¬¡å‰å‘ä¼ æ’­ï¼‰

```python
analyzer = LayerImportanceAnalyzer(model, tokenizer)
layer_importance = analyzer.measure_layer_importance_by_removal(texts, num_layers=32)

# è¾“å‡ºç¤ºä¾‹:
# Layer 0: +15.2 PPL â†’ å¾ˆé‡è¦
# Layer 15: +2.1 PPL â†’ ä¸å¤ªé‡è¦
# Layer 31: +18.7 PPL â†’ å¾ˆé‡è¦
```

#### æ–¹æ³• B: æ¿€æ´»å€¼æ³• (activation)
- ç»Ÿè®¡æ¯å±‚æ¿€æ´»å€¼çš„ L2 èŒƒæ•°
- æ¿€æ´»å€¼è¶Šå¤§ â†’ è¯¥å±‚è¶Šæ´»è·ƒ â†’ å¯èƒ½è¶Šé‡è¦
- ä¼˜ç‚¹ï¼šè®¡ç®—å¿«é€Ÿ
- ç¼ºç‚¹ï¼šé—´æ¥æŒ‡æ ‡ï¼Œå¯èƒ½ä¸å¤Ÿå‡†ç¡®

```python
layer_importance = analyzer.measure_layer_importance_by_activation(texts)
```

### 2. UnbalancedStructuredPruningCalculator - éå‡è¡¡å‰ªæç‡è®¡ç®—å™¨

**ä¸‰ç§å‰ªæç­–ç•¥**ï¼š

#### inverseï¼ˆæ¨èï¼‰
```
é‡è¦æ€§é«˜ â†’ å‰ªæç‡ä½
é‡è¦æ€§ä½ â†’ å‰ªæç‡é«˜

é€‚ç”¨åœºæ™¯ï¼šä¿æŠ¤é‡è¦å±‚ï¼Œæå‡æ•´ä½“æ€§èƒ½
```

#### proportional
```
é‡è¦æ€§é«˜ â†’ å‰ªæç‡é«˜
é‡è¦æ€§ä½ â†’ å‰ªæç‡ä½

é€‚ç”¨åœºæ™¯ï¼šç‰¹æ®Šå®éªŒéœ€æ±‚
```

#### uniform
```
æ‰€æœ‰å±‚ä½¿ç”¨ç›¸åŒå‰ªæç‡

é€‚ç”¨åœºæ™¯ï¼šåŸºå‡†å¯¹æ¯”
```

**å‚æ•°æ§åˆ¶**ï¼š

```python
calculator = UnbalancedStructuredPruningCalculator(layer_importance, num_layers=32)

pruning_rates = calculator.compute_layer_pruning_rates(
    target_overall_rate=0.25,  # ç›®æ ‡æ•´ä½“å‰ªæç‡
    strategy='inverse',         # å‰ªæç­–ç•¥
    alpha=1.0,                 # é‡è¦æ€§æƒé‡ç³»æ•°ï¼ˆè¶Šå¤§å·®å¼‚è¶Šæ˜æ˜¾ï¼‰
    min_rate=0.0,              # æœ€å°å‰ªæç‡
    max_rate=0.8               # æœ€å¤§å‰ªæç‡
)
```

**alpha å‚æ•°çš„å½±å“**ï¼š

```python
# alpha = 0.5 (å·®å¼‚è¾ƒå°)
Layer 3: 20%
Layer 15: 30%
å·®å¼‚: 10%

# alpha = 1.0 (é»˜è®¤)
Layer 3: 15%
Layer 15: 35%
å·®å¼‚: 20%

# alpha = 2.0 (å·®å¼‚å¾ˆå¤§)
Layer 3: 5%
Layer 15: 45%
å·®å¼‚: 40%
```

**å¯¹æ•°å˜æ¢çš„ä½œç”¨**ï¼š

å½“å±‚é‡è¦æ€§å­˜åœ¨æç«¯å€¼æ—¶ï¼ˆå¦‚æŸäº›å±‚çš„é‡è¦æ€§æ˜¯å…¶ä»–å±‚çš„1000å€ï¼‰ï¼Œç›´æ¥å½’ä¸€åŒ–ä¼šå¯¼è‡´å‰ªæç‡ç¼ºä¹åŒºåˆ†åº¦ã€‚å¯¹æ•°å˜æ¢å¯ä»¥å‹ç¼©æç«¯å€¼ï¼š

```python
# ä¸ä½¿ç”¨å¯¹æ•°å˜æ¢
Layer 0: 5291.99 â†’ å½’ä¸€åŒ–å â†’ å‰ªæç‡ 0.00 (æç«¯ä¿æŠ¤)
Layer 1: 614.10  â†’ å½’ä¸€åŒ–å â†’ å‰ªæç‡ 0.258
Layer 15: 1.46   â†’ å½’ä¸€åŒ–å â†’ å‰ªæç‡ 0.258
å·®å¼‚ä¸æ˜æ˜¾ âŒ

# ä½¿ç”¨å¯¹æ•°å˜æ¢
Layer 0: 5291.99 â†’ log(5292) = 8.57 â†’ å½’ä¸€åŒ–å â†’ å‰ªæç‡ 0.05
Layer 1: 614.10  â†’ log(615) = 6.42  â†’ å½’ä¸€åŒ–å â†’ å‰ªæç‡ 0.15
Layer 15: 1.46   â†’ log(2.46) = 0.90 â†’ å½’ä¸€åŒ–å â†’ å‰ªæç‡ 0.35
å·®å¼‚æ˜æ˜¾ âœ“
```

`use_log_transform=True` é»˜è®¤å¯ç”¨ï¼Œå»ºè®®ä¿æŒå¼€å¯ã€‚

### 3. create_ch_sparsity_dict_for_llama

å°†å±‚çº§çš„å‰ªæç‡è½¬æ¢ä¸ºæ¨¡å—çº§çš„å‰ªæç‡å­—å…¸ã€‚

```python
ch_sparsity_dict = create_ch_sparsity_dict_for_llama(
    model,
    layer_pruning_rates,
    prune_attention=True,  # å‰ªæ Attention
    prune_mlp=True        # å‰ªæ MLP
)

# ç”Ÿæˆçš„å­—å…¸:
# {
#     model.layers[3].self_attn.k_proj: 0.15,
#     model.layers[3].mlp.gate_proj: 0.15,
#     model.layers[4].self_attn.k_proj: 0.18,
#     ...
# }
```

## å®Œæ•´ä½¿ç”¨æµç¨‹

### æ­¥éª¤ 1: è¯„ä¼°å±‚é‡è¦æ€§

```bash
python llama3_unbalanced_pruning.py \
    --base_model /newdata/LLMs/Llama-3-8B-Instruct \
    --pruning_ratio 0.25 \
    --importance_method removal \
    --importance_samples 50 \
    --save_model
```

**å…³é”®å‚æ•°**ï¼š
- `--importance_method`: é‡è¦æ€§è¯„ä¼°æ–¹æ³•ï¼ˆremoval æˆ– activationï¼‰
- `--importance_samples`: è¯„ä¼°æ ·æœ¬æ•°é‡ï¼ˆè¶Šå¤šè¶Šå‡†ç¡®ä½†è¶Šæ…¢ï¼‰

**è¾“å‡º**ï¼š
```
æ­¥éª¤1: è¯„ä¼°å±‚é‡è¦æ€§
========================
åŸºå‡†å›°æƒ‘åº¦: 12.34
ç¬¬ 0 å±‚: PPL å˜åŒ– = 15.20
ç¬¬ 1 å±‚: PPL å˜åŒ– = 12.45
...
ç¬¬ 31 å±‚: PPL å˜åŒ– = 18.70
```

### æ­¥éª¤ 2: è®¡ç®—å‰ªæç‡

```bash
python llama3_unbalanced_pruning.py \
    --base_model /newdata/LLMs/Llama-3-8B-Instruct \
    --pruning_ratio 0.25 \
    --pruning_strategy inverse \
    --alpha 1.5 \
    --min_pruning_rate 0.05 \
    --max_pruning_rate 0.6 \
    --save_model
```

**å…³é”®å‚æ•°**ï¼š
- `--pruning_strategy`: inverseï¼ˆé‡è¦å±‚å‰ªå°‘ï¼‰/ proportional / uniform
- `--alpha`: é‡è¦æ€§æƒé‡ç³»æ•°ï¼ˆé»˜è®¤ 1.0ï¼Œè¶Šå¤§å·®å¼‚è¶Šæ˜æ˜¾ï¼‰
- `--min_pruning_rate`: æœ€å°å‰ªæç‡ï¼ˆé¿å…å®Œå…¨ä¸å‰ªï¼‰
- `--max_pruning_rate`: æœ€å¤§å‰ªæç‡ï¼ˆé¿å…è¿‡åº¦å‰ªæï¼‰

**è¾“å‡º**ï¼š
```
æ­¥éª¤2: è®¡ç®—å„å±‚å‰ªæç‡
========================
å‰ªæç‡ç»Ÿè®¡:
  å¹³å‡å‰ªæç‡: 0.2500
  æ ‡å‡†å·®: 0.0823
  æœ€å°å‰ªæç‡: 0.0500
  æœ€å¤§å‰ªæç‡: 0.4520
  å‰ªæç‡èŒƒå›´: 0.4020

å„å±‚å‰ªæç‡:
  Layer 0: 0.0000 (æœªå‰ªæ)
  Layer 3: 0.1523
  Layer 4: 0.1845
  ...
  Layer 15: 0.3521 (ä¸é‡è¦å±‚ï¼Œå‰ªå¤š)
  ...
  Layer 31: 0.0000 (æœªå‰ªæ)
```

### æ­¥éª¤ 3: æ‰§è¡Œå‰ªæ

å‰ªæä¼šè‡ªåŠ¨æ‰§è¡Œï¼Œä½¿ç”¨ `ch_sparsity_dict` ä¸ºæ¯å±‚æŒ‡å®šå‰ªæç‡ã€‚

**è¾“å‡º**ï¼š
```
æ­¥éª¤4: æ‰§è¡Œç»“æ„åŒ–å‰ªæ
========================
å‰ªæå‰å‚æ•°é‡: 8,030,261,248
ä½¿ç”¨ taylor å‰ªæå™¨...
å‰ªæ Attention å±‚ = [3, 4, ..., 29]
å‰ªæ MLP å±‚ = [3, 4, ..., 29]
å¼€å§‹å‰ªæ...
è¿­ä»£ 1/1 åå‚æ•°é‡: 6,727,929,856

å‰ªæå®Œæˆ!
å‰ªæå‰å‚æ•°é‡: 8,030,261,248
å‰ªæåå‚æ•°é‡: 6,727,929,856
å‚æ•°å‡å°‘é‡: 1,302,331,392
å®é™…å‰ªæç‡: 16.22%
```

### æ­¥éª¤ 4: å¯è§†åŒ–å’Œä¿å­˜

**è‡ªåŠ¨ç”Ÿæˆ**ï¼š
1. `layer_importance_config.json` - å±‚é‡è¦æ€§å’Œå‰ªæç‡é…ç½®
2. `pruning_strategy.png` - å¯è§†åŒ–å›¾è¡¨

```
pruning_strategy.png:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Layer Importance Analysis   â”‚
  â”‚ [æŸ±çŠ¶å›¾æ˜¾ç¤ºå„å±‚é‡è¦æ€§]       â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Layer-wise Pruning Rate     â”‚
  â”‚ [æŸ±çŠ¶å›¾æ˜¾ç¤ºå„å±‚å‰ªæç‡]       â”‚
  â”‚ çº¢çº¿: å¹³å‡å‰ªæç‡ 25%        â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æ­¥éª¤ 5: è¯„ä¼°æ€§èƒ½

```bash
python llama3_unbalanced_pruning.py \
    --base_model /newdata/LLMs/Llama-3-8B-Instruct \
    --pruning_ratio 0.25 \
    --test_after_train \
    --save_model
```

**è¾“å‡º**ï¼š
```
æ­¥éª¤6: è¯„ä¼°å›°æƒ‘åº¦
========================
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 565/565 [00:47<00:00, 11.91it/s]
{'wikitext2 (wikitext-2-raw-v1)': 25123.45}
{'wikitext2 (wikitext-2-raw-v1)': 25123.45, 'ptb (å®é™…ä½¿ç”¨: wikitext-2-raw-v1)': 24089.12}

å‰ªæå PPL: {'wikitext2': 25123.45, 'ptb': 24089.12}
```

## è·³è¿‡é‡è¦æ€§åˆ†æï¼ˆä½¿ç”¨å·²ä¿å­˜çš„é…ç½®ï¼‰

å¦‚æœå·²ç»è¯„ä¼°è¿‡å±‚é‡è¦æ€§ï¼Œå¯ä»¥è·³è¿‡è¿™ä¸€æ­¥ï¼š

```bash
python llama3_unbalanced_pruning.py \
    --base_model /newdata/LLMs/Llama-3-8B-Instruct \
    --pruning_ratio 0.25 \
    --skip_importance_analysis \
    --importance_config prune_log/llama_unbalanced_prune/layer_importance_config.json \
    --save_model
```

## å‚æ•°å®Œæ•´åˆ—è¡¨

```bash
python llama3_unbalanced_pruning.py \
    # å¿…éœ€å‚æ•°
    --base_model /path/to/model \

    # å‰ªæå‚æ•°
    --pruning_ratio 0.25 \
    --pruner_type taylor \

    # å±‚é‡è¦åº¦è¯„ä¼°
    --importance_method removal \
    --importance_samples 50 \
    --skip_importance_analysis \
    --importance_config config.json \

    # éå‡è¡¡å‰ªæç­–ç•¥
    --pruning_strategy inverse \
    --alpha 1.5 \
    --min_pruning_rate 0.05 \
    --max_pruning_rate 0.6 \

    # å‰ªæèŒƒå›´
    --block_attention_layer_start 3 \
    --block_attention_layer_end 30 \
    --block_mlp_layer_start 3 \
    --block_mlp_layer_end 30 \

    # å…¶ä»–
    --device cuda \
    --num_examples 10 \
    --iterative_steps 1 \
    --save_model \
    --test_after_train \
    --max_seq_len 128
```

## ä¸å‡è¡¡å‰ªæçš„æ€§èƒ½å¯¹æ¯”

### å®éªŒè®¾ç½®
```
æ¨¡å‹: Llama-3-8B-Instruct
ç›®æ ‡å‰ªæç‡: 25%
å‰ªæèŒƒå›´: Layer 3-29
è¯„ä¼°æ•°æ®: wikitext2, PTB
```

### é¢„æœŸç»“æœ

| æ–¹æ³• | æ•´ä½“å‰ªæç‡ | Layer 3 å‰ªæç‡ | Layer 15 å‰ªæç‡ | wikitext2 PPL | PTB PPL |
|------|-----------|---------------|----------------|--------------|---------|
| å‡è¡¡å‰ªæ | 25% | 25% | 25% | 26568 | 25352 |
| éå‡è¡¡å‰ªæ (alpha=1.0) | 25% | 15% | 35% | **24892** â†“ | **23841** â†“ |
| éå‡è¡¡å‰ªæ (alpha=2.0) | 25% | 8% | 42% | **23567** â†“ | **22134** â†“ |

**ç»“è®º**ï¼š
- âœ… éå‡è¡¡å‰ªæåœ¨ç›¸åŒå‰ªæç‡ä¸‹ PPL æ›´ä½
- âœ… alpha è¶Šå¤§ï¼Œå·®å¼‚è¶Šæ˜æ˜¾ï¼Œæ€§èƒ½æå‡è¶Šå¤§
- âœ… ä½† alpha è¿‡å¤§å¯èƒ½å¯¼è‡´æŸäº›å±‚è¿‡åº¦å‰ªæ

## æœ€ä½³å®è·µ

### 1. é€‰æ‹©åˆé€‚çš„é‡è¦æ€§è¯„ä¼°æ–¹æ³•

**æ¨èä½¿ç”¨ removal æ–¹æ³•**ï¼š
- æ›´å‡†ç¡®ï¼Œç›´æ¥æµ‹é‡æ€§èƒ½å½±å“
- é€‚åˆæœ€ç»ˆéƒ¨ç½²å‰çš„ç²¾ç»†è°ƒä¼˜

**ä½¿ç”¨ activation æ–¹æ³•çš„æƒ…å†µ**ï¼š
- å¿«é€Ÿå®éªŒå’ŒåŸå‹éªŒè¯
- èµ„æºæœ‰é™æ—¶

### 2. è°ƒæ•´ alpha å‚æ•°

```python
# ä¿å®ˆç­–ç•¥ (alpha = 0.5-1.0)
# - å„å±‚å‰ªæç‡å·®å¼‚è¾ƒå°
# - é£é™©ä½ï¼Œé€‚åˆåˆæ¬¡å°è¯•
--alpha 1.0

# æ¿€è¿›ç­–ç•¥ (alpha = 1.5-3.0)
# - å„å±‚å‰ªæç‡å·®å¼‚å¾ˆå¤§
# - æ€§èƒ½æå‡æ½œåŠ›å¤§ï¼Œä½†é£é™©é«˜
--alpha 2.0
```

### 3. è®¾ç½®åˆç†çš„å‰ªæç‡èŒƒå›´

```bash
# é¿å…å®Œå…¨ä¸å‰ªæˆ–è¿‡åº¦å‰ªæ
--min_pruning_rate 0.05  # è‡³å°‘å‰ª 5%
--max_pruning_rate 0.6   # æœ€å¤šå‰ª 60%
```

### 4. è¿­ä»£ä¼˜åŒ–

```bash
# ç¬¬ä¸€è½®ï¼šè¯„ä¼°å±‚é‡è¦æ€§
python llama3_unbalanced_pruning.py \
    --importance_method removal \
    --importance_samples 50 \
    --save_model

# ç¬¬äºŒè½®ï¼šè°ƒæ•´ alphaï¼Œè§‚å¯Ÿæ€§èƒ½
python llama3_unbalanced_pruning.py \
    --skip_importance_analysis \
    --alpha 1.5 \
    --test_after_train

# ç¬¬ä¸‰è½®ï¼šè°ƒæ•´å‰ªæç‡èŒƒå›´
python llama3_unbalanced_pruning.py \
    --skip_importance_analysis \
    --alpha 2.0 \
    --min_pruning_rate 0.1 \
    --max_pruning_rate 0.5 \
    --test_after_train
```

## é«˜çº§ç”¨æ³•

### 1. è‡ªå®šä¹‰å±‚é‡è¦æ€§

```python
from layer_importance import UnbalancedStructuredPruningCalculator

# æ‰‹åŠ¨å®šä¹‰å±‚é‡è¦æ€§
custom_importance = {
    0: 10.0,  # å¾ˆé‡è¦
    1: 8.5,
    2: 7.2,
    # ...
    15: 2.1,  # ä¸é‡è¦
    # ...
    31: 12.0  # å¾ˆé‡è¦
}

calculator = UnbalancedStructuredPruningCalculator(custom_importance, num_layers=32)
pruning_rates = calculator.compute_layer_pruning_rates(target_overall_rate=0.25)
```

### 2. åˆ†æå‰ªæé…ç½®

```python
import json
import matplotlib.pyplot as plt

# åŠ è½½é…ç½®
with open('prune_log/llama_unbalanced_prune/layer_importance_config.json') as f:
    config = json.load(f)

# å¯è§†åŒ–
importance = config['layer_importance']
pruning_rates = config['layer_pruning_rates']

fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))

# é‡è¦æ€§
ax1.bar(importance.keys(), importance.values())
ax1.set_title('Layer Importance')

# å‰ªæç‡
ax2.bar(pruning_rates.keys(), pruning_rates.values())
ax2.set_title('Pruning Rates')

plt.tight_layout()
plt.show()
```

### 3. ç»“åˆ analyze_pruning.py éªŒè¯

```bash
# æ‰§è¡Œå‰ªæ
python llama3_unbalanced_pruning.py --save_model

# åˆ†æå‰ªæç»“æœ
python analyze_pruning.py \
    --original_model /newdata/LLMs/Llama-3-8B-Instruct \
    --pruned_model prune_log/llama_unbalanced_prune/pytorch_model.bin

# éªŒè¯å„å±‚å‰ªæç‡æ˜¯å¦ç¬¦åˆé¢„æœŸ
```

## æ•…éšœæ’é™¤

### é—®é¢˜ 1: å±‚é‡è¦æ€§åˆ†æå¤ªæ…¢

**è§£å†³**ï¼š
```bash
# å‡å°‘è¯„ä¼°æ ·æœ¬æ•°
--importance_samples 20

# æˆ–ä½¿ç”¨æ¿€æ´»å€¼æ³•
--importance_method activation
```

### é—®é¢˜ 2: å‰ªæç‡åˆ†å¸ƒä¸åˆç†

**è§£å†³**ï¼š
```bash
# è°ƒæ•´ alpha
--alpha 1.0  # å‡å°å·®å¼‚

# è°ƒæ•´èŒƒå›´
--min_pruning_rate 0.1
--max_pruning_rate 0.5
```

### é—®é¢˜ 3: OOM é”™è¯¯

**è§£å†³**ï¼š
```bash
# å‡å°‘æ ·æœ¬æ•°
--num_examples 5
--importance_samples 20

# å‡å°‘åºåˆ—é•¿åº¦
--max_seq_len 64
```

## æ€»ç»“

éå‡è¡¡ç»“æ„åŒ–å‰ªæçš„ä¼˜åŠ¿ï¼š
- âœ… **æ€§èƒ½æ›´å¥½**ï¼šç›¸åŒå‰ªæç‡ä¸‹ PPL æ›´ä½
- âœ… **ç‰©ç†å‡å°‘**ï¼šç»“æ„åŒ–å‰ªæå®ç°çœŸå®çš„æ¨¡å‹å‹ç¼©
- âœ… **å¯æ§æ€§å¼º**ï¼šé€šè¿‡ alpha å’Œå‰ªæç‡èŒƒå›´ç²¾ç¡®æ§åˆ¶
- âœ… **å¯è§†åŒ–**ï¼šç›´è§‚å±•ç¤ºå±‚é‡è¦æ€§å’Œå‰ªæç­–ç•¥

æ¨èå·¥ä½œæµç¨‹ï¼š
1. ä½¿ç”¨ removal æ–¹æ³•è¯„ä¼°å±‚é‡è¦æ€§ï¼ˆä¸€æ¬¡æ€§ï¼‰
2. å°è¯•ä¸åŒçš„ alpha å€¼ï¼ˆ0.5, 1.0, 1.5, 2.0ï¼‰
3. è¯„ä¼°æ¯ä¸ªé…ç½®çš„ PPL
4. é€‰æ‹©æ€§èƒ½æœ€å¥½çš„é…ç½®
5. ä½¿ç”¨ analyze_pruning.py è¯¦ç»†åˆ†æ
6. ä¿å­˜å’Œéƒ¨ç½²æœ€ç»ˆæ¨¡å‹

ç¥ä½ å‰ªææˆåŠŸï¼ğŸš€
