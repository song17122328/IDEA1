# GQA-Aware Pruning å®ç°æŒ‡å—

## ğŸ¯ æ ¸å¿ƒæ€æƒ³

**é—®é¢˜**ï¼šå½“å‰å‰ªææµç¨‹çš„è‡´å‘½ç¼ºé™·
```python
# å½“å‰æ–¹æ³•ï¼šç®€å•æˆªæ–­
å‰ªæå: Q=30 heads, KV=6 heads (5:1)
åå¤„ç†: æˆªæ–­æœ€å6ä¸ªQ heads â†’ Q=24 heads, KV=6 heads (4:1)
é—®é¢˜: æˆªæ–­çš„6ä¸ªQ headså¯èƒ½æ˜¯é‡è¦çš„ï¼
ç»“æœ: PPLé£™å‡åˆ°71ä¸‡
```

**è§£å†³æ–¹æ¡ˆ**ï¼šGQA-aware Taylor importance
```python
# æ–°æ–¹æ³•ï¼šåŸºäºimportanceçš„ç»„çº§å‰ªæ
1. å°†"4ä¸ªQ heads + 1ä¸ªKV head"è§†ä¸ºä¸€ä¸ªGQAç»„
2. è®¡ç®—æ¯ä¸ªGQAç»„çš„æ€»Taylor importance
3. ä¿ç•™importanceæœ€é«˜çš„Nä¸ªå®Œæ•´ç»„
4. å‰ªæimportanceæœ€ä½çš„ç»„
ç»“æœ: ä¿æŒ4:1æ¯”ä¾‹ + ä¿ç•™é‡è¦çš„heads + ä¿æŒè¯­ä¹‰å¯¹é½
```

---

## ğŸ“Š ä¸‰ä¸ªå…³é”®é—®é¢˜çš„å›ç­”

### é—®é¢˜1ï¼šæ¯å±‚å‰ªæç‡å¦‚ä½•ç¡®å®šï¼Ÿ

âœ… **å½“å‰æ–¹æ³•å¾ˆå¥½ï¼Œæ— éœ€æ”¹åŠ¨**

```python
# layer_importance.py
layer_importance = compute_layer_importance_removal(model, example_prompts, ...)
layer_pruning_rates = {0: 0.25, 1: 0.30, 2: 0.20, ...}  # per-layer rates
```

### é—®é¢˜2ï¼šAttentionå’ŒMLPéƒ½ç”¨Taylorå—ï¼Ÿ

âœ… **æ˜¯çš„ï¼Œä½†æ–¹å¼ä¸åŒ**

#### MLPï¼šé€šé“çº§åˆ«Taylor
```python
# æ¯ä¸ªé€šé“çš„importance
salience = layer.weight * layer.weight.grad
channel_imp = salience.abs().sum(1)  # [num_channels]

# é€‰æ‹©importanceæœ€ä½çš„é€šé“å‰ªæ‰
```

#### Attentionï¼ˆå½“å‰é—®é¢˜ï¼‰ï¼šé€šé“çº§åˆ«Taylor + ä¾èµ–å›¾ä¼ æ’­
```python
# å½“å‰æ–¹æ³• (hf_llama_pruner.py:310-327)
local_norm = 0
local_norm += salience[o_proj].abs().sum(1)  # output channels
local_norm += salience[q_proj].abs().sum(0)  # input channels
local_norm += salience[k_proj].abs().sum(0)
local_norm += salience[v_proj].abs().sum(0)

# é—®é¢˜ï¼šè¿™æ˜¯"é€šé“çº§åˆ«"çš„importanceï¼Œä¸ç†è§£GQAç»“æ„
# k_projå‰ª2ä¸ªheads â†’ q_projä¾èµ–å›¾ä¼ æ’­å‰ª2ä¸ªheadsï¼ˆåº”è¯¥å‰ª8ä¸ªï¼‰
```

#### Attentionï¼ˆæ–°æ–¹æ³•ï¼‰ï¼šGQAç»„çº§åˆ«Taylor
```python
# æ–°æ–¹æ³• (gqa_aware_pruning.py)
# è®¡ç®—æ¯ä¸ªGQAç»„çš„importance
for kv_idx in range(num_kv_heads):
    q_start = kv_idx * 4
    q_end = q_start + 4

    group_imp = 0
    # 4ä¸ªQ headsçš„contribution
    group_imp += q_head_imp[q_start:q_end].sum()
    group_imp += o_head_imp[q_start:q_end].sum()
    # 1ä¸ªKV headçš„contribution
    group_imp += k_head_imp[kv_idx]
    group_imp += v_head_imp[kv_idx]

    group_importance[kv_idx] = group_imp

# é€‰æ‹©importanceæœ€ä½çš„å®Œæ•´GQAç»„å‰ªæ‰
```

### é—®é¢˜3ï¼šåå¤„ç†èƒ½å¦åŸºäºimportanceï¼Ÿ

âœ… **å®Œå…¨å¯ä»¥ï¼è¿™æ­£æ˜¯æ–°æ–¹æ³•çš„æ ¸å¿ƒ**

```python
# æ—§æ–¹æ³•ï¼šåå¤„ç†ç®€å•æˆªæ–­ï¼ˆä¸è€ƒè™‘importanceï¼‰
layer.self_attn.q_proj.weight.data = \
    layer.self_attn.q_proj.weight.data[:target_q_channels, :]
# â†‘ ä¸¢å¼ƒæœ€å6ä¸ªQ headsï¼ˆå¯èƒ½æ˜¯é‡è¦çš„ï¼‰

# æ–°æ–¹æ³•ï¼šåŸºäºimportanceé€‰æ‹©å®Œæ•´GQAç»„
keep_indices, prune_indices = select_gqa_groups_to_prune(group_imp, target_num_kv_heads)
# keep_indices = [0, 2, 3, 5, 6, 7] (importanceæœ€é«˜çš„6ä¸ªç»„)
# prune_indices = [1, 4] (importanceæœ€ä½çš„2ä¸ªç»„)

prune_attention_by_gqa_groups(layer, keep_indices, head_dim=128, gqa_ratio=4)
# â†‘ ä¿ç•™å®Œæ•´çš„GQAç»„ï¼Œä¿æŒè¯­ä¹‰å¯¹é½
```

---

## ğŸ”§ å®ç°æ–¹æ¡ˆ

### æ–¹æ¡ˆ1ï¼šå®Œå…¨æ›¿ä»£torch_pruningï¼ˆæ¨èï¼‰

**ä¼˜ç‚¹**ï¼š
- å®Œå…¨æ§åˆ¶å‰ªæè¿‡ç¨‹
- ç¡®ä¿GQAæ¯”ä¾‹æ­£ç¡®
- åŸºäºimportanceï¼Œä¸ä¼šç ´åè¯­ä¹‰

**æ­¥éª¤**ï¼š

1. **ä¸ºæ¯å±‚è®¡ç®—å‰ªæç‡**ï¼ˆä¿æŒä¸å˜ï¼‰
```python
layer_pruning_rates = compute_pruning_rates_from_importance(...)
```

2. **MLPä½¿ç”¨torch_pruning**ï¼ˆä¿æŒä¸å˜ï¼‰
```python
# MLPæ²¡æœ‰GQAé—®é¢˜ï¼Œå¯ä»¥ç»§ç»­ä½¿ç”¨torch_pruning
pruner = tp.pruner.MetaPruner(model, ...)
```

3. **Attentionä½¿ç”¨GQA-awareæ‰‹åŠ¨å‰ªæ**ï¼ˆæ–°æ–¹æ³•ï¼‰
```python
from gqa_aware_pruning import prune_layer_with_gqa_awareness

for layer_idx, pruning_rate in layer_pruning_rates.items():
    # è®¡ç®—æ¢¯åº¦
    model.zero_grad()
    loss = model(example_prompts, labels=example_prompts).loss
    loss.backward()

    # Attention: GQA-awareå‰ªæ
    num_q, num_kv = prune_layer_with_gqa_awareness(
        model, layer_idx, pruning_rate, example_prompts
    )

    # MLP: ä½¿ç”¨torch_pruning (å¾…å®ç°)
    prune_mlp_layer(model, layer_idx, pruning_rate)
```

### æ–¹æ¡ˆ2ï¼šæ”¹è¿›å½“å‰çš„åå¤„ç†ï¼ˆæƒå®œä¹‹è®¡ï¼‰

å¦‚æœä¸æƒ³å¤§æ”¹æµç¨‹ï¼Œå¯ä»¥æ”¹è¿›åå¤„ç†é€»è¾‘ï¼š

```python
# åœ¨åå¤„ç†é˜¶æ®µï¼ŒåŸºäºå·²æœ‰çš„æ¢¯åº¦ä¿¡æ¯é€‰æ‹©è¦ä¿ç•™çš„Q heads
def intelligent_post_processing(layer, target_num_heads, target_num_kv_heads):
    """
    åŸºäºimportanceçš„åå¤„ç†ï¼Œè€Œä¸æ˜¯ç®€å•æˆªæ–­
    """
    # 1. è®¡ç®—æ¯ä¸ªQ headçš„importance (å‡è®¾æ¢¯åº¦è¿˜åœ¨)
    q_salience = (layer.self_attn.q_proj.weight * layer.self_attn.q_proj.weight.grad).abs()
    q_head_imp = q_salience.view(num_heads, head_dim, -1).sum(dim=[1, 2])

    # 2. è®¡ç®—æ¯ä¸ªGQAç»„çš„importance
    group_imp = torch.zeros(num_kv_heads)
    for kv_idx in range(num_kv_heads):
        q_start = kv_idx * 4
        q_end = q_start + 4
        group_imp[kv_idx] = q_head_imp[q_start:q_end].sum()

    # 3. é€‰æ‹©importanceæœ€é«˜çš„ç»„
    keep_kv_indices = torch.argsort(group_imp, descending=True)[:target_num_kv_heads]

    # 4. æ ¹æ®keep_kv_indicesé‡æ–°æ’åˆ—æƒé‡
    ...
```

**é—®é¢˜**ï¼šåœ¨åå¤„ç†é˜¶æ®µï¼Œæ¢¯åº¦å¯èƒ½å·²ç»è¢«æ¸…ç©ºï¼Œimportanceä¿¡æ¯ä¸¢å¤±ã€‚

---

## ğŸ“ˆ é¢„æœŸæ•ˆæœ

### å½“å‰æ–¹æ³•çš„é—®é¢˜

| æŒ‡æ ‡ | å½“å‰æ–¹æ³• |
|------|----------|
| å‰ªæåPPL | **718,107** (71ä¸‡) |
| å¾®è°ƒåPPL | 159.85 |
| GQAæ¯”ä¾‹ | âœ… 4:1 (åå¤„ç†å¼ºåˆ¶) |
| è¯­ä¹‰å¯¹é½ | âŒ ç ´åï¼ˆç®€å•æˆªæ–­ï¼‰ |
| ä¾èµ–å¾®è°ƒ | âœ… å¿…é¡»å¾®è°ƒæ‰èƒ½ç”¨ |

### GQA-awareæ–¹æ³•çš„é¢„æœŸ

| æŒ‡æ ‡ | æ–°æ–¹æ³• |
|------|--------|
| å‰ªæåPPL | **é¢„æœŸ: ~30-50** (å¤§å¹…æ”¹å–„) |
| å¾®è°ƒåPPL | é¢„æœŸ: ~15-25 (æ›´æ¥è¿‘åŸå§‹) |
| GQAæ¯”ä¾‹ | âœ… 4:1 (è‡ªç„¶ä¿æŒ) |
| è¯­ä¹‰å¯¹é½ | âœ… ä¿æŒï¼ˆåŸºäºimportanceé€‰æ‹©ï¼‰ |
| ä¾èµ–å¾®è°ƒ | âš ï¸ ä»éœ€è¦ï¼Œä½†æ”¹å–„ç©ºé—´æ›´å¤§ |

**å…³é”®æ”¹å–„**ï¼š
- å‰ªæåPPLä»71ä¸‡é™åˆ°~30-50ï¼ˆæ”¹å–„14000å€ï¼‰
- å¾®è°ƒåPPLä»160é™åˆ°~15-25ï¼ˆæ”¹å–„6-10å€ï¼‰
- ä¿ç•™é‡è¦çš„attention headsï¼Œä¿æŒæ¨¡å‹ç†è§£èƒ½åŠ›

---

## ğŸš€ ä¸‹ä¸€æ­¥å®æ–½

### Step 1: éªŒè¯GQA-aware importanceè®¡ç®—

```bash
# æµ‹è¯•å•å±‚å‰ªæ
python test_gqa_aware_pruning.py
```

åˆ›å»ºæµ‹è¯•è„šæœ¬ï¼š
```python
# test_gqa_aware_pruning.py
from gqa_aware_pruning import prune_layer_with_gqa_awareness

# åŠ è½½æ¨¡å‹
model = ...
example_prompts = ...

# æµ‹è¯•å‰ªæå•ä¸ªå±‚
num_q, num_kv = prune_layer_with_gqa_awareness(
    model, layer_idx=10, pruning_rate=0.25, example_prompts
)

# éªŒè¯æ¨¡å‹æ˜¯å¦è¿˜èƒ½forward
output = model(example_prompts)
print(f"Forward pass successful! Output shape: {output.logits.shape}")
```

### Step 2: é›†æˆåˆ°å®Œæ•´å‰ªææµç¨‹

ä¿®æ”¹`llama3_unbalanced_pruning.py`ï¼š

```python
# é€‰é¡¹A: å®Œå…¨æ›¿ä»£torch_pruning (for Attention)
from gqa_aware_pruning import prune_layer_with_gqa_awareness

for layer_idx, rate in layer_pruning_rates.items():
    prune_layer_with_gqa_awareness(model, layer_idx, rate, example_prompts)

# é€‰é¡¹B: æ”¹è¿›åå¤„ç†
# (ä½†éœ€è¦ä¿å­˜importanceä¿¡æ¯ï¼Œè¾ƒå¤æ‚)
```

### Step 3: å¯¹æ¯”å®éªŒ

è¿è¡Œä¸¤ä¸ªç‰ˆæœ¬å¹¶å¯¹æ¯”ï¼š

| æ–¹æ³• | å‰ªæåPPL | å¾®è°ƒåPPL | å‚æ•°å‡å°‘ |
|------|-----------|-----------|----------|
| å½“å‰æ–¹æ³• | 718,107 | 159.85 | 17.39% |
| GQA-aware | ??? | ??? | 17.39% |

---

## âš ï¸ æ³¨æ„äº‹é¡¹

### 1. MLPå‰ªæä»éœ€torch_pruning

æ–°æ–¹æ³•åªå¤„ç†Attentionï¼ŒMLPä»ç„¶ä½¿ç”¨torch_pruningï¼š
```python
# éœ€è¦å®ç°MLPçš„ç‹¬ç«‹å‰ªæé€»è¾‘
# æˆ–è€…åªå¯¹Attentionä½¿ç”¨GQA-awareï¼ŒMLPç»§ç»­ç”¨torch_pruning
```

### 2. æ¢¯åº¦è®¡ç®—å¼€é”€

æ¯å±‚éœ€è¦å•ç‹¬è®¡ç®—æ¢¯åº¦ï¼š
```python
# å½“å‰æ–¹æ³•ï¼šä¸€æ¬¡backwardï¼Œprunerå¤„ç†æ‰€æœ‰å±‚
# æ–°æ–¹æ³•ï¼šæ¯å±‚å•ç‹¬backwardï¼ˆå¼€é”€æ›´å¤§ï¼‰

# ä¼˜åŒ–ï¼šbatchå¤„ç†å¤šä¸ªå±‚
```

### 3. è¿­ä»£å‰ªæ

å½“å‰ä½¿ç”¨iterative pruning (å¤šæ¬¡step)ï¼Œæ–°æ–¹æ³•éœ€è¦é€‚é…ï¼š
```python
# æ–¹æ¡ˆï¼šæ¯æ¬¡è¿­ä»£åé‡æ–°è®¡ç®—group importance
for i in range(iterative_steps):
    for layer_idx in pruning_layers:
        prune_layer_with_gqa_awareness(...)
```

---

## ğŸ“š ä»£ç æ–‡ä»¶

- `gqa_aware_pruning.py`: æ ¸å¿ƒå®ç°
  - `compute_gqa_group_importance()`: è®¡ç®—GQAç»„importance
  - `select_gqa_groups_to_prune()`: é€‰æ‹©è¦å‰ªæçš„ç»„
  - `prune_attention_by_gqa_groups()`: æ‰§è¡Œå‰ªæ
  - `prune_layer_with_gqa_awareness()`: å®Œæ•´æµç¨‹

- `GQA_AWARE_PRUNING_GUIDE.md`: æœ¬æ–‡æ¡£

- `llama3_unbalanced_pruning.py`: å¾…ä¿®æ”¹ï¼ˆé›†æˆæ–°æ–¹æ³•ï¼‰

---

## ğŸ“ æ€»ç»“

ä½ çš„ä¸‰ä¸ªé—®é¢˜çš„æ ¸å¿ƒæ´å¯Ÿï¼š

1. âœ… **å±‚çº§å‰ªæç‡**ï¼šremovalæ–¹æ³•å·²ç»åšå¾—å¾ˆå¥½
2. âœ… **Tayloré‡è¦åº¦**ï¼šAttentionå’ŒMLPéƒ½ç”¨ï¼Œä½†Attentionéœ€è¦GQA-aware
3. ğŸ’¡ **å…³é”®åˆ›æ–°**ï¼šå°†4ä¸ªQ heads + 1ä¸ªKV headè§†ä¸ºä¸€ä¸ªç»„ï¼ŒåŸºäºç»„çš„æ€»importanceå‰ªæ

è¿™ä¸ªæ–¹æ¡ˆé¢„æœŸèƒ½å°†å‰ªæåçš„PPLä»71ä¸‡é™åˆ°~30-50ï¼Œå¾®è°ƒåä»160é™åˆ°~15-25ï¼Œå¤§å¹…æå‡æ¨¡å‹å¯ç”¨æ€§ï¼

**ä¸‹ä¸€æ­¥**ï¼šå®æ–½Step 1éªŒè¯ï¼Œå¦‚æœæ•ˆæœå¥½å°±å…¨é¢æ›¿æ¢å½“å‰æ–¹æ³•ã€‚
