#!/usr/bin/env python3
"""
Llama-3 éå‡è¡¡ç»“æ„åŒ–å‰ªæè„šæœ¬
ç»“åˆå±‚é‡è¦åº¦è¯„ä¼°å’Œç»“æ„åŒ–å‰ªæ
"""

import os
import gc
import sys
import json
import torch
import argparse
import numpy as np
from transformers import AutoTokenizer, LlamaForCausalLM
from transformers.models.llama.modeling_llama import LlamaRMSNorm

import LLMPruner.torch_pruning as tp
from LLMPruner.pruner import hf_llama_pruner as llama_pruner
from LLMPruner.utils.logger import LoggerWithDepth
from LLMPruner.evaluator.ppl import PPLMetric
from LLMPruner.datasets.example_samples import get_examples

from layer_importance import (
    LayerImportanceAnalyzer,
    UnbalancedStructuredPruningCalculator,
    create_ch_sparsity_dict_for_llama
)


def load_evaluation_data(tokenizer, num_samples=100):
    """åŠ è½½è¯„ä¼°æ•°æ®"""
    from datasets import load_dataset

    print("åŠ è½½è¯„ä¼°æ•°æ®...")
    dataset = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')

    texts = []
    for i, item in enumerate(dataset):
        if i >= num_samples:
            break
        text = item['text'].strip()
        if len(text) > 50:  # åªä½¿ç”¨è¶³å¤Ÿé•¿çš„æ–‡æœ¬
            texts.append(text)

    return texts[:num_samples]


def main():
    parser = argparse.ArgumentParser(description='Llama-3 éå‡è¡¡ç»“æ„åŒ–å‰ªæ')

    # æ¨¡å‹å‚æ•°
    parser.add_argument('--base_model', type=str, required=True,
                       help='åŸå§‹æ¨¡å‹è·¯å¾„')
    parser.add_argument('--save_ckpt_log_name', type=str, default='llama_unbalanced_prune',
                       help='æ—¥å¿—å’Œæ¨¡å‹ä¿å­˜ç›®å½•åç§°')

    # å‰ªæå‚æ•°
    parser.add_argument('--pruning_ratio', type=float, default=0.25,
                       help='ç›®æ ‡å‰ªæç‡ï¼ˆæ•´ä½“å¹³å‡ï¼‰')
    parser.add_argument('--pruner_type', type=str, default='taylor',
                       choices=['random', 'l1', 'l2', 'taylor'],
                       help='å‰ªæé‡è¦æ€§è¯„ä¼°æ–¹æ³•')

    # å±‚é‡è¦åº¦è¯„ä¼°
    parser.add_argument('--importance_method', type=str, default='removal',
                       choices=['removal', 'activation'],
                       help='å±‚é‡è¦åº¦è¯„ä¼°æ–¹æ³•ï¼šremoval(ç§»é™¤å±‚) æˆ– activation(æ¿€æ´»å€¼)')
    parser.add_argument('--importance_samples', type=int, default=50,
                       help='ç”¨äºè¯„ä¼°å±‚é‡è¦åº¦çš„æ ·æœ¬æ•°é‡')
    parser.add_argument('--skip_importance_analysis', action='store_true',
                       help='è·³è¿‡å±‚é‡è¦åº¦åˆ†æï¼Œä½¿ç”¨å·²ä¿å­˜çš„é…ç½®')
    parser.add_argument('--importance_config', type=str, default='layer_importance_config.json',
                       help='å±‚é‡è¦åº¦é…ç½®æ–‡ä»¶è·¯å¾„')

    # éå‡è¡¡å‰ªæç­–ç•¥
    parser.add_argument('--pruning_strategy', type=str, default='inverse',
                       choices=['inverse', 'proportional', 'uniform'],
                       help='å‰ªæç­–ç•¥ï¼šinverse(é‡è¦å±‚å‰ªå°‘), proportional(é‡è¦å±‚å‰ªå¤š), uniform(å‡åŒ€)')
    parser.add_argument('--alpha', type=float, default=1.0,
                       help='é‡è¦æ€§æƒé‡ç³»æ•°ï¼Œè¶Šå¤§å·®å¼‚è¶Šæ˜æ˜¾')
    parser.add_argument('--min_pruning_rate', type=float, default=0.0,
                       help='æœ€å°å‰ªæç‡')
    parser.add_argument('--max_pruning_rate', type=float, default=0.8,
                       help='æœ€å¤§å‰ªæç‡')

    # å‰ªæèŒƒå›´ï¼ˆé»˜è®¤å‰ªææ‰€æœ‰å±‚ï¼‰
    parser.add_argument('--block_attention_layer_start', type=int, default=0,
                       help='Attention å‰ªæèµ·å§‹å±‚')
    parser.add_argument('--block_attention_layer_end', type=int, default=32,
                       help='Attention å‰ªæç»“æŸå±‚')
    parser.add_argument('--block_mlp_layer_start', type=int, default=0,
                       help='MLP å‰ªæèµ·å§‹å±‚')
    parser.add_argument('--block_mlp_layer_end', type=int, default=32,
                       help='MLP å‰ªæç»“æŸå±‚')

    # å…¶ä»–å‚æ•°
    parser.add_argument('--device', type=str, default='cuda',
                       help='è®¾å¤‡')
    parser.add_argument('--num_examples', type=int, default=10,
                       help='Taylor é‡è¦æ€§è¯„ä¼°çš„æ ·æœ¬æ•°')
    parser.add_argument('--iterative_steps', type=int, default=1,
                       help='è¿­ä»£å‰ªææ­¥æ•°')
    parser.add_argument('--save_model', action='store_true',
                       help='æ˜¯å¦ä¿å­˜æ¨¡å‹')
    parser.add_argument('--test_after_train', action='store_true',
                       help='å‰ªæåæ˜¯å¦è¯„ä¼°')
    parser.add_argument('--max_seq_len', type=int, default=128,
                       help='æœ€å¤§åºåˆ—é•¿åº¦')

    args = parser.parse_args()

    # è®¾ç½®è®¾å¤‡
    print(f"é»˜è®¤è®¾å¤‡: {args.device}")
    if args.device == "cuda":
        from get_best_gpu import get_best_gpu
        args.device = "cuda:" + str(get_best_gpu())
    print(f"æœ€ä¼˜è®¾å¤‡ä¸º: {args.device}")

    # åˆ›å»ºæ—¥å¿—
    logger = LoggerWithDepth(
        env_name=args.save_ckpt_log_name,
        config=args.__dict__,
        root_dir='prune_log',
        setup_sublogger=True
    )

    # åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
    print("åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨...")
    tokenizer = AutoTokenizer.from_pretrained(args.base_model)
    model = LlamaForCausalLM.from_pretrained(
        args.base_model,
        device_map=args.device,
        torch_dtype=torch.float16,
    )
    model.half()

    # è·å–å±‚æ•°
    num_layers = len(model.model.layers)
    logger.log(f"æ¨¡å‹æ€»å±‚æ•°: {num_layers}")

    # ==================== æ­¥éª¤1: è¯„ä¼°å±‚é‡è¦æ€§ ====================
    if not args.skip_importance_analysis:
        logger.log("=" * 80)
        logger.log("æ­¥éª¤1: è¯„ä¼°å±‚é‡è¦æ€§")
        logger.log("=" * 80)

        # åŠ è½½è¯„ä¼°æ•°æ®
        eval_texts = load_evaluation_data(tokenizer, num_samples=args.importance_samples)
        logger.log(f"åŠ è½½äº† {len(eval_texts)} ä¸ªè¯„ä¼°æ ·æœ¬")

        # åˆ›å»ºåˆ†æå™¨
        analyzer = LayerImportanceAnalyzer(model, tokenizer, device=args.device)

        # è¯„ä¼°å±‚é‡è¦æ€§
        if args.importance_method == 'removal':
            logger.log("ä½¿ç”¨å±‚ç§»é™¤æ³•è¯„ä¼°é‡è¦æ€§...")
            layer_importance = analyzer.measure_layer_importance_by_removal(
                eval_texts, num_layers=num_layers
            )
        else:
            logger.log("ä½¿ç”¨æ¿€æ´»å€¼æ³•è¯„ä¼°é‡è¦æ€§...")
            layer_importance = analyzer.measure_layer_importance_by_activation(eval_texts)

        # æ˜¾ç¤ºé‡è¦æ€§ç»“æœ
        logger.log("\nå±‚é‡è¦æ€§è¯„åˆ†:")
        for layer_idx, importance in sorted(layer_importance.items()):
            logger.log(f"  Layer {layer_idx}: {importance:.6f}")

    else:
        logger.log("è·³è¿‡å±‚é‡è¦åº¦åˆ†æï¼ŒåŠ è½½å·²ä¿å­˜çš„é…ç½®...")
        calculator = UnbalancedStructuredPruningCalculator({}, num_layers)
        layer_pruning_rates = calculator.load_pruning_rates(args.importance_config)

        # åæ¨é‡è¦æ€§ï¼ˆç®€åŒ–å¤„ç†ï¼‰
        layer_importance = {i: 1.0 for i in range(num_layers)}

    # ==================== æ­¥éª¤2: è®¡ç®—å„å±‚å‰ªæç‡ ====================
    logger.log("=" * 80)
    logger.log("æ­¥éª¤2: è®¡ç®—å„å±‚å‰ªæç‡")
    logger.log("=" * 80)

    calculator = UnbalancedStructuredPruningCalculator(layer_importance, num_layers)

    # è®¡ç®—å‰ªæç‡
    layer_pruning_rates = calculator.compute_layer_pruning_rates(
        target_overall_rate=args.pruning_ratio,
        strategy=args.pruning_strategy,
        alpha=args.alpha,
        min_rate=args.min_pruning_rate,
        max_rate=args.max_pruning_rate,
        use_log_transform=True  # ä½¿ç”¨å¯¹æ•°å˜æ¢å¤„ç†æç«¯å€¼
    )

    # éªŒè¯å‰ªæç‡
    stats = calculator.verify_average_pruning_rate(layer_pruning_rates)
    logger.log(f"\nå‰ªæç‡ç»Ÿè®¡:")
    logger.log(f"  å¹³å‡å‰ªæç‡: {stats['average_pruning_rate']:.4f}")
    logger.log(f"  æ ‡å‡†å·®: {stats['std_pruning_rate']:.4f}")
    logger.log(f"  æœ€å°å‰ªæç‡: {stats['min_pruning_rate']:.4f}")
    logger.log(f"  æœ€å¤§å‰ªæç‡: {stats['max_pruning_rate']:.4f}")
    logger.log(f"  å‰ªæç‡èŒƒå›´: {stats['rate_range']:.4f}")

    # æ˜¾ç¤ºå„å±‚å‰ªæç‡
    logger.log("\nå„å±‚å‰ªæç‡:")
    for layer_idx in range(num_layers):
        rate = layer_pruning_rates.get(layer_idx, 0.0)
        logger.log(f"  Layer {layer_idx}: {rate:.4f}")

    # ä¿å­˜é…ç½®
    config_path = os.path.join(logger.log_dir, args.importance_config)
    calculator.save_pruning_rates(layer_pruning_rates, config_path)

    # å¯è§†åŒ–
    viz_path = os.path.join(logger.log_dir, 'pruning_strategy.png')
    calculator.visualize_pruning_strategy(layer_pruning_rates, save_path=viz_path)

    # ==================== æ­¥éª¤3: åˆ›å»º ch_sparsity_dict ====================
    logger.log("=" * 80)
    logger.log("æ­¥éª¤3: åˆ›å»ºæ¨¡å—çº§å‰ªæç‡å­—å…¸")
    logger.log("=" * 80)

    # åªä¸ºè¦å‰ªæçš„å±‚åˆ›å»º ch_sparsity_dict
    pruning_layers = set(range(args.block_attention_layer_start, args.block_attention_layer_end)) | \
                    set(range(args.block_mlp_layer_start, args.block_mlp_layer_end))

    # è¿‡æ»¤ï¼šåªä¿ç•™åœ¨å‰ªæèŒƒå›´å†…çš„å±‚
    filtered_pruning_rates = {
        idx: rate for idx, rate in layer_pruning_rates.items()
        if idx in pruning_layers
    }

    # è¿›ä¸€æ­¥è¿‡æ»¤ï¼šç§»é™¤å‰ªæç‡å¤ªä½çš„å±‚ï¼ˆé¿å…å‰ªæ0ä¸ªé€šé“å¯¼è‡´é”™è¯¯ï¼‰
    # å¯¹äº k_proj (1024é€šé“ï¼Œhead_dim=128)ï¼Œè‡³å°‘éœ€è¦å‰ªæ 1ä¸ªhead = 128/1024 = 12.5%
    # ä¸ºäº†å®‰å…¨ï¼Œè®¾ç½®ä¸º 15%
    min_effective_rate = 0.15  # æœ€å°æœ‰æ•ˆå‰ªæç‡ï¼š15%
    effective_pruning_rates = {
        idx: rate for idx, rate in filtered_pruning_rates.items()
        if rate >= min_effective_rate
    }

    # è®°å½•è¢«è¿‡æ»¤æ‰çš„å±‚
    skipped_layers = set(filtered_pruning_rates.keys()) - set(effective_pruning_rates.keys())
    if skipped_layers:
        logger.log(f"è­¦å‘Šï¼šä»¥ä¸‹å±‚çš„å‰ªæç‡ < {min_effective_rate:.2%}ï¼Œå·²è·³è¿‡ï¼š{sorted(skipped_layers)}")
        for idx in sorted(skipped_layers):
            logger.log(f"  Layer {idx}: {filtered_pruning_rates[idx]:.4f} (éœ€è¦ >= {min_effective_rate:.2%} æ‰èƒ½å‰ªæè‡³å°‘1ä¸ªattention head)")

    ch_sparsity_dict = create_ch_sparsity_dict_for_llama(
        model,
        effective_pruning_rates,
        prune_attention=True,
        prune_mlp=True
    )

    logger.log(f"ä¸º {len(ch_sparsity_dict)} ä¸ªæ¨¡å—è®¾ç½®äº†è‡ªå®šä¹‰å‰ªæç‡")

    # ä¿å­˜ ch_sparsity_dict åˆ°æ–‡ä»¶ï¼ˆè½¬æ¢ä¸ºå¯è¯»æ ¼å¼ï¼‰
    ch_sparsity_readable = {}
    for module, rate in ch_sparsity_dict.items():
        # è·å–æ¨¡å—çš„å®Œæ•´åç§°
        module_name = None
        for name, m in model.named_modules():
            if m is module:
                module_name = name
                break
        if module_name:
            ch_sparsity_readable[module_name] = float(rate)

    ch_sparsity_path = os.path.join(logger.log_dir, 'ch_sparsity_dict.json')
    with open(ch_sparsity_path, 'w') as f:
        json.dump(ch_sparsity_readable, f, indent=2, ensure_ascii=False)
    logger.log(f"ch_sparsity_dict å·²ä¿å­˜åˆ°: {ch_sparsity_path}")

    # è·å–å®é™…å‚ä¸å‰ªæçš„å±‚åˆ—è¡¨
    actual_pruning_layers = sorted(effective_pruning_rates.keys())
    logger.log(f"å®é™…å‚ä¸å‰ªæçš„å±‚: {actual_pruning_layers}")

    # ==================== æ³¨æ„ï¼šå…³äº GQA æ¯”ä¾‹ ====================
    logger.log("=" * 80)
    logger.log("å…³äº GQA æ¯”ä¾‹çš„è¯´æ˜")
    logger.log("=" * 80)
    logger.log(f"âš ï¸  torch_pruning çš„ä¾èµ–å›¾ä¼ æ’­æ˜¯'é€šé“å¯¹é€šé“'çš„ï¼Œä¸ç†è§£ GQA çš„ 4:1 ç»“æ„")
    logger.log(f"   ç¤ºä¾‹ï¼šå‰ªæç‡ 0.25")
    logger.log(f"   - k_proj: å‰ªæ‰ 1024 Ã— 0.25 = 256 é€šé“ï¼ˆ2ä¸ªKV headsï¼‰â†’ å‰©ä½™ 6 KV heads")
    logger.log(f"   - q_proj: ä¾èµ–å›¾ä¼ æ’­ç›¸åŒçš„ 256 é€šé“ï¼ˆ2ä¸ªQ headsï¼‰â†’ å‰©ä½™ 30 Q heads")
    logger.log(f"   - ç»“æœï¼š30:6 = 5:1 âœ— (ä¸æ˜¯åŸå§‹çš„ 4:1)")
    logger.log(f"\nâœ… è§£å†³æ–¹æ¡ˆï¼šå‰ªæåçš„åå¤„ç†é˜¶æ®µä¼šå¼ºåˆ¶ä¿®æ­£ä¸º 4:1")
    logger.log(f"   - 30:6 (5:1) â†’ 24:6 (4:1)ï¼šæˆªæ–­ 6 ä¸ª Q heads")
    logger.log(f"   - è¿™ç¡®ä¿æ‰€æœ‰å±‚éƒ½ä¿æŒåŸå§‹çš„ GQA æ¯”ä¾‹")
    logger.log("=" * 80)

    # ==================== æ­¥éª¤4: æ‰§è¡Œç»“æ„åŒ–å‰ªæ ====================
    logger.log("=" * 80)
    logger.log("æ­¥éª¤4: æ‰§è¡Œç»“æ„åŒ–å‰ªæ")
    logger.log("=" * 80)

    # å¯ç”¨æ¢¯åº¦
    for param in model.parameters():
        param.requires_grad_(True)

    before_pruning_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)
    logger.log(f"å‰ªæå‰å‚æ•°é‡: {before_pruning_parameters:,}")

    # åˆ›å»º forward prompts
    forward_prompts = torch.tensor([
        [1, 306, 4658, 278, 6593, 310, 2834, 338],
        [1, 3439, 17632, 1925, 29892, 278, 6368, 310],
    ]).to(args.device)

    # é€‰æ‹©é‡è¦æ€§è¯„ä¼°æ–¹æ³•
    if args.pruner_type == 'random':
        imp = tp.importance.RandomImportance()
    elif args.pruner_type == 'l1':
        imp = llama_pruner.MagnitudeImportance(p=1)
    elif args.pruner_type == 'l2':
        imp = llama_pruner.MagnitudeImportance(p=2)
    elif args.pruner_type == 'taylor':
        imp = llama_pruner.TaylorImportance(group_reduction='sum', taylor='param_first')
    else:
        raise NotImplementedError

    logger.log(f"ä½¿ç”¨ {args.pruner_type} å‰ªæå™¨...")

    # ==================== æ–°æ–¹æ¡ˆï¼šæ‰‹åŠ¨æ§åˆ¶ GQA å‰ªæ ====================
    # ç­–ç•¥ï¼šè®© q_proj å’Œ o_proj ä¸å‚ä¸è‡ªåŠ¨ä¾èµ–å›¾ï¼Œæ‰‹åŠ¨å‰ªæä»¥ç¡®ä¿4:1æ¯”ä¾‹
    # åŸå› ï¼štorch_pruningçš„ä¾èµ–å›¾æ˜¯"é€šé“å¯¹é€šé“"ä¼ æ’­ï¼Œæ— æ³•ç†è§£GQAçš„4:1ç»“æ„
    #
    # æ­¥éª¤ï¼š
    # 1. pruner åªç®¡ç† k_proj, v_proj, å’Œ MLPï¼ˆä¸åŒ…æ‹¬ q_proj, o_projï¼‰
    # 2. æ¯æ¬¡ pruner.step() åï¼Œæ‰‹åŠ¨å‰ªæ q_projï¼ˆç¡®ä¿æ˜¯ k_proj çš„4å€ï¼‰
    # 3. åŒæ—¶æ‰‹åŠ¨è°ƒæ•´ o_proj è¾“å…¥ç»´åº¦ï¼ˆåŒ¹é… q_proj è¾“å‡ºï¼‰
    # 4. è¿™æ ·æ‰€æœ‰æ¨¡å—ä»å§‹è‡³ç»ˆä¿æŒä¸€è‡´çš„ç»´åº¦ï¼ŒLoRA ä¸ä¼šé‡åˆ°é…ç½®å†²çª

    root_instances = []
    for layer_idx in actual_pruning_layers:
        root_instances.append(model.model.layers[layer_idx].self_attn.k_proj)  # KV attention
        root_instances.append(model.model.layers[layer_idx].mlp.gate_proj)     # MLP

    # è·å– GQA é…ç½®
    num_heads = model.config.num_attention_heads  # 32
    num_key_value_heads = model.config.num_key_value_heads  # 8
    head_dim = 128
    gqa_ratio = num_heads // num_key_value_heads  # 4

    # é…ç½® consecutive_groupsï¼šè®© k_proj æŒ‰ head çº§åˆ«å‰ªæ
    consecutive_groups = {}
    for layer in model.model.layers:
        consecutive_groups[layer.self_attn.k_proj] = head_dim  # 1ä¸ªKV head = 128é€šé“

    # å…³é”®ï¼šå°† q_proj å’Œ o_proj åŠ å…¥ ignored_layersï¼Œä¸è®©å®ƒä»¬å‚ä¸è‡ªåŠ¨å‰ªæ
    # æˆ‘ä»¬ä¼šæ‰‹åŠ¨æ§åˆ¶å®ƒä»¬çš„å‰ªæ
    ignored_layers = []
    for layer in model.model.layers:
        ignored_layers.append(layer.self_attn.q_proj)
        ignored_layers.append(layer.self_attn.o_proj)

    logger.log("=" * 80)
    logger.log("ğŸ”§ æ–°çš„ GQA-aware å‰ªæç­–ç•¥")
    logger.log("=" * 80)
    logger.log(f"GQA é…ç½®: Q heads={num_heads}, KV heads={num_key_value_heads}, æ¯”ä¾‹={gqa_ratio}:1")
    logger.log(f"\nå‰ªæé…ç½®:")
    logger.log(f"  - root_instances: k_proj å’Œ gate_proj")
    logger.log(f"  - consecutive_groups: k_proj (128é€šé“/head)")
    logger.log(f"  - ignored_layers: q_proj å’Œ o_projï¼ˆæ‰‹åŠ¨æ§åˆ¶ä»¥ç¡®ä¿4:1æ¯”ä¾‹ï¼‰")
    logger.log(f"\nå·¥ä½œæµç¨‹:")
    logger.log(f"  1. pruner.step() å‰ªæ k_proj å’Œ MLP")
    logger.log(f"  2. æ‰‹åŠ¨å‰ªæ q_projï¼ˆç¡®ä¿ Q heads = KV heads Ã— 4ï¼‰")
    logger.log(f"  3. æ‰‹åŠ¨è°ƒæ•´ o_proj è¾“å…¥ç»´åº¦ï¼ˆåŒ¹é… q_proj è¾“å‡ºï¼‰")
    logger.log(f"  4. æ‰€æœ‰æ¨¡å—ç»´åº¦ä¸€è‡´ï¼Œæ— éœ€åå¤„ç†")
    logger.log("=" * 80 + "\n")

    kwargs = {
        "importance": imp,
        "global_pruning": False,
        "iterative_steps": args.iterative_steps,
        "ch_sparsity": args.pruning_ratio,  # é»˜è®¤å‰ªæç‡ï¼ˆä¸åº”è¯¥è¢«ä½¿ç”¨ï¼‰
        "ch_sparsity_dict": ch_sparsity_dict,  # â­ æ¯å±‚çš„å‰ªæç‡
        "ignored_layers": ignored_layers,  # â­ å¿½ç•¥ q_proj å’Œ o_projï¼Œæˆ‘ä»¬ä¼šæ‰‹åŠ¨å¤„ç†
        "channel_groups": {},  # â­ ç©ºå­—å…¸ï¼Œä¸åŸå§‹ llama3.py ä¸€è‡´
        "consecutive_groups": consecutive_groups,  # â­ å¼ºåˆ¶ k_proj æŒ‰ 128 é€šé“åˆ†ç»„
        "customized_pruners": {
            LlamaRMSNorm: llama_pruner.hf_rmsnorm_pruner,
        },
        "root_module_types": None,
        "root_instances": root_instances  # â­ åªæœ‰ k_proj å’Œ gate_projï¼ˆä¸llama3.pyä¸€è‡´ï¼‰
    }

    logger.log(f"å®é™…å‰ªæ Attention å’Œ MLP çš„å±‚: {actual_pruning_layers}")
    logger.log(f"root_instances æ•°é‡: {len(root_instances)} (æ¯å±‚2ä¸ªæ¨¡å—: k_proj + gate_proj)")

    # åˆ›å»ºå‰ªæå™¨
    pruner = tp.pruner.MetaPruner(model, forward_prompts, **kwargs)
    model.zero_grad()

    logger.log("å¼€å§‹å‰ªæ...")

    # è¿­ä»£å‰ªæ
    for i in range(args.iterative_steps):
        if args.pruner_type == 'taylor':
            example_prompts = get_examples('wikitext', tokenizer, args.num_examples, seq_len=64).to(args.device)
            logger.log(f"è¿­ä»£æ­¥éª¤ {i}: å¼€å§‹åå‘ä¼ æ’­...")

            loss = model(example_prompts, labels=example_prompts).loss
            logger.log(f"Loss = {loss.item()}")
            loss.backward()

        # æ‰§è¡Œå‰ªæï¼ˆåªå‰ªæ k_proj, v_proj, MLPï¼‰
        pruner.step()

        logger.log(f"\n{'='*80}")
        logger.log(f"è¿­ä»£ {i+1}/{args.iterative_steps}: æ‰‹åŠ¨å¤„ç† Q-projection å’Œ O-projection")
        logger.log(f"{'='*80}")

        # æ‰‹åŠ¨å‰ªæ q_proj å’Œè°ƒæ•´ o_projï¼ˆç¡®ä¿ 4:1 GQA æ¯”ä¾‹ï¼‰
        original_gqa_ratio = model.config.num_attention_heads // model.config.num_key_value_heads  # 4
        head_dim = 128

        for layer_idx, layer in enumerate(model.model.layers):
            # 1. è¯»å– k_proj å‰ªæåçš„ç»´åº¦ï¼ˆç”± pruner å¤„ç†ï¼‰
            k_out_channels = layer.self_attn.k_proj.weight.data.shape[0]
            assert k_out_channels % head_dim == 0, f"Layer {layer_idx}: k_proj ç»´åº¦ {k_out_channels} ä¸æ˜¯ head_dim {head_dim} çš„å€æ•°"
            num_kv_heads = k_out_channels // head_dim

            # 2. è®¡ç®—ç›®æ ‡ q_proj ç»´åº¦ï¼ˆä¿æŒ 4:1 æ¯”ä¾‹ï¼‰
            target_num_heads = num_kv_heads * original_gqa_ratio
            target_q_channels = target_num_heads * head_dim

            # 3. è·å–å½“å‰ q_proj ç»´åº¦
            current_q_channels = layer.self_attn.q_proj.weight.data.shape[0]
            current_q_in_channels = layer.self_attn.q_proj.weight.data.shape[1]

            # 4. å¦‚æœéœ€è¦ï¼Œæ‰‹åŠ¨å‰ªæ q_projï¼ˆé€‰æ‹©å‰ N ä¸ª headsï¼‰
            if current_q_channels != target_q_channels:
                logger.log(f"Layer {layer_idx}: æ‰‹åŠ¨å‰ªæ q_proj")
                logger.log(f"  å½“å‰ Q heads: {current_q_channels // head_dim}")
                logger.log(f"  å½“å‰ KV heads: {num_kv_heads}")
                logger.log(f"  ç›®æ ‡ Q heads: {target_num_heads} (KV {num_kv_heads} Ã— {original_gqa_ratio})")

                # å‰ªæ q_proj æƒé‡ï¼šåªä¿ç•™å‰ target_q_channels ä¸ªè¾“å‡ºé€šé“
                layer.self_attn.q_proj.weight.data = layer.self_attn.q_proj.weight.data[:target_q_channels, :]
                if layer.self_attn.q_proj.bias is not None:
                    layer.self_attn.q_proj.bias.data = layer.self_attn.q_proj.bias.data[:target_q_channels]

                # è°ƒæ•´ o_proj çš„è¾“å…¥ç»´åº¦ï¼ˆå®ƒæ¥æ”¶ q_proj çš„è¾“å‡ºï¼‰
                layer.self_attn.o_proj.weight.data = layer.self_attn.o_proj.weight.data[:, :target_q_channels]

                logger.log(f"  âœ… å·²è°ƒæ•´: Q {target_num_heads} heads, KV {num_kv_heads} heads â†’ {original_gqa_ratio}:1")

            # 5. æ›´æ–°å±‚é…ç½®
            layer.self_attn.num_heads = target_num_heads
            layer.self_attn.num_key_value_heads = num_kv_heads
            layer.self_attn.num_key_value_groups = target_num_heads // num_kv_heads

        after_pruning_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)
        logger.log(f"\nè¿­ä»£ {i+1}/{args.iterative_steps} å®Œæˆï¼Œå‚æ•°é‡: {after_pruning_parameters:,}")

    # æ¸…ç†æ¢¯åº¦
    model.zero_grad()
    for name, module in model.named_parameters():
        if 'weight' in name:
            module.grad = None

    del pruner
    torch.cuda.empty_cache()

    # æœ€ç»ˆç»Ÿè®¡
    final_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)
    logger.log(f"\nå‰ªæå®Œæˆ!")
    logger.log(f"å‰ªæå‰å‚æ•°é‡: {before_pruning_parameters:,}")
    logger.log(f"å‰ªæåå‚æ•°é‡: {final_parameters:,}")
    logger.log(f"å‚æ•°å‡å°‘é‡: {before_pruning_parameters - final_parameters:,}")
    logger.log(f"å®é™…å‰ªæç‡: {(1 - final_parameters/before_pruning_parameters)*100:.2f}%")

    # éªŒè¯å’Œæ‰“å°æ¯å±‚çš„ head æ•°é‡
    logger.log("\næ¯å±‚çš„ attention heads é…ç½®:")
    for idx, layer in enumerate(model.model.layers):
        logger.log(f"  Layer {idx}: q_heads={layer.self_attn.num_heads}, kv_heads={layer.self_attn.num_key_value_heads}, ratio={layer.self_attn.num_heads/layer.self_attn.num_key_value_heads:.1f}:1")

    # ==================== æ­¥éª¤5: ä¿å­˜æ¨¡å‹ ====================
    if args.save_model:
        logger.log("=" * 80)
        logger.log("æ­¥éª¤5: ä¿å­˜æ¨¡å‹")
        logger.log("=" * 80)

        # ä¿å­˜å‰æœ€ç»ˆé…ç½®ç¡®è®¤å’Œä¿®æ­£ï¼ˆç¡®ä¿æ‰€æœ‰é…ç½®æ­£ç¡®ï¼Œé¿å… LoRA ç»´åº¦ä¸åŒ¹é…ï¼‰
        logger.log("\nä¿å­˜å‰æœ€ç»ˆé…ç½®æ£€æŸ¥...")
        head_dim = 128
        for i, layer in enumerate(model.model.layers):
            # ä»å®é™…æƒé‡æ¨æ–­é…ç½®
            actual_q_heads = layer.self_attn.q_proj.weight.shape[0] // head_dim
            actual_kv_heads = layer.self_attn.k_proj.weight.shape[0] // head_dim

            # å¼ºåˆ¶æ›´æ–°æ‰€æœ‰ç›¸å…³é…ç½®
            layer.self_attn.num_heads = actual_q_heads
            layer.self_attn.num_key_value_heads = actual_kv_heads
            layer.self_attn.num_key_value_groups = actual_q_heads // actual_kv_heads

            logger.log(f"  Layer {i}: {actual_q_heads} Q heads, {actual_kv_heads} KV heads, ratio {actual_q_heads/actual_kv_heads:.1f}:1")

        logger.log("âœ… é…ç½®æ£€æŸ¥å®Œæˆ\n")

        model.half()
        torch.save({
            'model': model,
            'tokenizer': tokenizer,
            'layer_pruning_rates': layer_pruning_rates,
            'layer_importance': layer_importance,
        }, logger.best_checkpoint_path)

        logger.log(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {logger.best_checkpoint_path}")

    # ==================== æ­¥éª¤6: è¯„ä¼° PPL ====================
    if args.test_after_train:
        logger.log("=" * 80)
        logger.log("æ­¥éª¤6: è¯„ä¼°å›°æƒ‘åº¦")
        logger.log("=" * 80)

        # åœ¨è¯„ä¼°å‰å†æ¬¡æ›´æ–° head é…ç½®ï¼ˆç¡®ä¿æ­£ç¡®ï¼‰
        for layer in model.model.layers:
            layer.self_attn.num_heads = layer.self_attn.q_proj.weight.data.shape[0] // layer.self_attn.head_dim
            layer.self_attn.num_key_value_heads = layer.self_attn.k_proj.weight.data.shape[0] // layer.self_attn.head_dim

        model.to(args.device)
        model.eval()

        ppl = PPLMetric(model, tokenizer, ['wikitext2', 'ptb'],
                       args.max_seq_len, device=args.device)
        logger.log(f"å‰ªæå PPL: {ppl}")

    logger.log("\nå®Œæˆï¼")


if __name__ == "__main__":
    main()
