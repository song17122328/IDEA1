# åŸå§‹å‡åŒ€å‰ªæè¯¦ç»†è§£æ

## ğŸ¯ æ ¸å¿ƒæ€æƒ³

**æ‰€æœ‰æŒ‡å®šçš„å±‚ä½¿ç”¨ç›¸åŒçš„å‰ªæç‡**ï¼Œé€šè¿‡ä¸€ä¸ªå…¨å±€å‚æ•° `ch_sparsity` æ§åˆ¶ã€‚

---

## ğŸ“ å®Œæ•´ä»£ç æµç¨‹ï¼ˆllama3.pyï¼‰

### ç¬¬1æ­¥ï¼šåˆå§‹åŒ–é‡è¦æ€§è¯„ä¼°å™¨

```python
# llama3.py:98-107
pruner_type = args.pruner_type.lower()  # ä¾‹å¦‚: 'taylor'

if pruner_type == 'taylor':
    imp = llama_pruner.TaylorImportance(
        group_reduction=args.grouping_strategy,  # 'sum' æˆ– 'mean'
        taylor=args.taylor  # 'param_first' æˆ– 'param_mix'
    )
```

**Taylor é‡è¦æ€§ï¼š**
```
importance = |âˆ‚L/âˆ‚W Ã— W|

å…¶ä¸­:
- âˆ‚L/âˆ‚W: æŸå¤±å¯¹æƒé‡çš„æ¢¯åº¦ï¼ˆé€šè¿‡åå‘ä¼ æ’­å¾—åˆ°ï¼‰
- W: æƒé‡æœ¬èº«
- |Â·|: ç»å¯¹å€¼
```

è¿™ä¸ªé‡è¦æ€§ä¼šç”¨äº**åœ¨æ¯ä¸€å±‚å†…éƒ¨**é€‰æ‹©å“ªäº› head è¦å‰ªæã€‚

---

### ç¬¬2æ­¥ï¼šé…ç½®å‰ªæå‚æ•°ï¼ˆå…³é”®ï¼ï¼‰

```python
# llama3.py:112-129
kwargs = {
    "importance": imp,                          # Taylor é‡è¦æ€§è¯„ä¼°å™¨
    "global_pruning": args.global_pruning,      # False: å±€éƒ¨å‰ªæï¼ŒTrue: å…¨å±€å‰ªæ
    "iterative_steps": args.iterative_steps,    # è¿­ä»£æ¬¡æ•°ï¼ˆé€šå¸¸ä¸º1ï¼‰

    # â­ æ ¸å¿ƒå‚æ•°ï¼šå…¨å±€å‰ªæç‡
    "ch_sparsity": args.pruning_ratio,          # ä¾‹å¦‚: 0.25 (æ‰€æœ‰å±‚éƒ½ç”¨è¿™ä¸ª)

    "ignored_layers": [],                       # å¿½ç•¥çš„å±‚ï¼ˆç©ºåˆ—è¡¨ï¼‰

    # channel_groups: ç©ºå­—å…¸ï¼ˆä¸ä½¿ç”¨ï¼‰
    "channel_groups": {},

    # â­ consecutive_groups: å¼ºåˆ¶ head-level å‰ªæ
    "consecutive_groups": {
        layer.self_attn.k_proj: layer.self_attn.head_dim  # æ¯ä¸ª k_proj: 128
        for layer in model.model.layers
    },

    "customized_pruners": {
        LlamaRMSNorm: llama_pruner.hf_rmsnorm_pruner,
    },

    "root_module_types": None,

    # â­ root_instances: æŒ‡å®šå“ªäº›å±‚è¦å‰ªæ
    "root_instances": [
        model.model.layers[i].self_attn.k_proj
        for i in range(args.block_attention_layer_start, args.block_attention_layer_end)
    ] + [
        model.model.layers[i].mlp.gate_proj
        for i in range(args.block_mlp_layer_start, args.block_mlp_layer_end)
    ]
}
```

---

### ç¬¬3æ­¥ï¼šå…³é”®å‚æ•°è¯¦è§£

#### 3.1 `ch_sparsity: 0.25`

**å«ä¹‰ï¼š** æ‰€æœ‰åœ¨ `root_instances` ä¸­çš„æ¨¡å—éƒ½å‰ªæ 25%

```python
# å¦‚æœ ch_sparsity = 0.25
å¯¹äºæ¯ä¸ª root_instanceï¼ˆä¾‹å¦‚ Layer 5 çš„ k_projï¼‰:
  - åŸå§‹è¾“å‡ºé€šé“: 1024 (8 ä¸ª heads Ã— 128)
  - å‰ªæåé€šé“: 1024 Ã— (1 - 0.25) = 768 (6 ä¸ª heads Ã— 128)
  - å‰ªæçš„é€šé“: 256 (2 ä¸ª heads Ã— 128)
```

**é‡ç‚¹ï¼š**
- âœ… æ‰€æœ‰å±‚ä½¿ç”¨**ç›¸åŒçš„å‰ªæç‡** (0.25)
- âŒ æ²¡æœ‰ `ch_sparsity_dict`ï¼Œæ— æ³•ä¸ºæ¯å±‚æŒ‡å®šä¸åŒçš„å‰ªæç‡
- âœ… ç®€å•ã€å¯é¢„æµ‹

#### 3.2 `consecutive_groups`

**å«ä¹‰ï¼š** å¼ºåˆ¶æ¯ä¸ª k_proj å¿…é¡»å‰ªæå®Œæ•´çš„ headï¼ˆ128 ä¸ªè¿ç»­é€šé“ï¼‰

```python
"consecutive_groups": {
    model.layers[0].self_attn.k_proj: 128,
    model.layers[1].self_attn.k_proj: 128,
    ...
    model.layers[31].self_attn.k_proj: 128,
}
```

**å·¥ä½œåŸç†ï¼š**

```python
# å¯¹äº Layer 5 çš„ k_proj
k_proj: [4096, 1024]  # 8 ä¸ª heads

consecutive_group_size = 128  # head_dim

# å‰ªææ—¶çš„çº¦æŸ:
å‰ªæçš„é€šé“ç´¢å¼•å¿…é¡»æ˜¯è¿ç»­çš„ 128 ä¸ªé€šé“çš„å€æ•°

ä¾‹å¦‚:
  âœ… å‰ªæ head 2 å’Œ head 5: é€šé“ [256:384] å’Œ [640:768]
  âŒ å‰ªæé›¶æ•£çš„é€šé“: [10, 25, 67, 89, ...]
```

**ä¸ºä»€ä¹ˆéœ€è¦ï¼Ÿ**

Head-level å‰ªææ‰èƒ½ä¿æŒ attention æœºåˆ¶çš„å®Œæ•´æ€§ï¼

#### 3.3 `root_instances`

**å«ä¹‰ï¼š** æŒ‡å®šä»å“ªäº›æ¨¡å—å¼€å§‹å‰ªæ

```python
# é»˜è®¤å‚æ•°:
# --block_attention_layer_start 3
# --block_attention_layer_end 30

root_instances = [
    model.layers[3].self_attn.k_proj,   # Layer 3 Attention
    model.layers[4].self_attn.k_proj,   # Layer 4 Attention
    ...
    model.layers[29].self_attn.k_proj,  # Layer 29 Attention
    model.layers[3].mlp.gate_proj,      # Layer 3 MLP
    model.layers[4].mlp.gate_proj,      # Layer 4 MLP
    ...
    model.layers[29].mlp.gate_proj,     # Layer 29 MLP
]

# æ€»å…±: 27 å±‚ Ã— 2 æ¨¡å— = 54 ä¸ª root modules
```

**å‰ªææµç¨‹ï¼š**

1. **ä» root_instances å¼€å§‹**
   - k_proj è¢«å‰ªæ â†’ è‡ªåŠ¨ä¼ æ’­åˆ° q_proj, v_proj, o_proj
   - gate_proj è¢«å‰ªæ â†’ è‡ªåŠ¨ä¼ æ’­åˆ° up_proj, down_proj

2. **ä¸åœ¨ root_instances ä¸­çš„å±‚ä¸ä¼šè¢«å‰ªæ**
   - Layer 0, 1, 2: ä¸å‰ªæï¼ˆä¿æŠ¤ï¼‰
   - Layer 30, 31: ä¸å‰ªæï¼ˆä¿æŠ¤ï¼‰

---

### ç¬¬4æ­¥ï¼šMetaPruner æ‰§è¡Œå‰ªæ

```python
# llama3.py:133-177
pruner = tp.pruner.MetaPruner(model, forward_prompts, **kwargs)
model.zero_grad()

logger.log("Start Pruning")
for i in range(args.iterative_steps):  # é€šå¸¸åªæœ‰ 1 æ¬¡è¿­ä»£

    # å¦‚æœä½¿ç”¨ Taylor é‡è¦æ€§ï¼Œéœ€è¦è®¡ç®—æ¢¯åº¦
    if pruner_type == 'taylor':
        example_prompts = get_examples('bookcorpus', tokenizer, 10, seq_len=64)
        loss = model(example_prompts, labels=example_prompts).loss
        loss.backward()  # è®¡ç®—æ¢¯åº¦

    # æ‰§è¡Œå‰ªæ
    pruner.step()

    # æ›´æ–° attention é…ç½®
    for layer in model.model.layers:
        layer.self_attn.num_heads = layer.self_attn.q_proj.weight.shape[0] // 128
        layer.self_attn.num_key_value_heads = layer.self_attn.k_proj.weight.shape[0] // 128
```

---

## ğŸ” MetaPruner å†…éƒ¨æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿ

### é˜¶æ®µ1ï¼šæ„å»ºä¾èµ–å›¾

```python
# ä½¿ç”¨ forward_prompts æ‰§è¡Œä¸€æ¬¡å‰å‘ä¼ æ’­
forward_prompts = torch.tensor([
    [1, 306, 4658, 278, 6593, 310, 2834, 338],
    [1, 3439, 17632, 1925, 29892, 278, 6368, 310],
]).to(device)

output = model(forward_prompts)
```

**ç›®çš„ï¼š** è¿½è¸ªæ¨¡å—ä¹‹é—´çš„è¿æ¥å…³ç³»

```
Layer 5 ç¤ºä¾‹:

è¾“å…¥ (hidden_states)
  â”œâ”€â†’ q_proj [4096, 4096]
  â”œâ”€â†’ k_proj [4096, 1024]  â† root module
  â””â”€â†’ v_proj [4096, 1024]
       â†“
   Attention è®¡ç®—
       â†“
   o_proj [4096, 4096]
       â†“
   è¾“å‡º (hidden_states)
```

ä¾èµ–å›¾è®°å½•ï¼š
- k_proj çš„è¾“å‡ºè¿æ¥åˆ° Attention
- q_proj, v_proj çš„è¾“å‡ºä¹Ÿè¿æ¥åˆ° Attention
- Attention çš„è¾“å‡ºè¿æ¥åˆ° o_proj
- o_proj çš„è¾“å‡ºè¿æ¥åˆ°ä¸‹ä¸€å±‚çš„è¾“å…¥

### é˜¶æ®µ2ï¼šè®¡ç®—æ¯ä¸ªé€šé“çš„é‡è¦æ€§

å¯¹äºæ¯ä¸ª **root_instance**ï¼ˆä¾‹å¦‚ Layer 5 çš„ k_projï¼‰ï¼š

```python
# k_proj: [4096, 1024]
# 1024 ä¸ªè¾“å‡ºé€šé“ = 8 ä¸ª heads

# ä½¿ç”¨ Taylor é‡è¦æ€§
for head_idx in range(8):
    channels = range(head_idx * 128, (head_idx + 1) * 128)  # 128 ä¸ªè¿ç»­é€šé“

    # è®¡ç®—è¿™ä¸ª head çš„é‡è¦æ€§
    importance[head_idx] = sum(|âˆ‚L/âˆ‚W[c] Ã— W[c]| for c in channels)

# ç»“æœç¤ºä¾‹:
head_importance = [
    0.523,  # head 0
    0.891,  # head 1
    0.156,  # head 2 â† æœ€ä¸é‡è¦
    0.734,  # head 3
    0.621,  # head 4
    0.445,  # head 5 â† ç¬¬äºŒä¸é‡è¦
    0.812,  # head 6
    0.678,  # head 7
]
```

### é˜¶æ®µ3ï¼šé€‰æ‹©è¦å‰ªæçš„ heads

```python
# ch_sparsity = 0.25
num_heads = 8
num_to_prune = int(8 * 0.25) = 2  # å‰ªæ 2 ä¸ª heads

# é€‰æ‹©é‡è¦æ€§æœ€ä½çš„ 2 ä¸ª heads
sorted_heads = [2, 5, 4, 7, 0, 3, 6, 1]  # æŒ‰é‡è¦æ€§å‡åº
heads_to_prune = sorted_heads[:2] = [2, 5]  # æœ€ä¸é‡è¦çš„ 2 ä¸ª

# å‰ªæçš„é€šé“ç´¢å¼•
pruning_indices = [
    range(2 * 128, 3 * 128),  # head 2: [256, 257, ..., 383]
    range(5 * 128, 6 * 128),  # head 5: [640, 641, ..., 767]
]
```

### é˜¶æ®µ4ï¼šä¼ æ’­å‰ªæå†³ç­–

```python
# æ ¹æ®ä¾èµ–å›¾è‡ªåŠ¨ä¼ æ’­

k_proj å‰ªæ heads [2, 5]:
  â†’ v_proj ä¹Ÿå‰ªæ heads [2, 5]  (åŒæ­¥ï¼Œå› ä¸ºéƒ½æ˜¯ KV heads)
  â†’ q_proj å‰ªæå¯¹åº”çš„ Q heads
      (å› ä¸º GQA 4:1ï¼Œå‰ªæ 2 ä¸ª KV heads â†’ å‰ªæ 8 ä¸ª Q heads)
      ä¾‹å¦‚: Q heads [8, 9, 20, 21]ï¼ˆå¯¹åº” KV heads 2 å’Œ 5ï¼‰
  â†’ o_proj çš„è¾“å…¥ç»´åº¦ç›¸åº”å‡å°‘
```

**GQA æ¯”ä¾‹ä¼ æ’­ï¼š**

```
åŸå§‹:
  k_proj: 1024 (8 KV heads)
  q_proj: 4096 (32 Q heads)
  æ¯”ä¾‹: 32:8 = 4:1

å‰ªæ 2 ä¸ª KV heads:
  k_proj: 768 (6 KV heads)
  q_proj: 3072 (24 Q heads)
  æ¯”ä¾‹: 24:6 = 4:1 âœ… ä¿æŒä¸å˜ï¼
```

### é˜¶æ®µ5ï¼šç‰©ç†æ‰§è¡Œå‰ªæ

```python
# å¯¹äº k_proj
original_weight = k_proj.weight  # [1024, 4096]

# åˆ é™¤è¦å‰ªæçš„ heads (head 2 å’Œ head 5)
keep_indices = [0,1, 3,4, 6,7]  # ä¿ç•™çš„ heads
keep_channels = []
for head in keep_indices:
    keep_channels.extend(range(head * 128, (head + 1) * 128))

# æ–°çš„æƒé‡çŸ©é˜µ
new_weight = original_weight[keep_channels, :]  # [768, 4096]
k_proj.weight = nn.Parameter(new_weight)

# æ›´æ–° k_proj çš„é…ç½®
k_proj.out_features = 768  # ä» 1024 â†’ 768
```

---

## ğŸ“Š å®Œæ•´ç¤ºä¾‹ï¼šå‰ªæ Layer 5 çš„ 25%

### åŸå§‹çŠ¶æ€

```
Layer 5:
  q_proj: [4096, 4096]  â†’ 32 Q heads Ã— 128 = 4096
  k_proj: [4096, 1024]  â†’ 8 KV heads Ã— 128 = 1024
  v_proj: [4096, 1024]  â†’ 8 KV heads Ã— 128 = 1024
  o_proj: [4096, 4096]  â†’ è¾“å‡ºæŠ•å½±

å‚æ•°é‡: 16,777,216 + 4,194,304 + 4,194,304 + 16,777,216 = 41,943,040
```

### å‰ªæè¿‡ç¨‹

```
æ­¥éª¤1: è®¡ç®—é‡è¦æ€§
  k_proj çš„ 8 ä¸ª heads é‡è¦æ€§: [0.52, 0.89, 0.16, 0.73, 0.62, 0.45, 0.81, 0.68]

æ­¥éª¤2: é€‰æ‹©æœ€ä¸é‡è¦çš„ 2 ä¸ª headsï¼ˆ25% = 2/8ï¼‰
  å‰ªæ heads: [2, 5]

æ­¥éª¤3: å‰ªæ k_proj
  k_proj: [4096, 1024] â†’ [4096, 768]
  å‰ªæé€šé“: 256 (2 heads)

æ­¥éª¤4: è‡ªåŠ¨ä¼ æ’­
  v_proj: [4096, 1024] â†’ [4096, 768]  (åŒæ­¥ KV)
  q_proj: [4096, 4096] â†’ [4096, 3072] (GQA 4:1)
  o_proj: [4096, 4096] â†’ [3072, 4096] (è¾“å…¥ç»´åº¦åŒ¹é…)
```

### å‰ªæåçŠ¶æ€

```
Layer 5:
  q_proj: [4096, 3072]  â†’ 24 Q heads Ã— 128 = 3072
  k_proj: [4096, 768]   â†’ 6 KV heads Ã— 128 = 768
  v_proj: [4096, 768]   â†’ 6 KV heads Ã— 128 = 768
  o_proj: [3072, 4096]  â†’ è¾“å‡ºæŠ•å½±

å‚æ•°é‡: 12,582,912 + 3,145,728 + 3,145,728 + 12,582,912 = 31,457,280

å‡å°‘: 41,943,040 - 31,457,280 = 10,485,760 (25%)
```

---

## ğŸ¯ å…³é”®è¦ç‚¹æ€»ç»“

### 1. å•ä¸€å‰ªæç‡

```python
"ch_sparsity": 0.25  # æ‰€æœ‰å±‚éƒ½ç”¨è¿™ä¸ª
```

- âœ… **ç®€å•**: åªéœ€è¦ä¸€ä¸ªå‚æ•°
- âœ… **å¯é¢„æµ‹**: æ‰€æœ‰å±‚è¡Œä¸ºä¸€è‡´
- âŒ **ä¸çµæ´»**: æ— æ³•æ ¹æ®å±‚é‡è¦æ€§è°ƒæ•´

### 2. Head-level å‰ªæ

```python
"consecutive_groups": {
    layer.self_attn.k_proj: 128  # å¼ºåˆ¶ 128 çš„å€æ•°
}
```

- âœ… **ä¿æŒå®Œæ•´æ€§**: æ¯ä¸ª head æ˜¯ç‹¬ç«‹çš„ attention å•å…ƒ
- âœ… **GQA å‹å¥½**: å¤©ç„¶æ”¯æŒ 4:1 æ¯”ä¾‹
- âŒ **ç²’åº¦é™åˆ¶**: åªèƒ½æŒ‰ 12.5% çš„å€æ•°å‰ªæ

### 3. Taylor é‡è¦æ€§

```python
importance = |âˆ‚L/âˆ‚W Ã— W|
```

- âœ… **å‡†ç¡®**: è€ƒè™‘æ¢¯åº¦å’Œæƒé‡
- âœ… **ä¸€é˜¶è¿‘ä¼¼**: è®¡ç®—æ•ˆç‡é«˜
- âŒ **éœ€è¦æ¢¯åº¦**: å¿…é¡»æ‰§è¡Œåå‘ä¼ æ’­

### 4. ä¾èµ–å›¾ä¼ æ’­

```
k_proj å‰ªæ â†’ è‡ªåŠ¨ä¼ æ’­åˆ° q_proj, v_proj, o_proj
```

- âœ… **è‡ªåŠ¨åŒ–**: ä¸éœ€è¦æ‰‹åŠ¨æŒ‡å®šæ¯ä¸ªæ¨¡å—
- âœ… **ä¿è¯ä¸€è‡´æ€§**: ç»´åº¦è‡ªåŠ¨åŒ¹é…
- âœ… **æ”¯æŒ GQA**: è‡ªåŠ¨å¤„ç† Q/KV æ¯”ä¾‹

---

## ğŸ†š ä¸éå‡è¡¡å‰ªæçš„å¯¹æ¯”

| ç‰¹æ€§ | å‡åŒ€å‰ªæ | éå‡è¡¡å‰ªæ |
|------|---------|-----------|
| **å‰ªæç‡æ§åˆ¶** | `ch_sparsity: 0.25` | `ch_sparsity_dict: {...}` |
| **æ¯å±‚å‰ªæç‡** | å…¨éƒ¨ç›¸åŒ | æ¯å±‚ä¸åŒï¼ˆåŸºäºé‡è¦æ€§ï¼‰ |
| **å±‚é‡è¦æ€§** | ä¸è¯„ä¼° | è¯„ä¼°å¹¶ä½¿ç”¨ |
| **å®ç°å¤æ‚åº¦** | ç®€å• | å¤æ‚ |
| **æ€§èƒ½/å‰ªæç‡** | ä¸€èˆ¬ | æ›´å¥½ |

---

## ğŸš€ è¿è¡Œç¤ºä¾‹

```bash
# å‡åŒ€å‰ªæï¼šæ‰€æœ‰å±‚ 25%
python llama3.py \
    --base_model /path/to/Llama-3-8B-Instruct \
    --pruning_ratio 0.25 \
    --pruner_type taylor \
    --block_attention_layer_start 3 \
    --block_attention_layer_end 30 \
    --block_mlp_layer_start 3 \
    --block_mlp_layer_end 30
```

**ç»“æœï¼š**
- Layer 0-2: ä¸å‰ªæ
- Layer 3-29: æ¯å±‚éƒ½å‰ªæ 25%
- Layer 30-31: ä¸å‰ªæ
- å®é™…æ€»å‰ªæç‡: ~17-18%ï¼ˆå› ä¸ºä¿æŠ¤äº† 5 å±‚ï¼‰
