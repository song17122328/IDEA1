# å‡åŒ€å‰ªæ vs éå‡è¡¡å‰ªæè¯¦ç»†å¯¹æ¯”

## ğŸ“Š æ ¸å¿ƒåŒºåˆ«æ€»ç»“

| ç‰¹æ€§ | å‡åŒ€å‰ªæ (llama3.py) | éå‡è¡¡å‰ªæ (llama3_unbalanced_pruning.py) |
|------|---------------------|-------------------------------------------|
| **å‰ªæç‡** | æ‰€æœ‰å±‚ç›¸åŒ | æ¯å±‚ä¸åŒï¼ˆåŸºäºå±‚é‡è¦æ€§ï¼‰ |
| **å±‚é‡è¦æ€§è¯„ä¼°** | âŒ æ—  | âœ… æœ‰ï¼ˆremoval/activationæ–¹æ³•ï¼‰ |
| **ch_sparsity_dict** | âŒ æ—  | âœ… æœ‰ï¼ˆæ¯å±‚è‡ªå®šä¹‰å‰ªæç‡ï¼‰ |
| **å‰ªæç²’åº¦** | Head-level (128é€šé“) | Head-level (128é€šé“) |
| **GQAçº¦æŸ** | âœ… æœ‰ (consecutive_groups) | âœ… æœ‰ (consecutive_groups + channel_groups) |
| **ä¿æŠ¤å…³é”®å±‚** | âŒ æ‰‹åŠ¨æŒ‡å®šèŒƒå›´ | âœ… è‡ªåŠ¨è¿‡æ»¤ä½é‡è¦æ€§å±‚ |
| **å®é™…å‰ªæç‡** | æ¥è¿‘ç›®æ ‡ | å¯èƒ½ä½äºç›®æ ‡ï¼ˆä¿æŠ¤å±‚ï¼‰ |

---

## ğŸ”§ åŸæ¥çš„å‡åŒ€å‰ªæ (llama3.py)

### å…³é”®ä»£ç 

```python
# llama3.py:112-129
kwargs = {
    "importance": imp,                    # Taylor/L1/L2
    "global_pruning": args.global_pruning,
    "iterative_steps": args.iterative_steps,
    "ch_sparsity": args.pruning_ratio,   # â­ å…¨å±€å‰ªæç‡ï¼ˆæ‰€æœ‰å±‚ç›¸åŒï¼‰
    "ignored_layers": [],
    "channel_groups": {},                 # ç©ºå­—å…¸
    "consecutive_groups": {
        layer.self_attn.k_proj: layer.self_attn.head_dim
        for layer in model.model.layers   # æ‰€æœ‰å±‚çš„ head_dim = 128
    },
    "customized_pruners": {
        LlamaRMSNorm: llama_pruner.hf_rmsnorm_pruner,
    },
    "root_module_types": None,
    "root_instances": [
        model.model.layers[i].self_attn.k_proj
        for i in range(args.block_attention_layer_start, args.block_attention_layer_end)
    ] + [
        model.model.layers[i].mlp.gate_proj
        for i in range(args.block_mlp_layer_start, args.block_mlp_layer_end)
    ]
}
```

### å·¥ä½œåŸç†

#### 1ï¸âƒ£ æ‰€æœ‰å±‚ä½¿ç”¨ç›¸åŒçš„å‰ªæç‡

```python
"ch_sparsity": 0.25  # æ‰€æœ‰å±‚éƒ½å‰ªæ 25%
```

**ç¤ºä¾‹ï¼š** å¦‚æœè®¾ç½® `--pruning_ratio 0.25`
- Layer 0: å‰ªæ 25%
- Layer 1: å‰ªæ 25%
- Layer 2: å‰ªæ 25%
- ...
- Layer 31: å‰ªæ 25%

**é—®é¢˜ï¼š**
- âŒ ä¸è€ƒè™‘å±‚çš„é‡è¦æ€§å·®å¼‚
- âŒ å¯èƒ½è¿‡åº¦å‰ªæé‡è¦å±‚
- âŒ å¯èƒ½æ¬ å‰ªæä¸é‡è¦å±‚

#### 2ï¸âƒ£ Head-level å‰ªæï¼ˆé€šè¿‡ consecutive_groupsï¼‰

```python
"consecutive_groups": {
    layer.self_attn.k_proj: 128  # æ¯ä¸ª head æœ‰ 128 ä¸ªé€šé“
}
```

**å«ä¹‰ï¼š**
- k_proj çš„ 1024 ä¸ªè¾“å‡ºé€šé“è¢«åˆ†ä¸º 8 ä¸ª head
- æ¯æ¬¡å¿…é¡»å‰ªæå®Œæ•´çš„ headï¼ˆ128 ä¸ªè¿ç»­é€šé“ï¼‰
- å‰ªæç‡å¿…é¡»æ˜¯ 128/1024 çš„å€æ•°

**æœ‰æ•ˆå‰ªæç‡ï¼š**
```
12.5%  â†’ å‰ªæ 1 ä¸ª head (8 â†’ 7)
25.0%  â†’ å‰ªæ 2 ä¸ª head (8 â†’ 6)
37.5%  â†’ å‰ªæ 3 ä¸ª head (8 â†’ 5)
50.0%  â†’ å‰ªæ 4 ä¸ª head (8 â†’ 4)
```

#### 3ï¸âƒ£ é€‰æ‹©è¦å‰ªæçš„å±‚èŒƒå›´

```python
"root_instances": [
    # Attention: å±‚ 3-29
    model.model.layers[i].self_attn.k_proj
    for i in range(3, 30)
] + [
    # MLP: å±‚ 3-29
    model.model.layers[i].mlp.gate_proj
    for i in range(3, 30)
]
```

**ä¿æŠ¤çš„å±‚ï¼š**
- Layer 0-2: å‰ 3 å±‚ä¸å‰ªæ
- Layer 30-31: å 2 å±‚ä¸å‰ªæ

**åŸå› ï¼š** ç»éªŒæ€§é€‰æ‹©ï¼Œä¿æŠ¤å…³é”®å±‚

#### 4ï¸âƒ£ é‡è¦æ€§è¯„ä¼°ï¼ˆTaylorï¼‰

```python
# llama3.py:143-165
# åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œä½¿ç”¨ç¤ºä¾‹æ•°æ®è®¡ç®— Taylor é‡è¦æ€§
example_prompts = get_examples('bookcorpus', tokenizer, 10, seq_len=64)
loss = model(example_prompts, labels=example_prompts).loss
loss.backward()

# Taylor é‡è¦æ€§ = |âˆ‚L/âˆ‚W Ã— W|
```

**ç”¨é€”ï¼š**
- åœ¨æ¯ä¸€å±‚å†…éƒ¨ï¼Œé€‰æ‹©å“ªäº› head è¦å‰ªæ
- **ä¸æ˜¯** ç”¨æ¥å†³å®šæ¯å±‚çš„å‰ªæç‡ï¼ˆæ‰€æœ‰å±‚éƒ½æ˜¯ 25%ï¼‰

---

## ğŸš€ æ–°çš„éå‡è¡¡å‰ªæ (llama3_unbalanced_pruning.py)

### å…³é”®ä»£ç 

```python
# llama3_unbalanced_pruning.py:313-330
kwargs = {
    "importance": imp,
    "global_pruning": False,
    "iterative_steps": args.iterative_steps,
    "ch_sparsity": args.pruning_ratio,        # é»˜è®¤å‰ªæç‡ï¼ˆå¤‡ç”¨ï¼‰
    "ch_sparsity_dict": ch_sparsity_dict,     # â­ æ¯å±‚çš„è‡ªå®šä¹‰å‰ªæç‡
    "ignored_layers": [],
    "channel_groups": {
        layer.self_attn.q_proj: 4             # â­ GQA æ¯”ä¾‹çº¦æŸ
        for layer in model.model.layers
    },
    "consecutive_groups": {
        layer.self_attn.k_proj: layer.self_attn.head_dim
        for layer in model.model.layers
    },
    "customized_pruners": {
        LlamaRMSNorm: llama_pruner.hf_rmsnorm_pruner,
    },
    "root_module_types": None,
    "root_instances": [...]  # åŒä¸Š
}
```

### å·¥ä½œåŸç†

#### 1ï¸âƒ£ æ¯å±‚ä½¿ç”¨ä¸åŒçš„å‰ªæç‡ï¼ˆåŸºäºå±‚é‡è¦æ€§ï¼‰

```python
# æ­¥éª¤1: è¯„ä¼°å±‚é‡è¦æ€§
layer_importance = {
    0: 5291.99,  # éå¸¸é‡è¦
    1: 614.10,   # å¾ˆé‡è¦
    2: 2.43,
    ...
    31: 31.60
}

# æ­¥éª¤2: è®¡ç®—éå‡è¡¡å‰ªæç‡
layer_pruning_rates = {
    0: 0.0000,   # æœ€é‡è¦ â†’ ä¸å‰ªæ
    1: 0.0967,   # å¾ˆé‡è¦ â†’ å‰ªæ 9.67%
    2: 0.2612,   # ä¸€èˆ¬ â†’ å‰ªæ 26.12%
    11: 0.2750,  # ä¸å¤ªé‡è¦ â†’ å‰ªæ 27.5%
    31: 0.2039,  # é‡è¦ â†’ å‰ªæ 20.39%
}
```

**ç¤ºä¾‹å¯¹æ¯”ï¼š**

| å±‚ | é‡è¦æ€§ | å‰ªæç‡ | è¯´æ˜ |
|---|--------|--------|------|
| Layer 0 | 5291.99 | 0.00% | æœ€é‡è¦ï¼Œå®Œå…¨ä¿æŠ¤ |
| Layer 1 | 614.10 | 9.67% | å¾ˆé‡è¦ï¼Œè½»åº¦å‰ªæ |
| Layer 11 | 0.22 | 27.50% | ä¸é‡è¦ï¼Œé‡åº¦å‰ªæ |
| Layer 31 | 31.60 | 20.39% | è¾ƒé‡è¦ï¼Œä¸­åº¦å‰ªæ |

#### 2ï¸âƒ£ ch_sparsity_dictï¼šæ¯å±‚è‡ªå®šä¹‰å‰ªæç‡

```python
# layer_importance.py:312-327
ch_sparsity_dict = {}

for layer_idx, pruning_rate in layer_pruning_rates.items():
    layer = model.model.layers[layer_idx]

    # Attention: k_proj ä½œä¸º root module
    ch_sparsity_dict[layer.self_attn.k_proj] = pruning_rate

    # MLP: gate_proj ä½œä¸º root module
    ch_sparsity_dict[layer.mlp.gate_proj] = pruning_rate
```

**ç”Ÿæˆçš„å­—å…¸å†…å®¹ï¼š**
```python
{
    <Layer 2 çš„ k_proj>: 0.2612,
    <Layer 2 çš„ gate_proj>: 0.2612,
    <Layer 3 çš„ k_proj>: 0.2651,
    <Layer 3 çš„ gate_proj>: 0.2651,
    ...
}
```

è¿™ä¸ªå­—å…¸å‘Šè¯‰ MetaPrunerï¼š
- Layer 2 çš„ k_proj å‰ªæ 26.12%
- Layer 3 çš„ k_proj å‰ªæ 26.51%
- æ¯å±‚éƒ½æœ‰è‡ªå·±çš„å‰ªæç‡ï¼

#### 3ï¸âƒ£ å¢å¼ºçš„ GQA çº¦æŸï¼ˆchannel_groupsï¼‰

```python
"channel_groups": {
    layer.self_attn.q_proj: 4  # q_heads : kv_heads = 4:1
    for layer in model.model.layers
}
```

**ä½œç”¨ï¼š**
- ç¡®ä¿ q_proj å’Œ k_proj æŒ‰ 4:1 æ¯”ä¾‹å‰ªæ
- ä¾‹å¦‚ï¼šk_proj å‰ªæ 2 ä¸ª head â†’ q_proj å‰ªæ 8 ä¸ª head

**å¯¹æ¯”ï¼š**
- å‡åŒ€å‰ªæï¼šæ²¡æœ‰ channel_groupsï¼Œä¾èµ–ä¾èµ–å›¾ä¼ æ’­ï¼ˆå¯èƒ½ä¸å¤Ÿç²¾ç¡®ï¼‰
- éå‡è¡¡å‰ªæï¼šæ˜¾å¼æŒ‡å®š channel_groupsï¼Œç¡®ä¿ GQA æ¯”ä¾‹

#### 4ï¸âƒ£ è‡ªåŠ¨è¿‡æ»¤ä½å‰ªæç‡å±‚

```python
# llama3_unbalanced_pruning.py:232-246
min_effective_rate = 0.15  # æœ€å°æœ‰æ•ˆå‰ªæç‡ 15%

effective_pruning_rates = {
    idx: rate for idx, rate in filtered_pruning_rates.items()
    if rate >= min_effective_rate
}

# è‡ªåŠ¨è·³è¿‡å‰ªæç‡ < 15% çš„å±‚
# ä¾‹å¦‚ï¼šLayer 0 (0%) å’Œ Layer 1 (9.67%) è¢«è·³è¿‡
```

**åŸå› ï¼š**
- k_proj æœ‰ 1024 é€šé“ = 8 ä¸ª head
- æ¯ä¸ª head æœ‰ 128 é€šé“
- è‡³å°‘éœ€è¦å‰ªæ 1 ä¸ª head = 12.5%
- è®¾ç½®ä¸º 15% ç¡®ä¿å®‰å…¨

---

## ğŸ“ˆ å®é™…æ•ˆæœå¯¹æ¯”

### åœºæ™¯1ï¼šå‡åŒ€å‰ªæï¼ˆllama3.pyï¼‰

```bash
python llama3.py \
    --base_model /path/to/Llama-3-8B-Instruct \
    --pruning_ratio 0.25 \
    --block_attention_layer_start 3 \
    --block_attention_layer_end 30
```

**ç»“æœï¼š**
- å‰ªæ Layer 3-29ï¼ˆ27 å±‚ï¼‰
- æ¯å±‚éƒ½å‰ªæ 25%
- å®é™…å‰ªæç‡ï¼š~17-18%ï¼ˆå› ä¸ºä¿æŠ¤äº† 5 å±‚ï¼‰
- PPLï¼šå‡è®¾ä¸º X

**é—®é¢˜ï¼š**
- Layer 11ï¼ˆä¸é‡è¦ï¼‰åªå‰ªæäº† 25%ï¼Œå¯ä»¥å‰ªæ›´å¤š
- Layer 31ï¼ˆé‡è¦ï¼‰å‰ªæäº† 25%ï¼Œå¯èƒ½å¤ªå¤š

### åœºæ™¯2ï¼šéå‡è¡¡å‰ªæï¼ˆllama3_unbalanced_pruning.pyï¼‰

```bash
python llama3_unbalanced_pruning.py \
    --base_model /path/to/Llama-3-8B-Instruct \
    --pruning_ratio 0.25 \
    --block_attention_layer_start 0 \
    --block_attention_layer_end 32
```

**ç»“æœï¼š**
- è‡ªåŠ¨è¿‡æ»¤ Layer 0-1ï¼ˆå‰ªæç‡å¤ªä½ï¼‰
- å‰ªæ Layer 2-31ï¼ˆ30 å±‚ï¼‰
- æ¯å±‚å‰ªæç‡ä¸åŒï¼š20.39% ~ 27.50%
- å®é™…å‰ªæç‡ï¼š~20%
- PPLï¼šé¢„æœŸæ¯”å‡åŒ€å‰ªææ›´ä½ï¼ˆå› ä¸ºä¿æŠ¤äº†é‡è¦å±‚ï¼‰

**ä¼˜åŠ¿ï¼š**
- âœ… Layer 11ï¼ˆä¸é‡è¦ï¼‰å‰ªæ 27.5%ï¼ˆæ›´æ¿€è¿›ï¼‰
- âœ… Layer 0ï¼ˆæœ€é‡è¦ï¼‰ä¸å‰ªæï¼ˆå®Œå…¨ä¿æŠ¤ï¼‰
- âœ… Layer 31ï¼ˆé‡è¦ï¼‰å‰ªæ 20.39%ï¼ˆæ¯” 25% æ›´ä¿å®ˆï¼‰
- âœ… æ›´å¥½çš„æ€§èƒ½/å‰ªæç‡æƒè¡¡

---

## ğŸ¯ å¦‚ä½•é€‰æ‹©ï¼Ÿ

### ä½¿ç”¨å‡åŒ€å‰ªæï¼ˆllama3.pyï¼‰å½“ï¼š
- âœ… æƒ³è¦ç®€å•ã€å¯é¢„æµ‹çš„å‰ªæç‡
- âœ… å¯¹æ‰€æœ‰å±‚ä¸€è§†åŒä»
- âœ… å¿«é€Ÿå®éªŒï¼Œä¸éœ€è¦å±‚é‡è¦æ€§è¯„ä¼°
- âœ… å·²çŸ¥å“ªäº›å±‚è¦ä¿æŠ¤ï¼ˆæ‰‹åŠ¨è®¾ç½®èŒƒå›´ï¼‰

### ä½¿ç”¨éå‡è¡¡å‰ªæï¼ˆllama3_unbalanced_pruning.pyï¼‰å½“ï¼š
- âœ… æƒ³è¦æ›´å¥½çš„æ€§èƒ½/å‰ªæç‡æƒè¡¡
- âœ… æ„¿æ„èŠ±æ—¶é—´è¯„ä¼°å±‚é‡è¦æ€§
- âœ… å¸Œæœ›è‡ªåŠ¨ä¿æŠ¤é‡è¦å±‚
- âœ… è¿½æ±‚æ›´ä½çš„ PPL
- âœ… åšå­¦æœ¯ç ”ç©¶ï¼Œéœ€è¦åˆ›æ–°æ–¹æ³•

---

## ğŸ“ å…±åŒç‚¹

ä¸¤ç§æ–¹æ³•éƒ½ï¼š
1. âœ… ä½¿ç”¨ **Head-level å‰ªæ**ï¼ˆconsecutive_groupsï¼‰
2. âœ… ä¿æŒ **GQA çº¦æŸ**ï¼ˆq_heads : kv_heads = 4:1ï¼‰
3. âœ… ä½¿ç”¨ **Taylor é‡è¦æ€§** é€‰æ‹©æ¯å±‚å†…éƒ¨å“ªäº› head è¦å‰ªæ
4. âœ… æ”¯æŒ **è¿­ä»£å¼å‰ªæ**ï¼ˆiterative_stepsï¼‰
5. âœ… ä½¿ç”¨ç›¸åŒçš„ **Torch-Pruning** åº“å’Œ **MetaPruner**

**æ ¸å¿ƒåŒºåˆ«åªåœ¨äºï¼š** æ¯å±‚çš„å‰ªæç‡æ˜¯å¦ç›¸åŒï¼

---

## ğŸ”¬ å®éªŒå»ºè®®

è¿è¡Œå¯¹æ¯”å®éªŒï¼š

```bash
# å®éªŒ1ï¼šå‡åŒ€å‰ªæ
python llama3.py \
    --pruning_ratio 0.25 \
    --save_ckpt_log_name "llama_uniform_25"

# å®éªŒ2ï¼šéå‡è¡¡å‰ªæ
python llama3_unbalanced_pruning.py \
    --pruning_ratio 0.25 \
    --save_ckpt_log_name "llama_unbalanced_25"
```

å¯¹æ¯”æŒ‡æ ‡ï¼š
- å®é™…å‰ªæç‡
- PPL (wikitext2)
- æ¨¡å‹å¤§å°
- ç”Ÿæˆè´¨é‡

é¢„æœŸï¼šéå‡è¡¡å‰ªæçš„ PPL æ›´ä½ï¼ˆæ€§èƒ½æ›´å¥½ï¼‰ï¼
