# éå‡è¡¡ç»“æ„åŒ–å‰ªæè¯¦ç»†è§£é‡Š

## æ‚¨çš„é—®é¢˜

> ä¸ºä»€ä¹ˆæˆ‘ä»¬åœ¨å‰é¢å¾—åˆ°äº†æ¯ä¸ªå±‚ï¼ˆ0-31ï¼‰çš„å‰ªæç‡ï¼Œä½†åœ¨æ‰§è¡Œç»“æ„åŒ–å‰ªæçš„æ—¶å€™ä»ç„¶å‰ªæçš„å±‚æ•°çš„idxä¸º3å±‚åˆ°29å±‚ï¼Ÿ

## ç®€çŸ­ç­”æ¡ˆ

**æ­¥éª¤2** ç¡®å®è®¡ç®—äº†æ‰€æœ‰32å±‚ï¼ˆ0-31ï¼‰çš„å‰ªæç‡ï¼Œä½† **æ­¥éª¤3** é€šè¿‡å‚æ•°è¿‡æ»¤ï¼Œåªé€‰æ‹©äº†3-29å±‚å®é™…æ‰§è¡Œå‰ªæã€‚è¿™æ˜¯ä¸ºäº†**ä¿æŠ¤å…³é”®å±‚**ï¼Œåœ¨å‰ªæç‡å’Œæ¨¡å‹æ€§èƒ½ä¹‹é—´å–å¾—å¹³è¡¡ã€‚

---

## è¯¦ç»†è§£é‡Š

### 1ï¸âƒ£ ä¸ºä»€ä¹ˆåªå‰ªæ 3-29 å±‚ï¼Ÿ

ä»£ç ä¸­çš„è¿‡æ»¤é€»è¾‘ï¼ˆ`llama3_unbalanced_pruning.py:222-228`ï¼‰ï¼š

```python
pruning_layers = set(range(args.block_attention_layer_start, args.block_attention_layer_end)) | \
                 set(range(args.block_mlp_layer_start, args.block_mlp_layer_end))

filtered_pruning_rates = {
    idx: rate for idx, rate in layer_pruning_rates.items()
    if idx in pruning_layers
}
```

**é»˜è®¤å‚æ•°ï¼š**
- `--block_attention_layer_start = 3`
- `--block_attention_layer_end = 30`  ï¼ˆæ³¨æ„ï¼šPythonçš„ `range(3, 30)` ä¸åŒ…å«30ï¼‰
- `--block_mlp_layer_start = 3`
- `--block_mlp_layer_end = 30`

**ç»“æœï¼š**
- å®é™…å‰ªæå±‚ = [3, 4, 5, ..., 29]ï¼Œå…±27å±‚
- ä¿æŠ¤çš„å±‚ = [0, 1, 2, 30, 31]ï¼Œå…±5å±‚

**ä¿æŠ¤åŸå› ï¼š**
- ğŸ›¡ï¸ **å‰3å±‚ï¼ˆ0-2ï¼‰**ï¼šåº•å±‚ç‰¹å¾æå–å±‚ï¼Œå¯¹æ¨¡å‹æ€§èƒ½å½±å“å¤§
- âœ‚ï¸ **ä¸­é—´27å±‚ï¼ˆ3-29ï¼‰**ï¼šæ ¹æ®å±‚é‡è¦æ€§åˆ†é…ä¸åŒå‰ªæç‡
- ğŸ›¡ï¸ **å2å±‚ï¼ˆ30-31ï¼‰**ï¼šé«˜å±‚è¯­ä¹‰ç†è§£å±‚ï¼Œå¯¹æ¨¡å‹æ€§èƒ½å½±å“å¤§

---

### 2ï¸âƒ£ æ­¥éª¤3ï¼šch_sparsity_dict æ˜¯ä»€ä¹ˆï¼Ÿ

#### å®šä¹‰
`ch_sparsity_dict` æ˜¯ä¸€ä¸ªPythonå­—å…¸ï¼Œå°† **PyTorchæ¨¡å—å¯¹è±¡** æ˜ å°„åˆ°å…¶ **å‰ªæç‡**ï¼š

```python
{
    <module object>: pruning_rate,
    ...
}
```

#### ä½œç”¨
å‘Šè¯‰ `MetaPruner` æ¯ä¸ªæ¨¡å—åº”è¯¥å‰ªæå¤šå°‘æ¯”ä¾‹çš„é€šé“æ•°ã€‚

#### ä¸ºä»€ä¹ˆé€‰æ‹© k_proj å’Œ gate_projï¼Ÿ

**Llama æ¨¡å‹æ¶æ„ï¼š**
æ¯å±‚åŒ…å«ä¸¤ä¸ªä¸»è¦ç»„ä»¶ï¼š
1. **Self-Attention**: `q_proj`, `k_proj`, `v_proj`, `o_proj`
2. **MLP**: `gate_proj`, `up_proj`, `down_proj`

**ç»“æ„åŒ–å‰ªæè§„åˆ™ï¼š**
- **Attention**: å‰ªæ `k_proj` çš„è¾“å‡ºé€šé“ â†’ è‡ªåŠ¨ä¼ æ’­åˆ° `q_proj`, `v_proj`, `o_proj`
- **MLP**: å‰ªæ `gate_proj` çš„è¾“å‡ºé€šé“ â†’ è‡ªåŠ¨ä¼ æ’­åˆ° `up_proj`, `down_proj`

**ä¼˜åŠ¿ï¼š** åªéœ€è®¾ç½® 2 ä¸ª root moduleï¼Œå°±èƒ½å‰ªææ•´å±‚çš„ 7 ä¸ªçº¿æ€§å±‚ï¼

#### åˆ›å»ºè¿‡ç¨‹

ä»£ç ï¼ˆ`layer_importance.py:312-327`ï¼‰ï¼š

```python
def create_ch_sparsity_dict_for_llama(model, layer_pruning_rates,
                                      prune_attention=True, prune_mlp=True):
    ch_sparsity_dict = {}

    for layer_idx, pruning_rate in layer_pruning_rates.items():
        layer = model.model.layers[layer_idx]

        # Attention æ¨¡å—
        if prune_attention:
            ch_sparsity_dict[layer.self_attn.k_proj] = pruning_rate

        # MLP æ¨¡å—
        if prune_mlp:
            ch_sparsity_dict[layer.mlp.gate_proj] = pruning_rate

    return ch_sparsity_dict
```

#### å†…å®¹ç¤ºä¾‹

ä»æ‚¨çš„æ—¥å¿—æ¥çœ‹ï¼Œåˆ›å»ºäº† **54ä¸ªæ¨¡å—** çš„å‰ªæç‡æ˜ å°„ï¼š

```
Layer 3 (å‰ªæç‡: 0.2651):
  - model.model.layers[3].self_attn.k_proj â†’ 0.2651
  - model.model.layers[3].mlp.gate_proj â†’ 0.2651

Layer 4 (å‰ªæç‡: 0.2657):
  - model.model.layers[4].self_attn.k_proj â†’ 0.2657
  - model.model.layers[4].mlp.gate_proj â†’ 0.2657

...

Layer 29 (å‰ªæç‡: 0.2496):
  - model.model.layers[29].self_attn.k_proj â†’ 0.2496
  - model.model.layers[29].mlp.gate_proj â†’ 0.2496

æ€»è®¡: 27å±‚ Ã— 2æ¨¡å—/å±‚ = 54ä¸ªæ¨¡å—
```

---

### 3ï¸âƒ£ æ­¥éª¤4ï¼šMetaPruner å¦‚ä½•å·¥ä½œï¼Ÿ

#### ä»€ä¹ˆæ˜¯ç»“æ„åŒ–å‰ªæï¼Ÿ

**éç»“æ„åŒ–å‰ªæï¼ˆç¨€ç–å‰ªæï¼‰ï¼š**
- å°†æƒé‡çŸ©é˜µä¸­çš„æŸäº›å…ƒç´ è®¾ä¸º 0
- å‚æ•°é€»è¾‘ä¸Šå‡å°‘ï¼Œä½†ç‰©ç†å†…å­˜ä¸å‡å°‘
- éœ€è¦ç¨€ç–çŸ©é˜µè¿ç®—æ”¯æŒæ‰èƒ½åŠ é€Ÿ

**ç»“æ„åŒ–å‰ªæï¼ˆé€šé“å‰ªæï¼‰ï¼š**
- åˆ é™¤æ•´ä¸ªé€šé“ï¼ˆç¥ç»å…ƒï¼‰
- ç‰©ç†ä¸Šå‡å°æ¨¡å‹å°ºå¯¸
- ç›´æ¥åŠ é€Ÿï¼Œæ— éœ€ç‰¹æ®Šç¡¬ä»¶æ”¯æŒ

**ç¤ºä¾‹ï¼š**
```
åŸå§‹:    Linear(4096 â†’ 4096)
         æƒé‡çŸ©é˜µ: [4096, 4096]
         å‚æ•°é‡: 16,777,216

å‰ªæ 25%: Linear(4096 â†’ 3072)  âœ… çœŸæ­£å‡å°‘å‚æ•°å’Œæ˜¾å­˜
         æƒé‡çŸ©é˜µ: [3072, 4096]
         å‚æ•°é‡: 12,582,912
         å‡å°‘: 4,194,304 (25%)
```

#### MetaPruner æ˜¯ä»€ä¹ˆï¼Ÿ

`MetaPruner` æ˜¯ Torch-Pruning åº“çš„æ ¸å¿ƒå‰ªæå™¨ã€‚

**ç‰¹ç‚¹ï¼š**
1. è‡ªåŠ¨è¿½è¸ªæ¨¡å—ä¹‹é—´çš„ä¾èµ–å…³ç³»
2. ç¡®ä¿å‰ªæåæ¨¡å‹ç»“æ„ä¸€è‡´æ€§
3. æ”¯æŒå„ç§é‡è¦æ€§è¯„ä¼°æ–¹æ³•ï¼ˆTaylorã€L1ã€L2ç­‰ï¼‰

#### MetaPruner å·¥ä½œæµç¨‹

**ç¬¬1æ­¥ï¼šæ„å»ºä¾èµ–å›¾ï¼ˆDependency Graphï¼‰**
- **è¾“å…¥**: `forward_prompts`ï¼ˆç¤ºä¾‹è¾“å…¥å¼ é‡ï¼‰
- **è¿‡ç¨‹**: æ‰§è¡Œä¸€æ¬¡å‰å‘ä¼ æ’­ï¼Œè®°å½•æ‰€æœ‰æ¨¡å—çš„è¾“å…¥è¾“å‡ºå…³ç³»
- **è¾“å‡º**: ä¾èµ–å›¾ï¼Œè®°å½•å“ªäº›æ¨¡å—çš„è¾“å‡ºè¿æ¥åˆ°å“ªäº›æ¨¡å—çš„è¾“å…¥

ç¤ºä¾‹ï¼ˆLayer 3ï¼‰ï¼š
```
è¾“å…¥: hidden_states [batch, seq_len, 4096]
  â”‚
  â”œâ”€â”€â†’ q_proj: Linear(4096 â†’ 4096)
  â”‚
  â”œâ”€â”€â†’ k_proj: Linear(4096 â†’ 1024) â† root module
  â”‚
  â””â”€â”€â†’ v_proj: Linear(4096 â†’ 1024)

  â†’ Attention è®¡ç®—

  â†’ o_proj: Linear(4096 â†’ 4096)
```

**ç¬¬2æ­¥ï¼šè®¡ç®—é€šé“é‡è¦æ€§**
- **Taylor é‡è¦æ€§**: `importance = |âˆ‚L/âˆ‚W Ã— W|`
- **L1 é‡è¦æ€§**: `importance = |W|`
- **L2 é‡è¦æ€§**: `importance = ||W||â‚‚`

ä¸ºæ¯ä¸ªè¾“å‡ºé€šé“è®¡ç®—é‡è¦æ€§åˆ†æ•°ã€‚

ç¤ºä¾‹ï¼ˆLayer 5 çš„ k_projï¼‰ï¼š
```
k_proj æƒé‡: [1024, 4096]  (1024ä¸ªè¾“å‡ºé€šé“)

è®¡ç®—æ¯ä¸ªè¾“å‡ºé€šé“çš„é‡è¦æ€§:
  Channel 0:   importance = 0.523
  Channel 1:   importance = 0.891
  Channel 2:   importance = 0.156  â† ä¸é‡è¦
  ...
  Channel 1023: importance = 0.734
```

**ç¬¬3æ­¥ï¼šé€‰æ‹©è¦å‰ªæçš„é€šé“**

ä¾‹å¦‚ï¼šLayer 5 çš„ k_projï¼Œå‰ªæç‡ = 0.2629
```
åŸå§‹é€šé“æ•°: 1024
ä¿ç•™é€šé“æ•°: 1024 Ã— (1 - 0.2629) â‰ˆ 755
å‰ªæé€šé“æ•°: 1024 - 755 = 269

é€‰æ‹©é‡è¦æ€§æœ€ä½çš„ 269 ä¸ªé€šé“è¿›è¡Œå‰ªæ
```

**ç¬¬4æ­¥ï¼šä¼ æ’­å‰ªæå†³ç­–**

æ ¹æ®ä¾èµ–å›¾ï¼Œè‡ªåŠ¨ä¼ æ’­å‰ªæï¼š

```
Layer 5 ç¤ºä¾‹:

k_proj è¾“å‡º: 1024 â†’ 755 é€šé“
  â†“
(GQA: q_proj å’Œ k_proj çš„æ¯”ä¾‹å…³ç³»)
  â†“
q_proj è¾“å‡º: 4096 â†’ 3020 é€šé“  (4:1 æ¯”ä¾‹)
v_proj è¾“å‡º: 1024 â†’ 755 é€šé“   (1:1 æ¯”ä¾‹)
  â†“
o_proj è¾“å…¥: 4096 â†’ 3020 é€šé“  (å¿…é¡»åŒ¹é…)

è¿™ä¿è¯äº† Attention æœºåˆ¶çš„ç»´åº¦ä¸€è‡´æ€§ï¼

åŒç†ï¼ŒMLP çš„ä¼ æ’­:
gate_proj è¾“å‡º: 14336 â†’ 10570 é€šé“
up_proj   è¾“å‡º: 14336 â†’ 10570 é€šé“
down_proj è¾“å…¥: 14336 â†’ 10570 é€šé“
```

**ç¬¬5æ­¥ï¼šç‰©ç†æ‰§è¡Œå‰ªæ**

å¯¹äºæ¯ä¸ªè¢«æ ‡è®°å‰ªæçš„é€šé“ï¼š
1. ä»æƒé‡çŸ©é˜µä¸­åˆ é™¤å¯¹åº”çš„è¡Œ/åˆ—
2. æ›´æ–°æ¨¡å—çš„ `in_features` å’Œ `out_features`
3. é‡Šæ”¾æ˜¾å­˜

ç¤ºä¾‹ï¼š
```
åŸå§‹: k_proj = Linear(in=4096, out=1024)
  weight.shape = [1024, 4096]
  å‚æ•°é‡ = 4,194,304
  æ˜¾å­˜å ç”¨ (FP16) = 8.4 MB

å‰ªæå: k_proj = Linear(in=4096, out=755)
  weight.shape = [755, 4096]
  å‚æ•°é‡ = 3,092,480
  æ˜¾å­˜å ç”¨ (FP16) = 6.2 MB

  å‡å°‘å‚æ•° = 1,101,824 (26.29%)
  å‡å°‘æ˜¾å­˜ = 2.2 MB
```

---

## å¯¹æ¯”ï¼šå‰ªææ‰€æœ‰å±‚ vs åªå‰ªæ 3-29 å±‚

### åœºæ™¯Aï¼šå‰ªææ‰€æœ‰å±‚ï¼ˆ0-31ï¼‰
- âœ… **ä¼˜ç‚¹**: æ›´é«˜çš„å‚æ•°å‡å°‘ç‡
- âŒ **ç¼ºç‚¹**: PPL æ˜¾è‘—ä¸Šå‡ï¼Œæ€§èƒ½ä¸‹é™æ˜æ˜¾

### åœºæ™¯Bï¼šåªå‰ªæ 3-29 å±‚ï¼ˆå½“å‰æ–¹æ¡ˆï¼‰
- âœ… **ä¼˜ç‚¹**: ä¿æŒè¾ƒå¥½æ€§èƒ½ï¼ŒPPL ä¸Šå‡è¾ƒå°
- âŒ **ç¼ºç‚¹**: å‚æ•°å‡å°‘ç‡ç•¥ä½

### ä»æ‚¨çš„æ—¥å¿—çœ‹ï¼š
```
å®é™…å‰ªæç‡: 17.19% (ç›®æ ‡ 25%)
â†’ å› ä¸ºä¿æŠ¤äº† 5 å±‚ï¼Œå®é™…å‰ªæçš„å±‚æ•°å‡å°‘
â†’ å¦‚æœå‰ªææ‰€æœ‰å±‚ï¼Œå®é™…å‰ªæç‡ä¼šæ›´æ¥è¿‘ 25%
```

---

## å¦‚ä½•è°ƒæ•´ï¼Ÿ

å¦‚æœæ‚¨æƒ³å‰ªææ‰€æœ‰å±‚ï¼Œå¯ä»¥ä¿®æ”¹å‚æ•°ï¼š

```bash
python llama3_unbalanced_pruning.py \
    --base_model "$ORIGINAL_MODEL" \
    --save_ckpt_log_name "llama_unbalanced_prune_all_layers" \
    --pruning_ratio 0.25 \
    --block_attention_layer_start 0  â† ä»ç¬¬0å±‚å¼€å§‹
    --block_attention_layer_end 32    â† åˆ°ç¬¬31å±‚ç»“æŸ
    --block_mlp_layer_start 0
    --block_mlp_layer_end 32
```

è¿™æ ·ä¼šå‰ªææ‰€æœ‰å±‚ï¼Œä½†å¯èƒ½ä¼šå¯¼è‡´ï¼š
- å®é™…å‰ªæç‡æ›´æ¥è¿‘ 25%
- PPL ä¸Šå‡æ›´æ˜æ˜¾
- éœ€è¦æ›´å¤šçš„å¾®è°ƒæ¥æ¢å¤æ€§èƒ½

---

## æ€»ç»“

1. **æ­¥éª¤2** è®¡ç®—æ‰€æœ‰å±‚ï¼ˆ0-31ï¼‰çš„å‰ªæç‡ï¼ˆåŸºäºå±‚é‡è¦æ€§ï¼‰
2. **æ­¥éª¤3** è¿‡æ»¤å±‚ + åˆ›å»ºæ¨¡å—çº§å­—å…¸ï¼ˆä¿æŠ¤å…³é”®å±‚ 0-2 å’Œ 30-31ï¼‰
3. **æ­¥éª¤4** MetaPruner æ‰§è¡Œç»“æ„åŒ–å‰ªæï¼ˆç‰©ç†å‡å°æ¨¡å‹ï¼‰

**è¿™å°±æ˜¯ä¸ºä»€ä¹ˆï¼š**
- ğŸ“Š çœ‹åˆ° 32 å±‚çš„å‰ªæç‡
- âœ‚ï¸ ä½†åªå‰ªæ 27 å±‚ï¼ˆ3-29ï¼‰
- ğŸ›¡ï¸ ä¿æŠ¤ 5 å±‚ï¼ˆ0-2, 30-31ï¼‰
- ğŸ¯ åœ¨å‰ªæç‡å’Œæ€§èƒ½ä¹‹é—´å–å¾—å¹³è¡¡

---

## ç›¸å…³æ–‡ä»¶

- `llama3_unbalanced_pruning.py` - å®Œæ•´çš„éå‡è¡¡å‰ªææµç¨‹
- `layer_importance.py` - å±‚é‡è¦æ€§è¯„ä¼°å’Œ ch_sparsity_dict åˆ›å»º
- `analyze_pruning_details_simple.py` - è¯¦ç»†åˆ†æè„šæœ¬ï¼ˆæ— éœ€åŠ è½½æ¨¡å‹ï¼‰
- `visualize_pruning_flow.py` - å¯è§†åŒ–è„šæœ¬ï¼ˆéœ€è¦ matplotlibï¼‰

è¿è¡Œåˆ†æè„šæœ¬æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯ï¼š
```bash
python analyze_pruning_details_simple.py
```
